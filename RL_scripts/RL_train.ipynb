{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN policy ?\n",
    "- grid search for HP tuning (OK)\n",
    "- Increasingly difficult Environment\n",
    "- Positive reward for populating increasingly \"deep\" blending tanks ?\n",
    "- RL for chem sched paper (https://arxiv.org/pdf/2203.00636)\n",
    "- Masking (https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html, https://arxiv.org/pdf/2006.14171)\n",
    "    - Adding binary decision variables ?g  \n",
    "    - Requires discrete action space (only integer flows -> treated as categories ?)\n",
    "    - masking: disable incoming flows (resp. outgoing flows) for tanks at UB inv limit (resp. LB inv. limit), disable selling/buying when available = 0\n",
    "    - multiple envs with multiple agents ? (MARL, https://arxiv.org/pdf/2103.01955)\n",
    "        - Predict successive pipelines (\"source > blend\" then \"blend > blend\" (as many as required) then \"blend > demand\")\n",
    "        - Each agent has access to the whole state\n",
    "        - Action mask is derived from the previous agent's actions (0 if inventory at bounds or incoming flow already reserved, else 1)\n",
    "        - https://github.com/Rohan138/marl-baselines3/blob/main/marl_baselines3/independent_ppo.py\n",
    "- Safe RL: (https://proceedings.mlr.press/v119/wachi20a/wachi20a.pdf)\n",
    "    - \"Unsafe state\" ? > Do not enforce constraints strictly, instead opt for early episode termination to show which states are unsafe ? \n",
    "    - Implementations:\n",
    "        - https://pypi.org/project/fast-safe-rl/#description (Policy optimizers)\n",
    "        - https://github.com/PKU-Alignment/safety-gymnasium/tree/main/safety_gymnasium (environments; \"cost\" ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try other learning rates/CNN policies\n",
    "2. Implement Masking with single agent\n",
    "3. Try other ways to tell the model what are illegal/unsafe states (safe RL)\n",
    "4. Try multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masking: Discretization of action space is too slow/might not work -> Need to implement masking for continuous action space\n",
    "- Recurrent policy makes the most sense ? (window of demand forecasts)\n",
    "- https://www.reddit.com/r/reinforcementlearning/comments/17l5b47/invalid_action_masking_when_action_space_is/\n",
    "    - Suggestion of autoregressive model for having constraints respected: one predicted action is input to a second model\n",
    "    - Suggestion of editing the distribution in such a way that the constraint is respected\n",
    "- https://www.sciencedirect.com/science/article/pii/S0098135420301599\n",
    "    - Choice of ELU activation ?\n",
    "    - Choice of NN size ?\n",
    "    - \"The feature engineering in the net inventory means the network does not have to learn these relationships itself, which did help speed training.\" ?\n",
    "- Simplify the problem (remove tanks 5 to 8), find the optimal solution with Gurobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove all constraints except in/out\n",
    "- https://arxiv.org/pdf/1711.11157\n",
    "- https://arxiv.org/pdf/2111.01564\n",
    "- Softmax with large coef to produce action mask\n",
    "- Graph convolution NN instead of RNN ?\n",
    "    - https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "    - Graph rep. learning - William L Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPG\n",
    "- Softmax\n",
    "- ~~Remove non-selling rewards~~\n",
    "- MultiplexNet\n",
    "- Why softmax doesn't work ? -> gradient doesn't compute properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finalize adjustment of flows\n",
    "- Add more difficulty (bigger env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Added penalty types logging\n",
    "- Added regularization, no success\n",
    "- Tried SDE briefly, no success\n",
    "- Hard/Unable to reproduce results on best configs (3 and 12)\n",
    "\n",
    "Next steps:\n",
    "- Find non-trivial optimal solution\n",
    "- Need different env versions to compare how flows are processed and the impact on performance\n",
    "- Read SDE method & distribution \n",
    "- Generate random supply/demands and solutions with solver; train classic NN in a supervised fashion (loss function = reward function ? ; multi-period aspect ?)\n",
    "- \"Hire\" another student ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "try:\n",
    "    print(curr_dir)\n",
    "except:\n",
    "    curr_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "    os.chdir(curr_dir)\n",
    "    print(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO, DDPG, SAC, TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import *\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
    "\n",
    "from envs import BlendEnv, flatten_and_track_mappings, reconstruct_dict\n",
    "from models import *\n",
    "from math import exp, log\n",
    "import yaml\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./configs/29.yaml\", \"r\") as f:\n",
    "    s = \"\".join(f.readlines())\n",
    "    cfg = yaml.load(s, Loader=yaml.FullLoader)\n",
    "    \n",
    "layout = \"simplest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"custom_softmax\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_softmax\n",
    "elif cfg[\"policytype\"] == \"MLP\":\n",
    "    policytype = \"MlpPolicy\"\n",
    "elif cfg[\"policytype\"] == \"MLPtanh\":\n",
    "    policytype = CustomMLP_ACP_simplest_tanh\n",
    "    \n",
    "optimizer_cls = eval(cfg[\"optimizer\"])\n",
    "\n",
    "if cfg[\"model\"][\"act_fn\"] == \"ReLU\":\n",
    "    act_cls = th.nn.ReLU\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"tanh\":\n",
    "    act_cls = th.nn.Tanh\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"sigmoid\":\n",
    "    act_cls = th.nn.Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./configs/json/connections_{layout}.json\" ,\"r\") as f:\n",
    "    connections_s = f.readline()\n",
    "connections = json.loads(connections_s)\n",
    "\n",
    "with open(f\"./configs/json/action_sample_{layout}.json\" ,\"r\") as f:\n",
    "    action_sample_s = f.readline()\n",
    "action_sample = json.loads(action_sample_s)\n",
    "action_sample_flat, mapp = flatten_and_track_mappings(action_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbp(connections):\n",
    "    sources = list(connections[\"source_blend\"].keys())\n",
    "    \n",
    "    b_list = list(connections[\"blend_blend\"].keys())\n",
    "    for b in connections[\"blend_blend\"].keys():\n",
    "        b_list += connections[\"blend_blend\"][b]\n",
    "    b_list += list(connections[\"blend_demand\"].keys())\n",
    "    blenders = list(set(b_list))\n",
    "    \n",
    "    p_list = []\n",
    "    for p in connections[\"blend_demand\"].keys():\n",
    "        p_list += connections[\"blend_demand\"][p]\n",
    "    demands = list(set(p_list))\n",
    "    \n",
    "    return sources, blenders, demands\n",
    "sources, blenders, demands = get_sbp(connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 6\n",
    "if layout == \"base\":\n",
    "    sigma = {\"s1\":{\"q1\": 0.06}, \"s2\":{\"q1\": 0.26}}\n",
    "    sigma_ub = {\"p1\":{\"q1\": 0.16}, \"p2\":{\"q1\": 1}}\n",
    "    sigma_lb = {\"p1\":{\"q1\": 0}, \"p2\":{\"q1\": 0}}\n",
    "else:\n",
    "    sigma = {s:{\"q1\": 0.06} for s in sources}\n",
    "    sigma_ub = {d:{\"q1\": 0.16} for d in demands}\n",
    "    sigma_lb = {d:{\"q1\": 0} for d in demands}\n",
    "    \n",
    "s_inv_lb = {s: 0 for s in sources}\n",
    "s_inv_ub = {s: 999 for s in sources}\n",
    "d_inv_lb = {d: 0 for d in demands}\n",
    "d_inv_ub = {d: 999 for d in demands}\n",
    "betaT_d = {d: 1 for d in demands} # Price of sold products\n",
    "b_inv_ub = {j: 30 for j in blenders} \n",
    "b_inv_lb = {j: 0 for j in blenders}\n",
    "betaT_s = {s: cfg[\"env\"][\"product_cost\"]  for s in sources} # Cost of bought products\n",
    "\n",
    "if cfg[\"env\"][\"uniform_data\"]:\n",
    "    tau0   = {s: [np.random.normal(15, 2) for _ in range(13)] for s in sources}\n",
    "    delta0 = {d: [np.random.normal(15, 2) for _ in range(13)] for d in demands}\n",
    "else:\n",
    "    tau0   = {s: [10, 10, 10, 0, 0, 0] for s in sources}\n",
    "    delta0 = {d: [0, 0, 0, 10, 10, 10] for d in demands}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = False, T = T,\n",
    "               D = cfg[\"env\"][\"D\"], Q = cfg[\"env\"][\"Q\"], P = cfg[\"env\"][\"P\"], B = cfg[\"env\"][\"B\"], Z = cfg[\"env\"][\"Z\"], M = cfg[\"env\"][\"M\"],\n",
    "               reg = cfg[\"env\"][\"reg\"], reg_lambda = cfg[\"env\"][\"reg_lambda\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"], alpha = cfg[\"env\"][\"alpha\"], \n",
    "               beta = cfg[\"env\"][\"beta\"], connections = connections, \n",
    "               action_sample = action_sample, tau0 = tau0, delta0 = delta0, sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, \n",
    "                   norm_obs=cfg[\"obs_normalizer\"], \n",
    "                   norm_reward=cfg[\"reward_normalizer\"])\n",
    "# env = VecCheckNan(env, raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"], \n",
    "                   vf = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"])],\n",
    "    activation_fn = act_cls,\n",
    "    log_std_init = cfg[\"model\"][\"log_std_init\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpPolicy\n"
     ]
    }
   ],
   "source": [
    "print(policytype)\n",
    "\n",
    "if optimizer_cls == PPO:\n",
    "    kwa = dict(policy = policytype, \n",
    "                env = env,\n",
    "                tensorboard_log = \"./logs\",\n",
    "                clip_range = cfg[\"model\"][\"clip_range\"],\n",
    "                learning_rate = cfg[\"model\"][\"lr\"],\n",
    "                ent_coef = cfg[\"model\"][\"ent_coef\"],\n",
    "                use_sde = cfg[\"model\"][\"use_sde\"],\n",
    "                batch_size = cfg[\"model\"][\"batch_size\"],\n",
    "                policy_kwargs = policy_kwargs)\n",
    "    \n",
    "else:\n",
    "    kwa = dict(policy = policytype, \n",
    "                env = env,\n",
    "                tensorboard_log = \"./logs\",\n",
    "                batch_size = cfg[\"model\"][\"batch_size\"],\n",
    "                learning_rate = cfg[\"model\"][\"lr\"])\n",
    "\n",
    "model = optimizer_cls(**kwa)\n",
    "\n",
    "if cfg[\"starting_point\"]:\n",
    "    model.set_parameters(cfg[\"starting_point\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If batch_size = 64 and n_steps = 2048, then 1 epoch = 2048/64 = 32 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/simplest/25-36/29/29_0918-1408'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "bin_ = f\"{(cfg['id']//12)*12 +1}-{(cfg['id']//12 +1)*12 }\"\n",
    "entcoef = str(model.ent_coef) if type(model) == PPO else \"\"\n",
    "cliprange = str(model.clip_range(0)) if type(model) == PPO else \"\"\n",
    "model_name = f\"models/simplest/{bin_}/{cfg['id']}/{cfg['id']}_{datetime.datetime.now().strftime('%m%d-%H%M')}\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallbackPPO(BaseCallback):\n",
    "    def __init__(self, schedule_timesteps, start_log_std=2, end_log_std=-1, std_control = cfg[\"clipped_std\"]):\n",
    "        super().__init__(verbose = 0)\n",
    "        self.print_flag = False\n",
    "        self.std_control = std_control\n",
    "        \n",
    "        self.start_log_std = start_log_std\n",
    "        self.end_log_std = end_log_std\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [[]]*4\n",
    "        \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \n",
    "        # self.logger.record(\"train/std\", th.exp(self.model.policy.log_std).mean().item())\n",
    "        self.logger.record(\"penalties/in_out\", sum(self.pen_M)/len(self.pen_M))\n",
    "        self.logger.record(\"penalties/buysell_bounds\", sum(self.pen_B)/len(self.pen_B))\n",
    "        self.logger.record(\"penalties/tank_bounds\", sum(self.pen_P)/len(self.pen_P))\n",
    "        # self.logger.record(\"penalties/regterm\", sum(self.pen_reg)/len(self.pen_reg))\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []\n",
    "        \n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # params = [k for k in self.model.policy.named_parameters()]\n",
    "            \n",
    "        # for j in range(len(params)):\n",
    "        #     if any(th.isnan(params[j][1]).flatten()):\n",
    "        #         print(\"NAN detected\")\n",
    "        #         print(params[j])\n",
    "        #         return(False)\n",
    "        \n",
    "        \n",
    "        log_std: th.Tensor = self.model.policy.log_std\n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        \n",
    "        if self.locals[\"dones\"][0]: # record info at each episode end\n",
    "            self.pen_M.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"M\"])\n",
    "            self.pen_B.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"B\"])\n",
    "            self.pen_P.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"P\"])\n",
    "        \n",
    "        if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "            self.print_flag = True\n",
    "            \n",
    "        if self.print_flag:\n",
    "            print(\"\\nt:\", t)\n",
    "            if np.isnan(self.locals['rewards'][0]) or np.isinf(self.locals['rewards'][0]):\n",
    "                print(f\"is invalid reward {self.locals['rewards'][0]}\")\n",
    "            for i in ['obs_tensor', 'actions', 'values', 'clipped_actions', 'new_obs', 'rewards']:\n",
    "                if i in self.locals:\n",
    "                    print(f\"{i}: \" + str(self.locals[i]))\n",
    "            if t == 6:\n",
    "                self.print_flag = False\n",
    "                print(f\"\\n\\nLog-Std at step {self.num_timesteps}: {log_std.detach().numpy()}\")\n",
    "                print(\"\\n\\n\\n\\n\\n\")\n",
    "                \n",
    "        if self.std_control:\n",
    "            progress = self.current_step / self.schedule_timesteps\n",
    "            new_log_std = self.start_log_std + progress * (self.end_log_std - self.start_log_std)\n",
    "            self.model.policy.log_std.data.fill_(new_log_std)\n",
    "            self.current_step += 1\n",
    "                \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallbackDDPG(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.print_flag = False\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []\n",
    "        \n",
    "    def _on_rollout_end(self) -> None: ...\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        \n",
    "        if self.locals[\"dones\"][0]: # record info at each episode end\n",
    "            self.pen_M.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"M\"])\n",
    "            self.pen_B.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"B\"])\n",
    "            self.pen_P.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"P\"])\n",
    "            self.pen_reg.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"reg\"])\n",
    "            \n",
    "            self.total_rewards.append(self.locals['rewards'][0])\n",
    "            \n",
    "        \n",
    "        if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "            self.print_flag = True\n",
    "            \n",
    "        if self.print_flag:\n",
    "            print(\"\\nt:\", t)\n",
    "            if np.isnan(self.locals['rewards'][0]) or np.isinf(self.locals['rewards'][0]):\n",
    "                print(f\"is invalid reward {self.locals['rewards'][0]}\")\n",
    "            for i in ['obs_tensor', 'actions', 'values', 'new_obs', 'rewards']:\n",
    "                if i in self.locals:\n",
    "                    print(f\"{i}: \" + str(self.locals[i]))\n",
    "            if t == 6:\n",
    "                self.print_flag = False\n",
    "                # print(f\"\\nAvg rewards over the last 100 episodes:{sum(self.total_rewards[-100:])/100} ; last reward: {self.total_rewards[-1]}\")\n",
    "                \n",
    "                self.logger.record('train/learning_rate', self.model.learning_rate)\n",
    "                self.logger.record(\"penalties/in_out\", sum(self.pen_M)/len(self.pen_M))\n",
    "                self.logger.record(\"penalties/buysell_bounds\", sum(self.pen_B)/len(self.pen_B))\n",
    "                self.logger.record(\"penalties/tank_bounds\", sum(self.pen_P)/len(self.pen_P))\n",
    "                self.logger.record(\"penalties/regterm\", sum(self.pen_reg)/len(self.pen_reg))\n",
    "        \n",
    "                self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []   \n",
    "                \n",
    "                print(\"\\n\\n\\n\\n\\n\")\n",
    "                \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/simplest/25-36/29/29_0918-1408'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_timesteps = 1e5\n",
    "log_callback = CustomLoggingCallbackPPO(schedule_timesteps=total_timesteps) if optimizer_cls == PPO else CustomLoggingCallbackDDPG()\n",
    "callback = CallbackList([log_callback])\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Higher batch size, no STD control (25)\n",
    "- making the env progressively harder (26)\n",
    "- harder env (no incentive) from scratch (27)\n",
    "- higher penalties for contraints (28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging at simplest/13-24/23/23_0916-1521\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.7422, 14.5545, 14.6201,  0.0000,\n",
      "         15.1697, 15.2425, 17.4497, 12.5866, 14.7327,  0.0000, 18.2978, 13.2086,\n",
      "          0.0000]])\n",
      "actions: [[ 0.02349485 -0.30684718  0.07184143 -0.1634581 ]]\n",
      "values: tensor([[-1.0350]])\n",
      "clipped_actions: [[0.02349485 0.         0.07184143 0.        ]]\n",
      "new_obs: [[ 0.04834658  0.02349485  0.          0.06       14.620074    0.\n",
      "  15.169688   15.242549   17.449656   12.586577   14.732715    0.\n",
      "  18.297802   13.208551   11.476386    0.          1.        ]]\n",
      "rewards: [-2.814742]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0483,  0.0235,  0.0000,  0.0600, 14.6201,  0.0000, 15.1697, 15.2425,\n",
      "         17.4497, 12.5866, 14.7327,  0.0000, 18.2978, 13.2086, 11.4764,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 2.4307098 -7.8670273 -8.713527   3.757355 ]]\n",
      "values: tensor([[-0.7191]])\n",
      "clipped_actions: [[2.4307098 0.        0.        3.757355 ]]\n",
      "new_obs: [[ 0.          0.07184143  0.          0.10037775 15.169688   15.242549\n",
      "  17.449656   12.586577   14.732715    0.         18.297802   13.208551\n",
      "  11.476386    0.          0.         15.814357    2.        ]]\n",
      "rewards: [-2.0000815]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0718,  0.0000,  0.1004, 15.1697, 15.2425, 17.4497, 12.5866,\n",
      "         14.7327,  0.0000, 18.2978, 13.2086, 11.4764,  0.0000,  0.0000, 15.8144,\n",
      "          2.0000]])\n",
      "actions: [[14.113202    0.45522946  6.907154    8.292726  ]]\n",
      "values: tensor([[1.1062]])\n",
      "clipped_actions: [[14.113202    0.45522946  6.907154    8.292726  ]]\n",
      "new_obs: [[ 0.          0.07184143  0.          0.10037775 17.449656   12.586577\n",
      "  14.732715    0.         18.297802   13.208551   11.476386    0.\n",
      "   0.         15.814357   15.423427   15.730383    3.        ]]\n",
      "rewards: [-1.9260623]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0718,  0.0000,  0.1004, 17.4497, 12.5866, 14.7327,  0.0000,\n",
      "         18.2978, 13.2086, 11.4764,  0.0000,  0.0000, 15.8144, 15.4234, 15.7304,\n",
      "          3.0000]])\n",
      "actions: [[ 13.4363985   3.6170664 -10.128906  -13.315294 ]]\n",
      "values: tensor([[1.2355]])\n",
      "clipped_actions: [[13.4363985  3.6170664  0.         0.       ]]\n",
      "new_obs: [[ 0.          0.          0.07184143  0.         14.732715    0.\n",
      "  18.297802   13.208551   11.476386    0.          0.         15.814357\n",
      "  15.423427   15.730383   20.091785   13.165531    4.        ]]\n",
      "rewards: [-1.4545724]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0718,  0.0000, 14.7327,  0.0000, 18.2978, 13.2086,\n",
      "         11.4764,  0.0000,  0.0000, 15.8144, 15.4234, 15.7304, 20.0918, 13.1655,\n",
      "          4.0000]])\n",
      "actions: [[-15.464928    5.632007  -10.943532    1.9446455]]\n",
      "values: tensor([[0.0694]])\n",
      "clipped_actions: [[0.        5.632007  0.        1.9446455]]\n",
      "new_obs: [[ 0.          0.          0.07184143  0.         18.297802   13.208551\n",
      "  11.476386    0.          0.         15.814357   15.423427   15.730383\n",
      "  20.091785   13.165531    0.         16.140884    5.        ]]\n",
      "rewards: [-1.0789483]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0718,  0.0000, 18.2978, 13.2086, 11.4764,  0.0000,\n",
      "          0.0000, 15.8144, 15.4234, 15.7304, 20.0918, 13.1655,  0.0000, 16.1409,\n",
      "          5.0000]])\n",
      "actions: [[ 7.558914   -2.5525143  -8.836484    0.66140044]]\n",
      "values: tensor([[-0.1037]])\n",
      "clipped_actions: [[7.558914   0.         0.         0.66140044]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.742218 14.554528 14.620074\n",
      "   0.       15.169688 15.242549 17.449656 12.586577 14.732715  0.\n",
      "  18.297802 13.208551  0.      ]]\n",
      "rewards: [-0.83523613]\n",
      "\n",
      "\n",
      "Log-Std at step 6: [1.99988 1.99988 1.99988 1.99988]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.7422, 14.5545, 14.6201,  0.0000,\n",
      "         15.1697, 15.2425, 17.4497, 12.5866, 14.7327,  0.0000, 18.2978, 13.2086,\n",
      "          0.0000]])\n",
      "actions: [[ 3.2501452  9.941311  -3.0076709 -3.740257 ]]\n",
      "values: tensor([[-0.0363]])\n",
      "clipped_actions: [[3.2501452 9.941311  0.        0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.620074  0.       15.169688\n",
      "  15.242549 17.449656 12.586577 14.732715  0.       18.297802 13.208551\n",
      "  11.476386  0.        1.      ]]\n",
      "rewards: [-0.06231113]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.6201,  0.0000, 15.1697, 15.2425,\n",
      "         17.4497, 12.5866, 14.7327,  0.0000, 18.2978, 13.2086, 11.4764,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 1.9761988 -0.4261992  6.4055386  7.787693 ]]\n",
      "values: tensor([[-0.1371]])\n",
      "clipped_actions: [[1.9761988 0.        6.4055386 7.787693 ]]\n",
      "new_obs: [[ 4.42934    1.9761988  0.         0.06      15.169688  15.242549\n",
      "  17.449656  12.586577  14.732715   0.        18.297802  13.208551\n",
      "  11.476386   0.         0.        15.814357   2.       ]]\n",
      "rewards: [-0.04689628]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 4.4293,  1.9762,  0.0000,  0.0600, 15.1697, 15.2425, 17.4497, 12.5866,\n",
      "         14.7327,  0.0000, 18.2978, 13.2086, 11.4764,  0.0000,  0.0000, 15.8144,\n",
      "          2.0000]])\n",
      "actions: [[-7.15055     0.33824623  0.9369752   7.927718  ]]\n",
      "values: tensor([[0.1932]])\n",
      "clipped_actions: [[0.         0.33824623 0.9369752  7.927718  ]]\n",
      "new_obs: [[ 5.366315    1.6379526   0.          0.04760967 17.449656   12.586577\n",
      "  14.732715    0.         18.297802   13.208551   11.476386    0.\n",
      "   0.         15.814357   15.423427   15.730383    3.        ]]\n",
      "rewards: [0.06895837]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 5.3663,  1.6380,  0.0000,  0.0476, 17.4497, 12.5866, 14.7327,  0.0000,\n",
      "         18.2978, 13.2086, 11.4764,  0.0000,  0.0000, 15.8144, 15.4234, 15.7304,\n",
      "          3.0000]])\n",
      "actions: [[-2.457571  -4.4069595  7.724573   0.8863061]]\n",
      "values: tensor([[-0.0856]])\n",
      "clipped_actions: [[0.        0.        7.724573  0.8863061]]\n",
      "new_obs: [[13.090888    1.6379526   0.          0.04760967 14.732715    0.\n",
      "  18.297802   13.208551   11.476386    0.          0.         15.814357\n",
      "  15.423427   15.730383   20.091785   13.165531    4.        ]]\n",
      "rewards: [0.08950033]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[13.0909,  1.6380,  0.0000,  0.0476, 14.7327,  0.0000, 18.2978, 13.2086,\n",
      "         11.4764,  0.0000,  0.0000, 15.8144, 15.4234, 15.7304, 20.0918, 13.1655,\n",
      "          4.0000]])\n",
      "actions: [[ 6.534575   -7.3410707   0.18872648 -0.9141851 ]]\n",
      "values: tensor([[-0.2975]])\n",
      "clipped_actions: [[6.534575   0.         0.18872648 0.        ]]\n",
      "new_obs: [[ 6.7450395   8.172527    0.          0.09558436 18.297802   13.208551\n",
      "  11.476386    0.          0.         15.814357   15.423427   15.730383\n",
      "  20.091785   13.165531    0.         16.140884    5.        ]]\n",
      "rewards: [0.11259786]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 6.7450,  8.1725,  0.0000,  0.0956, 18.2978, 13.2086, 11.4764,  0.0000,\n",
      "          0.0000, 15.8144, 15.4234, 15.7304, 20.0918, 13.1655,  0.0000, 16.1409,\n",
      "          5.0000]])\n",
      "actions: [[ 9.556181   0.3844689 -2.9312153  2.4746492]]\n",
      "values: tensor([[0.2173]])\n",
      "clipped_actions: [[9.556181  0.3844689 0.        2.4746492]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.742218 14.554528 14.620074\n",
      "   0.       15.169688 15.242549 17.449656 12.586577 14.732715  0.\n",
      "  18.297802 13.208551  0.      ]]\n",
      "rewards: [0.02602811]\n",
      "\n",
      "\n",
      "Log-Std at step 2058: [1.93832 1.93832 1.93832 1.93832]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.7422, 14.5545, 14.6201,  0.0000,\n",
      "         15.1697, 15.2425, 17.4497, 12.5866, 14.7327,  0.0000, 18.2978, 13.2086,\n",
      "          0.0000]])\n",
      "actions: [[  2.2031488  -2.1158066  -4.34332   -10.633302 ]]\n",
      "values: tensor([[-0.6427]])\n",
      "clipped_actions: [[2.2031488 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.620074  0.       15.169688\n",
      "  15.242549 17.449656 12.586577 14.732715  0.       18.297802 13.208551\n",
      "  11.476386  0.        1.      ]]\n",
      "rewards: [-0.01867254]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.6201,  0.0000, 15.1697, 15.2425,\n",
      "         17.4497, 12.5866, 14.7327,  0.0000, 18.2978, 13.2086, 11.4764,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 0.8748214 -2.1348207 -1.8061268 17.224365 ]]\n",
      "values: tensor([[0.0350]])\n",
      "clipped_actions: [[ 0.8748214  0.         0.        17.224365 ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.169688 15.242549 17.449656\n",
      "  12.586577 14.732715  0.       18.297802 13.208551 11.476386  0.\n",
      "   0.       15.814357  2.      ]]\n",
      "rewards: [-0.04026457]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.1697, 15.2425, 17.4497, 12.5866,\n",
      "         14.7327,  0.0000, 18.2978, 13.2086, 11.4764,  0.0000,  0.0000, 15.8144,\n",
      "          2.0000]])\n",
      "actions: [[18.595886  -7.8068676 -7.9645286  3.644909 ]]\n",
      "values: tensor([[-0.3504]])\n",
      "clipped_actions: [[18.595886  0.        0.        3.644909]]\n",
      "new_obs: [[ 0.        0.        0.        0.       17.449656 12.586577 14.732715\n",
      "   0.       18.297802 13.208551 11.476386  0.        0.       15.814357\n",
      "  15.423427 15.730383  3.      ]]\n",
      "rewards: [-0.13877676]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 17.4497, 12.5866, 14.7327,  0.0000,\n",
      "         18.2978, 13.2086, 11.4764,  0.0000,  0.0000, 15.8144, 15.4234, 15.7304,\n",
      "          3.0000]])\n",
      "actions: [[ 2.444944  -2.3870974  7.215815   3.8034432]]\n",
      "values: tensor([[-0.7565]])\n",
      "clipped_actions: [[2.444944  0.        7.215815  3.8034432]]\n",
      "new_obs: [[ 4.770871  2.444944  0.        0.06     14.732715  0.       18.297802\n",
      "  13.208551 11.476386  0.        0.       15.814357 15.423427 15.730383\n",
      "  20.091785 13.165531  4.      ]]\n",
      "rewards: [-0.11658569]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 4.7709,  2.4449,  0.0000,  0.0600, 14.7327,  0.0000, 18.2978, 13.2086,\n",
      "         11.4764,  0.0000,  0.0000, 15.8144, 15.4234, 15.7304, 20.0918, 13.1655,\n",
      "          4.0000]])\n",
      "actions: [[-13.155528  11.770957   2.075131  -9.831667]]\n",
      "values: tensor([[-0.0132]])\n",
      "clipped_actions: [[ 0.       11.770957  2.075131  0.      ]]\n",
      "new_obs: [[ 6.846002  0.        2.444944  0.       18.297802 13.208551 11.476386\n",
      "   0.        0.       15.814357 15.423427 15.730383 20.091785 13.165531\n",
      "   0.       16.140884  5.      ]]\n",
      "rewards: [-0.12534995]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 6.8460,  0.0000,  2.4449,  0.0000, 18.2978, 13.2086, 11.4764,  0.0000,\n",
      "          0.0000, 15.8144, 15.4234, 15.7304, 20.0918, 13.1655,  0.0000, 16.1409,\n",
      "          5.0000]])\n",
      "actions: [[-14.354935   -6.7411337  -2.7221894  -3.5011764]]\n",
      "values: tensor([[-0.1994]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.742218 14.554528 14.620074\n",
      "   0.       15.169688 15.242549 17.449656 12.586577 14.732715  0.\n",
      "  18.297802 13.208551  0.      ]]\n",
      "rewards: [-0.12536125]\n",
      "\n",
      "\n",
      "Log-Std at step 4104: [1.87694 1.87694 1.87694 1.87694]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m logpath \u001b[38;5;241m=\u001b[39m model_name[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m):]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogging at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:179\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 179\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\stable_baselines3\\common\\policies.py:647\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    645\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 647\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    649\u001b[0m     pi_features, vf_features \u001b[38;5;241m=\u001b[39m features\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:222\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[th\u001b[38;5;241m.\u001b[39mTensor, th\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    :return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m        If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_critic(features)\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:225\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv2\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logpath = model_name[len(\"models/\"):]\n",
    "print(f\"logging at {logpath}\")\n",
    "model.learn(total_timesteps = total_timesteps,\n",
    "            progress_bar = False,\n",
    "            tb_log_name = logpath,\n",
    "            callback = callback,\n",
    "            reset_num_timesteps = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def save_next_file(directory, model_name):\n",
    "    base_pattern = re.compile(model_name + r\"_(\\d+)\\.zip\")\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    max_number = 0\n",
    "    for file in files:\n",
    "        match = base_pattern.match(file)\n",
    "        if match:\n",
    "            number = int(match.group(1))\n",
    "            max_number = max(max_number, number)\n",
    "    \n",
    "    # Generate the next filename\n",
    "    next_file_number = max_number + 1\n",
    "    next_file_name = f\"{model_name}_{next_file_number}\"\n",
    "    next_file_path = os.path.join(directory, next_file_name)\n",
    "    \n",
    "    model.save(next_file_path)\n",
    "    \n",
    "save_next_file(os.path.dirname(model_name), os.path.basename(model_name) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models\\\\simplest\\\\25-36\\\\29\\\\29_0918-1503.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M,Q,P,B,Z,D = 10, 0, 5, 5, 1, 0\n",
    "M, Q, P, B, Z, D  = cfg[\"env\"][\"M\"], cfg[\"env\"][\"Q\"], cfg[\"env\"][\"P\"], cfg[\"env\"][\"B\"], cfg[\"env\"][\"Z\"], 0\n",
    "# M,Q,P,B,Z,D = 0, 0, 0, 0, 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"env\"][\"uniform_data\"]:\n",
    "    tau0   = {s: [np.random.binomial(1, 0.7) * np.random.normal(15, 2) for _ in range(13)] for s in sources}\n",
    "    delta0 = {d: [np.random.binomial(1, 0.7) * np.random.normal(15, 2) for _ in range(13)] for d in demands}\n",
    "else:\n",
    "    tau0   = {s: [10, 10, 10, 0, 0, 0] for s in sources}\n",
    "    delta0 = {d: [0, 0, 0, 10, 10, 10] for d in demands}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = True, \n",
    "               D = cfg[\"env\"][\"D\"], \n",
    "               Q = cfg[\"env\"][\"Q\"], \n",
    "               P = cfg[\"env\"][\"P\"], \n",
    "               B = cfg[\"env\"][\"B\"], \n",
    "               Z = cfg[\"env\"][\"Z\"], \n",
    "               M = cfg[\"env\"][\"M\"],\n",
    "               reg = cfg[\"env\"][\"reg\"],\n",
    "               reg_lambda = cfg[\"env\"][\"reg_lambda\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"],\n",
    "               alpha = cfg[\"env\"][\"alpha\"],\n",
    "               beta = cfg[\"env\"][\"beta\"],\n",
    "               connections = connections, \n",
    "               action_sample = action_sample,\n",
    "               tau0 = tau0,delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " {'M': 0, 'B': 0, 'P': 0, 'Q': 0, 'reg': 0}\n",
      "[ 0.        0.       11.364664  0.      ]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 11.364664}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t1; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "[ 0.        0.        0.        0.       14.644503 15.507391 17.358292\n",
      " 14.621485 12.706727 17.684172  0.       16.69194  12.181685 13.330943\n",
      " 12.561095 13.936094  1.      ]\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 0.0} {'p1': 0.0}\n",
      "    -100.0\n",
      "\n",
      "\n",
      " {'M': 0, 'B': -100, 'P': 0, 'Q': 0, 'reg': 0}\n",
      "[ 0.        0.       10.162529  0.      ]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 10.162529}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 10.162528991699219 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "[10.162529  0.        0.        0.       17.358292 14.621485 12.706727\n",
      " 17.684172  0.       16.69194  12.181685 13.330943 12.561095 13.936094\n",
      "  0.        0.        2.      ]\n",
      "\n",
      "    >>      {'s1': 10.162529} {'j1': 0.0} {'p1': 0.0}\n",
      "    -89.83747100830078\n",
      "\n",
      "\n",
      " {'M': 0, 'B': -100, 'P': 0, 'Q': 0, 'reg': 0}\n",
      "[5.129453 0.       5.009461 0.      ]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 5.129453}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 5.009461}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 5.129453182220459, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 10.258906364440918 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "[10.042537  5.129453  0.        0.06     12.706727 17.684172  0.\n",
      " 16.69194  12.181685 13.330943 12.561095 13.936094  0.        0.\n",
      "  0.       13.259562  3.      ]\n",
      "\n",
      "    >>      {'s1': 10.042537} {'j1': 5.129453} {'p1': 0.0}\n",
      "    -84.80801782608032\n",
      "\n",
      "\n",
      " {'M': 0, 'B': -100, 'P': 0, 'Q': 0, 'reg': 0}\n",
      "[0.       0.       2.831287 0.      ]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 2.831287}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 2.831286907196045 through tank population in s1\n",
      "j1: inv: 5.129453182220459, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "[12.873823  5.129453  0.        0.06      0.       16.69194  12.181685\n",
      " 13.330943 12.561095 13.936094  0.        0.        0.       13.259562\n",
      " 12.086735 15.473319  4.      ]\n",
      "\n",
      "    >>      {'s1': 12.873823} {'j1': 5.129453} {'p1': 0.0}\n",
      "    -81.97673091888427\n",
      "\n",
      "\n",
      " {'M': 0, 'B': -100, 'P': 0, 'Q': 0, 'reg': 0}\n",
      "[9.350929 0.       0.       0.      ]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 9.350929}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 5.129453182220459, in_flow_sources: 9.350929260253906, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 18.701858520507812 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "[ 3.5228944  14.480383    0.          0.09874592 12.181685   13.330943\n",
      " 12.561095   13.936094    0.          0.          0.         13.259562\n",
      " 12.086735   15.473319   14.906733    0.          5.        ]\n",
      "\n",
      "    >>      {'s1': 3.5228944} {'j1': 14.480383} {'p1': 0.0}\n",
      "    -72.72580165863036\n",
      "\n",
      "\n",
      " {'M': 0, 'B': -100, 'P': 0, 'Q': 0, 'reg': 0}\n",
      "[ 0.       10.562005  0.        8.750221]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 10.562005}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 8.750221}}\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 14.480382442474365, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 10.562005043029785\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 7.247135162353516 through tank population in p1\n",
      "[PEN] t6; p1; q1; j1:\t\t\tSold qualities out of bounds (-0.17)\n",
      "[ 3.5228944   3.9183774   1.8117838  -0.16742419 12.561095   13.936094\n",
      "  0.          0.          0.         13.259562   12.086735   15.473319\n",
      " 14.906733    0.          0.          0.          6.        ]\n",
      "\n",
      "    >>      {'s1': 3.5228944} {'j1': 3.9183774} {'p1': 1.8117838}\n",
      "    798.881453704834\n"
     ]
    }
   ],
   "source": [
    "with th.autograd.set_detect_anomaly(True):\n",
    "    obs = env.reset()\n",
    "    obs, obs_dict = obs\n",
    "    for k in range(env.T):\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        print(\"\\n\\n\",env.pen_tracker)\n",
    "        print(action)\n",
    "        print(\"\\n\\n   \",reconstruct_dict(action, env.mapping_act))\n",
    "        obs, reward, done, term, _ = env.step(action)\n",
    "        print(obs)\n",
    "        dobs = reconstruct_dict(obs, env.mapping_obs)\n",
    "        print(\"\\n    >>     \",dobs[\"sources\"], dobs[\"blenders\"], dobs[\"demands\"])\n",
    "        print(\"   \" ,reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0149, 0.0122, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.Tensor(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 (only once per episode)\n",
    "episode_rewards = []\n",
    "obs = env.reset()\n",
    "obs, obs_dict = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 1 Get first action\n",
    "print(env.t)\n",
    "action, _ = model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'s1': 17.46205}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_blend': {'s1': {'j1': 0.0}},\n",
       " 'blend_demand': {'j1': {'p1': 30.307917}},\n",
       " 'tau': {'s1': 8.731916},\n",
       " 'delta': {'p1': 17.08481}}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Visualize action\n",
    "print(env.t)\n",
    "reconstruct_dict(action, env.mapping_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Step once: get 2nd action\n",
    "print(env.t)\n",
    "obs, reward, done, term, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'s1': 26.193966}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# 4 Visualize new state\n",
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error [from server]: \n****************************************************************\n****************************************************************\n****************************************************************\n\nERROR: TensorBoard.dev has been shut down.\n\nThis command is no longer operational and will be removed.\n\nSee the FAQ at https://tensorboard.dev.\n\n****************************************************************\n****************************************************************\n****************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc1KCv3X3QvGwaXfgX1c4tg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[43mtb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExperimentFromDev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39mget_scalars()\n\u001b[0;32m      4\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\tensorboard\\data\\experimental\\experiment_from_dev.py:65\u001b[0m, in \u001b[0;36mExperimentFromDev.__init__\u001b[1;34m(self, experiment_id, api_endpoint)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_id \u001b[38;5;241m=\u001b[39m experiment_id\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client \u001b[38;5;241m=\u001b[39m \u001b[43mget_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_endpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\tensorboard\\data\\experimental\\experiment_from_dev.py:129\u001b[0m, in \u001b[0;36mget_api_client\u001b[1;34m(api_endpoint)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_api_client\u001b[39m(api_endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m     server_info \u001b[38;5;241m=\u001b[39m _get_server_info(api_endpoint\u001b[38;5;241m=\u001b[39mapi_endpoint)\n\u001b[1;32m--> 129\u001b[0m     \u001b[43m_handle_server_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     channel_creds \u001b[38;5;241m=\u001b[39m grpc\u001b[38;5;241m.\u001b[39mssl_channel_credentials()\n\u001b[0;32m    131\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m auth\u001b[38;5;241m.\u001b[39mCredentialsStore()\u001b[38;5;241m.\u001b[39mread_credentials()\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\tensorboard\\data\\experimental\\experiment_from_dev.py:159\u001b[0m, in \u001b[0;36m_handle_server_info\u001b[1;34m(info)\u001b[0m\n\u001b[0;32m    157\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mverdict \u001b[38;5;241m==\u001b[39m server_info_pb2\u001b[38;5;241m.\u001b[39mVERDICT_ERROR:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError [from server]: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m compat\u001b[38;5;241m.\u001b[39mdetails)\n",
      "\u001b[1;31mValueError\u001b[0m: Error [from server]: \n****************************************************************\n****************************************************************\n****************************************************************\n\nERROR: TensorBoard.dev has been shut down.\n\nThis command is no longer operational and will be removed.\n\nSee the FAQ at https://tensorboard.dev.\n\n****************************************************************\n****************************************************************\n****************************************************************\n"
     ]
    }
   ],
   "source": [
    "experiment_id = \"c1KCv3X3QvGwaXfgX1c4tg\"\n",
    "experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "df = experiment.get_scalars()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blendv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
