{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN policy ?\n",
    "- grid search for HP tuning (OK)\n",
    "- Increasingly difficult Environment\n",
    "- Positive reward for populating increasingly \"deep\" blending tanks ?\n",
    "- RL for chem sched paper (https://arxiv.org/pdf/2203.00636)\n",
    "- Masking (https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html, https://arxiv.org/pdf/2006.14171)\n",
    "    - Adding binary decision variables ?g  \n",
    "    - Requires discrete action space (only integer flows -> treated as categories ?)\n",
    "    - masking: disable incoming flows (resp. outgoing flows) for tanks at UB inv limit (resp. LB inv. limit), disable selling/buying when available = 0\n",
    "    - multiple envs with multiple agents ? (MARL, https://arxiv.org/pdf/2103.01955)\n",
    "        - Predict successive pipelines (\"source > blend\" then \"blend > blend\" (as many as required) then \"blend > demand\")\n",
    "        - Each agent has access to the whole state\n",
    "        - Action mask is derived from the previous agent's actions (0 if inventory at bounds or incoming flow already reserved, else 1)\n",
    "        - https://github.com/Rohan138/marl-baselines3/blob/main/marl_baselines3/independent_ppo.py\n",
    "- Safe RL: (https://proceedings.mlr.press/v119/wachi20a/wachi20a.pdf)\n",
    "    - \"Unsafe state\" ? > Do not enforce constraints strictly, instead opt for early episode termination to show which states are unsafe ? \n",
    "    - Implementations:\n",
    "        - https://pypi.org/project/fast-safe-rl/#description (Policy optimizers)\n",
    "        - https://github.com/PKU-Alignment/safety-gymnasium/tree/main/safety_gymnasium (environments; \"cost\" ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try other learning rates/CNN policies\n",
    "2. Implement Masking with single agent\n",
    "3. Try other ways to tell the model what are illegal/unsafe states (safe RL)\n",
    "4. Try multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masking: Discretization of action space is too slow/might not work -> Need to implement masking for continuous action space\n",
    "- Recurrent policy makes the most sense ? (window of demand forecasts)\n",
    "- https://www.reddit.com/r/reinforcementlearning/comments/17l5b47/invalid_action_masking_when_action_space_is/\n",
    "    - Suggestion of autoregressive model for having constraints respected: one predicted action is input to a second model\n",
    "    - Suggestion of editing the distribution in such a way that the constraint is respected\n",
    "- https://www.sciencedirect.com/science/article/pii/S0098135420301599\n",
    "    - Choice of ELU activation ?\n",
    "    - Choice of NN size ?\n",
    "    - \"The feature engineering in the net inventory means the network does not have to learn these relationships itself, which did help speed training.\" ?\n",
    "- Simplify the problem (remove tanks 5 to 8), find the optimal solution with Gurobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove all constraints except in/out\n",
    "- https://arxiv.org/pdf/1711.11157\n",
    "- https://arxiv.org/pdf/2111.01564\n",
    "- Softmax with large coef to produce action mask\n",
    "- Graph convolution NN instead of RNN ?\n",
    "    - https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "    - Graph rep. learning - William L Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPG\n",
    "- Softmax\n",
    "- ~~Remove non-selling rewards~~\n",
    "- MultiplexNet\n",
    "- Why softmax doesn't work ? -> gradient doesn't compute properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finalize adjustment of flows\n",
    "- Add more difficulty (bigger env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Added penalty types logging\n",
    "- Added regularization, no success\n",
    "- Tried SDE briefly, no success\n",
    "- Hard/Unable to reproduce results on best configs (3 and 12)\n",
    "\n",
    "Next steps:\n",
    "- Find non-trivial optimal solution\n",
    "- Need different env versions to compare how flows are processed and the impact on performance\n",
    "- Read SDE method & distribution \n",
    "- Generate random supply/demands and solutions with solver; train classic NN in a supervised fashion (loss function = reward function ? ; multi-period aspect ?)\n",
    "- \"Hire\" another student ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "try:\n",
    "    print(curr_dir)\n",
    "except:\n",
    "    curr_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "    os.chdir(curr_dir)\n",
    "    print(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\gymnasium\\envs\\registration.py:604: UserWarning: \u001b[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\gymnasium\\envs\\registration.py\", line 602, in load_plugin_envs\n",
      "    fn()\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\shimmy\\registration.py\", line 303, in register_gymnasium_envs\n",
      "    _register_dm_control_envs()\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\shimmy\\registration.py\", line 63, in _register_dm_control_envs\n",
      "    from shimmy.dm_control_compatibility import DmControlCompatibilityV0\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\shimmy\\dm_control_compatibility.py\", line 16, in <module>\n",
      "    from dm_control import composer\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\composer\\__init__.py\", line 18, in <module>\n",
      "    from dm_control.composer.arena import Arena\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\composer\\arena.py\", line 20, in <module>\n",
      "    from dm_control import mjcf\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\mjcf\\__init__.py\", line 18, in <module>\n",
      "    from dm_control.mjcf.attribute import Asset\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\mjcf\\attribute.py\", line 28, in <module>\n",
      "    from dm_control.mujoco.wrapper import util\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\mujoco\\__init__.py\", line 18, in <module>\n",
      "    from dm_control.mujoco.engine import action_spec\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\mujoco\\engine.py\", line 42, in <module>\n",
      "    from dm_control.mujoco import index\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\mujoco\\index.py\", line 88, in <module>\n",
      "    from dm_control.mujoco.wrapper import util\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\mujoco\\wrapper\\__init__.py\", line 18, in <module>\n",
      "    from dm_control.mujoco.wrapper import mjbindings\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\mujoco\\wrapper\\mjbindings\\__init__.py\", line 29, in <module>\n",
      "    from dm_control.mujoco.wrapper.mjbindings import functions\n",
      "  File \"c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\dm_control\\mujoco\\wrapper\\mjbindings\\functions.py\", line 21, in <module>\n",
      "    import mujoco\n",
      "ModuleNotFoundError: No module named 'mujoco'\n",
      "\u001b[0m\n",
      "  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO, DDPG, SAC, TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import *\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
    "\n",
    "from envs import BlendEnv, flatten_and_track_mappings, reconstruct_dict\n",
    "from models import *\n",
    "from math import exp, log\n",
    "import yaml\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./configs/23.yaml\", \"r\") as f:\n",
    "    s = \"\".join(f.readlines())\n",
    "    cfg = yaml.load(s, Loader=yaml.FullLoader)\n",
    "    \n",
    "layout = \"simplest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"custom_softmax\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_softmax\n",
    "elif cfg[\"policytype\"] == \"MLP\":\n",
    "    policytype = \"MlpPolicy\"\n",
    "elif cfg[\"policytype\"] == \"MLPtanh\":\n",
    "    policytype = CustomMLP_ACP_simplest_tanh\n",
    "    \n",
    "optimizer_cls = eval(cfg[\"optimizer\"])\n",
    "\n",
    "if cfg[\"model\"][\"act_fn\"] == \"ReLU\":\n",
    "    act_cls = th.nn.ReLU\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"tanh\":\n",
    "    act_cls = th.nn.Tanh\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"sigmoid\":\n",
    "    act_cls = th.nn.Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./configs/json/connections_{layout}.json\" ,\"r\") as f:\n",
    "    connections_s = f.readline()\n",
    "connections = json.loads(connections_s)\n",
    "\n",
    "with open(f\"./configs/json/action_sample_{layout}.json\" ,\"r\") as f:\n",
    "    action_sample_s = f.readline()\n",
    "action_sample = json.loads(action_sample_s)\n",
    "action_sample_flat, mapp = flatten_and_track_mappings(action_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbp(connections):\n",
    "    sources = list(connections[\"source_blend\"].keys())\n",
    "    \n",
    "    b_list = list(connections[\"blend_blend\"].keys())\n",
    "    for b in connections[\"blend_blend\"].keys():\n",
    "        b_list += connections[\"blend_blend\"][b]\n",
    "    b_list += list(connections[\"blend_demand\"].keys())\n",
    "    blenders = list(set(b_list))\n",
    "    \n",
    "    p_list = []\n",
    "    for p in connections[\"blend_demand\"].keys():\n",
    "        p_list += connections[\"blend_demand\"][p]\n",
    "    demands = list(set(p_list))\n",
    "    \n",
    "    return sources, blenders, demands\n",
    "sources, blenders, demands = get_sbp(connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 6\n",
    "if layout == \"base\":\n",
    "    sigma = {\"s1\":{\"q1\": 0.06}, \"s2\":{\"q1\": 0.26}}\n",
    "    sigma_ub = {\"p1\":{\"q1\": 0.16}, \"p2\":{\"q1\": 1}}\n",
    "    sigma_lb = {\"p1\":{\"q1\": 0}, \"p2\":{\"q1\": 0}}\n",
    "else:\n",
    "    sigma = {s:{\"q1\": 0.06} for s in sources}\n",
    "    sigma_ub = {d:{\"q1\": 0.16} for d in demands}\n",
    "    sigma_lb = {d:{\"q1\": 0} for d in demands}\n",
    "    \n",
    "s_inv_lb = {s: 0 for s in sources}\n",
    "s_inv_ub = {s: 999 for s in sources}\n",
    "d_inv_lb = {d: 0 for d in demands}\n",
    "d_inv_ub = {d: 999 for d in demands}\n",
    "betaT_d = {d: 1 for d in demands} # Price of sold products\n",
    "b_inv_ub = {j: 30 for j in blenders} \n",
    "b_inv_lb = {j: 0 for j in blenders}\n",
    "betaT_s = {s: cfg[\"env\"][\"product_cost\"]  for s in sources} # Cost of bought products\n",
    "\n",
    "if cfg[\"env\"][\"uniform_data\"]:\n",
    "    tau0   = {s: [np.random.binomial(1, 0.7) * np.random.normal(15, 2) for _ in range(13)] for s in sources}\n",
    "    delta0 = {d: [np.random.binomial(1, 0.7) * np.random.normal(15, 2) for _ in range(13)] for d in demands}\n",
    "else:\n",
    "    tau0   = {s: [10, 10, 10, 0, 0, 0] for s in sources}\n",
    "    delta0 = {d: [0, 0, 0, 10, 10, 10] for d in demands}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = False, T = T,\n",
    "               D = cfg[\"env\"][\"D\"], Q = cfg[\"env\"][\"Q\"], P = cfg[\"env\"][\"P\"], B = cfg[\"env\"][\"B\"], Z = cfg[\"env\"][\"Z\"], M = cfg[\"env\"][\"M\"],\n",
    "               reg = cfg[\"env\"][\"reg\"], reg_lambda = cfg[\"env\"][\"reg_lambda\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"], alpha = cfg[\"env\"][\"alpha\"], \n",
    "               beta = cfg[\"env\"][\"beta\"], connections = connections, \n",
    "               action_sample = action_sample, tau0 = tau0, delta0 = delta0, sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, \n",
    "                   norm_obs=cfg[\"obs_normalizer\"], \n",
    "                   norm_reward=cfg[\"reward_normalizer\"])\n",
    "# env = VecCheckNan(env, raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"], \n",
    "                   vf = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"])],\n",
    "    activation_fn = act_cls,\n",
    "    log_std_init = cfg[\"model\"][\"log_std_init\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpPolicy\n"
     ]
    }
   ],
   "source": [
    "print(policytype)\n",
    "\n",
    "if optimizer_cls == PPO:\n",
    "    kwa = dict(policy = policytype, \n",
    "                env = env,\n",
    "                tensorboard_log = \"./logs\",\n",
    "                clip_range = cfg[\"model\"][\"clip_range\"],\n",
    "                learning_rate = cfg[\"model\"][\"lr\"],\n",
    "                ent_coef = cfg[\"model\"][\"ent_coef\"],\n",
    "                use_sde = cfg[\"model\"][\"use_sde\"],\n",
    "                batch_size = cfg[\"model\"][\"batch_size\"],\n",
    "                policy_kwargs = policy_kwargs)\n",
    "    \n",
    "else:\n",
    "    kwa = dict(policy = policytype, \n",
    "                env = env,\n",
    "                tensorboard_log = \"./logs\",\n",
    "                batch_size = cfg[\"model\"][\"batch_size\"],\n",
    "                learning_rate = cfg[\"model\"][\"lr\"])\n",
    "\n",
    "model = optimizer_cls(**kwa)\n",
    "\n",
    "if cfg[\"starting_point\"]:\n",
    "    model.set_parameters(cfg[\"starting_point\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If batch_size = 64 and n_steps = 2048, then 1 epoch = 2048/64 = 32 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/simplest/13-24/23/23_0916-1416'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "bin_ = f\"{(cfg['id']//12)*12 +1}-{(cfg['id']//12 +1)*12 }\"\n",
    "entcoef = str(model.ent_coef) if type(model) == PPO else \"\"\n",
    "cliprange = str(model.clip_range(0)) if type(model) == PPO else \"\"\n",
    "model_name = f\"models/simplest/{bin_}/{cfg['id']}/{cfg['id']}_{datetime.datetime.now().strftime('%m%d-%H%M')}\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallbackPPO(BaseCallback):\n",
    "    def __init__(self, schedule_timesteps, start_log_std=2, end_log_std=-1, std_control = cfg[\"clipped_std\"]):\n",
    "        super().__init__(verbose = 0)\n",
    "        self.print_flag = False\n",
    "        self.std_control = std_control\n",
    "        \n",
    "        self.start_log_std = start_log_std\n",
    "        self.end_log_std = end_log_std\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [[]]*4\n",
    "        \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \n",
    "        # self.logger.record(\"train/std\", th.exp(self.model.policy.log_std).mean().item())\n",
    "        self.logger.record(\"penalties/in_out\", sum(self.pen_M)/len(self.pen_M))\n",
    "        self.logger.record(\"penalties/buysell_bounds\", sum(self.pen_B)/len(self.pen_B))\n",
    "        self.logger.record(\"penalties/tank_bounds\", sum(self.pen_P)/len(self.pen_P))\n",
    "        # self.logger.record(\"penalties/regterm\", sum(self.pen_reg)/len(self.pen_reg))\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []\n",
    "        \n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # params = [k for k in self.model.policy.named_parameters()]\n",
    "            \n",
    "        # for j in range(len(params)):\n",
    "        #     if any(th.isnan(params[j][1]).flatten()):\n",
    "        #         print(\"NAN detected\")\n",
    "        #         print(params[j])\n",
    "        #         return(False)\n",
    "        \n",
    "        \n",
    "        log_std: th.Tensor = self.model.policy.log_std\n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        \n",
    "        if self.locals[\"dones\"][0]: # record info at each episode end\n",
    "            self.pen_M.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"M\"])\n",
    "            self.pen_B.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"B\"])\n",
    "            self.pen_P.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"P\"])\n",
    "        \n",
    "        if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "            self.print_flag = True\n",
    "            \n",
    "        if self.print_flag:\n",
    "            print(\"\\nt:\", t)\n",
    "            if np.isnan(self.locals['rewards'][0]) or np.isinf(self.locals['rewards'][0]):\n",
    "                print(f\"is invalid reward {self.locals['rewards'][0]}\")\n",
    "            for i in ['obs_tensor', 'actions', 'values', 'clipped_actions', 'new_obs', 'rewards']:\n",
    "                if i in self.locals:\n",
    "                    print(f\"{i}: \" + str(self.locals[i]))\n",
    "            if t == 6:\n",
    "                self.print_flag = False\n",
    "                print(f\"\\n\\nLog-Std at step {self.num_timesteps}: {log_std.detach().numpy()}\")\n",
    "                print(\"\\n\\n\\n\\n\\n\")\n",
    "                \n",
    "        if self.std_control:\n",
    "            progress = self.current_step / self.schedule_timesteps\n",
    "            new_log_std = self.start_log_std + progress * (self.end_log_std - self.start_log_std)\n",
    "            self.model.policy.log_std.data.fill_(new_log_std)\n",
    "            self.current_step += 1\n",
    "                \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallbackDDPG(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.print_flag = False\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []\n",
    "        \n",
    "    def _on_rollout_end(self) -> None: ...\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        \n",
    "        if self.locals[\"dones\"][0]: # record info at each episode end\n",
    "            self.pen_M.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"M\"])\n",
    "            self.pen_B.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"B\"])\n",
    "            self.pen_P.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"P\"])\n",
    "            self.pen_reg.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"reg\"])\n",
    "            \n",
    "            self.total_rewards.append(self.locals['rewards'][0])\n",
    "            \n",
    "        \n",
    "        if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "            self.print_flag = True\n",
    "            \n",
    "        if self.print_flag:\n",
    "            print(\"\\nt:\", t)\n",
    "            if np.isnan(self.locals['rewards'][0]) or np.isinf(self.locals['rewards'][0]):\n",
    "                print(f\"is invalid reward {self.locals['rewards'][0]}\")\n",
    "            for i in ['obs_tensor', 'actions', 'values', 'new_obs', 'rewards']:\n",
    "                if i in self.locals:\n",
    "                    print(f\"{i}: \" + str(self.locals[i]))\n",
    "            if t == 6:\n",
    "                self.print_flag = False\n",
    "                # print(f\"\\nAvg rewards over the last 100 episodes:{sum(self.total_rewards[-100:])/100} ; last reward: {self.total_rewards[-1]}\")\n",
    "                \n",
    "                self.logger.record('train/learning_rate', self.model.learning_rate)\n",
    "                self.logger.record(\"penalties/in_out\", sum(self.pen_M)/len(self.pen_M))\n",
    "                self.logger.record(\"penalties/buysell_bounds\", sum(self.pen_B)/len(self.pen_B))\n",
    "                self.logger.record(\"penalties/tank_bounds\", sum(self.pen_P)/len(self.pen_P))\n",
    "                self.logger.record(\"penalties/regterm\", sum(self.pen_reg)/len(self.pen_reg))\n",
    "        \n",
    "                self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []   \n",
    "                \n",
    "                print(\"\\n\\n\\n\\n\\n\")\n",
    "                \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/simplest/13-24/23/23_0916-1416'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_timesteps = 1e5\n",
    "log_callback = CustomLoggingCallbackPPO(schedule_timesteps=total_timesteps) if optimizer_cls == PPO else CustomLoggingCallbackDDPG()\n",
    "callback = CallbackList([log_callback])\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Higher batch size, no STD control (25)\n",
    "- making the env progressively harder (26)\n",
    "- harder env (no incentive) from scratch (27)\n",
    "- higher penalties for contraints (28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging at simplest/13-24/23/23_0916-1416\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[ 0.15735444 -0.06061666 -0.08556438 -0.04354066]]\n",
      "values: tensor([[-0.6532]])\n",
      "clipped_actions: [[0.15735444 0.         0.         0.        ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       15.370173  0.\n",
      "   0.        0.       13.39785   0.       17.799267 14.10087  15.758581\n",
      "   0.        0.        1.      ]]\n",
      "rewards: [-10.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[  1.1821247  -7.805929   -6.209317  -12.17787  ]]\n",
      "values: tensor([[2.2945]])\n",
      "clipped_actions: [[1.1821247 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.        0.        0.\n",
      "  13.39785   0.       17.799267 14.10087  15.758581  0.        0.\n",
      "  14.640791  0.        2.      ]]\n",
      "rewards: [-2.008061]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 5.91471   -3.1593919 13.190676  -4.1499267]]\n",
      "values: tensor([[0.0091]])\n",
      "clipped_actions: [[ 5.91471   0.       13.190676  0.      ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       13.39785   0.\n",
      "  17.799267 14.10087  15.758581  0.        0.       14.640791  0.\n",
      "   0.       15.826144  3.      ]]\n",
      "rewards: [-1.7503635]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[ -2.1974752  11.044113    5.228356  -11.127087 ]]\n",
      "values: tensor([[3.1562]])\n",
      "clipped_actions: [[ 0.       11.044113  5.228356  0.      ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       17.799267 14.10087\n",
      "  15.758581  0.        0.       14.640791  0.        0.       15.826144\n",
      "  15.974512 14.589509  4.      ]]\n",
      "rewards: [-1.5352024]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[ 2.9960365   1.5238434   8.003153   -0.16119082]]\n",
      "values: tensor([[2.2272]])\n",
      "clipped_actions: [[2.9960365 1.5238434 8.003153  0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.10087  15.758581  0.\n",
      "   0.       14.640791  0.        0.       15.826144 15.974512 14.589509\n",
      "  15.715956 15.271763  5.      ]]\n",
      "rewards: [-1.2114048]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-7.3666353 -8.923579  -0.5774293 -9.422444 ]]\n",
      "values: tensor([[1.2755]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [-0.8583193]\n",
      "\n",
      "\n",
      "Log-Std at step 6: [1.99988 1.99988 1.99988 1.99988]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[-1.9682736 -8.077846   5.735559  -7.652156 ]]\n",
      "values: tensor([[-0.3301]])\n",
      "clipped_actions: [[0.       0.       5.735559 0.      ]]\n",
      "new_obs: [[ 5.735559  0.        0.        0.        0.       15.370173  0.\n",
      "   0.        0.       13.39785   0.       17.799267 14.10087  15.758581\n",
      "   0.        0.        1.      ]]\n",
      "rewards: [0.03200821]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 5.7356,  0.0000,  0.0000,  0.0000,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 0.00976133 -2.2041392   2.9840004  -7.135969  ]]\n",
      "values: tensor([[0.2889]])\n",
      "clipped_actions: [[0.00976133 0.         2.9840004  0.        ]]\n",
      "new_obs: [[5.7257977e+00 9.7613260e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.3397850e+01 0.0000000e+00 1.7799267e+01\n",
      "  1.4100870e+01 1.5758581e+01 0.0000000e+00 0.0000000e+00 1.4640791e+01\n",
      "  0.0000000e+00 2.0000000e+00]]\n",
      "rewards: [0.02034779]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[5.7258e+00, 9.7613e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.3398e+01, 0.0000e+00, 1.7799e+01, 1.4101e+01, 1.5759e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.4641e+01, 0.0000e+00, 2.0000e+00]])\n",
      "actions: [[10.810172  4.128429 -5.404763  5.647096]]\n",
      "values: tensor([[-0.0658]])\n",
      "clipped_actions: [[10.810172  4.128429  0.        5.647096]]\n",
      "new_obs: [[0.0000000e+00 9.7613260e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  1.3397850e+01 0.0000000e+00 1.7799267e+01 1.4100870e+01 1.5758581e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.4640791e+01 0.0000000e+00 0.0000000e+00\n",
      "  1.5826144e+01 3.0000000e+00]]\n",
      "rewards: [-0.14233457]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[0.0000e+00, 9.7613e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3398e+01,\n",
      "         0.0000e+00, 1.7799e+01, 1.4101e+01, 1.5759e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.4641e+01, 0.0000e+00, 0.0000e+00, 1.5826e+01, 3.0000e+00]])\n",
      "actions: [[-5.418054   6.145254  -9.054115   2.3445878]]\n",
      "values: tensor([[0.3350]])\n",
      "clipped_actions: [[0.        6.145254  0.        2.3445878]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       17.799267 14.10087\n",
      "  15.758581  0.        0.       14.640791  0.        0.       15.826144\n",
      "  15.974512 14.589509  4.      ]]\n",
      "rewards: [-0.19413327]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[-11.2102995   -0.43902802  -0.01446896   8.631396  ]]\n",
      "values: tensor([[-0.4849]])\n",
      "clipped_actions: [[0.       0.       0.       8.631396]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.10087  15.758581  0.\n",
      "   0.       14.640791  0.        0.       15.826144 15.974512 14.589509\n",
      "  15.715956 15.271763  5.      ]]\n",
      "rewards: [-0.20534955]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[  1.5595195   2.72256   -10.678556   -1.6995506]]\n",
      "values: tensor([[-0.2059]])\n",
      "clipped_actions: [[1.5595195 2.72256   0.        0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [-0.2527763]\n",
      "\n",
      "\n",
      "Log-Std at step 2058: [1.93832 1.93832 1.93832 1.93832]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[11.729046    2.518558   -3.5883732  -0.49240243]]\n",
      "values: tensor([[0.0699]])\n",
      "clipped_actions: [[11.729046  2.518558  0.        0.      ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       15.370173  0.\n",
      "   0.        0.       13.39785   0.       17.799267 14.10087  15.758581\n",
      "   0.        0.        1.      ]]\n",
      "rewards: [-0.08761083]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[-2.155185   0.4534557  4.656952  10.840325 ]]\n",
      "values: tensor([[-0.6471]])\n",
      "clipped_actions: [[ 0.         0.4534557  4.656952  10.840325 ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.        0.        0.\n",
      "  13.39785   0.       17.799267 14.10087  15.758581  0.        0.\n",
      "  14.640791  0.        2.      ]]\n",
      "rewards: [-0.11874879]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-3.8799095  7.2937007 -1.4471631  4.2138257]]\n",
      "values: tensor([[-0.6360]])\n",
      "clipped_actions: [[0.        7.2937007 0.        4.2138257]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       13.39785   0.\n",
      "  17.799267 14.10087  15.758581  0.        0.       14.640791  0.\n",
      "   0.       15.826144  3.      ]]\n",
      "rewards: [-0.17288618]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[-9.5993     -9.930836   -0.81192756 -3.6003647 ]]\n",
      "values: tensor([[-1.2720]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       17.799267 14.10087\n",
      "  15.758581  0.        0.       14.640791  0.        0.       15.826144\n",
      "  15.974512 14.589509  4.      ]]\n",
      "rewards: [-0.17290422]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[ 0.9564383  -0.18106526  4.3590493   7.361424  ]]\n",
      "values: tensor([[-0.1969]])\n",
      "clipped_actions: [[0.9564383 0.        4.3590493 7.361424 ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.10087  15.758581  0.\n",
      "   0.       14.640791  0.        0.       15.826144 15.974512 14.589509\n",
      "  15.715956 15.271763  5.      ]]\n",
      "rewards: [-0.20644467]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-10.25639      0.34811044   1.1875173    7.821462  ]]\n",
      "values: tensor([[0.1977]])\n",
      "clipped_actions: [[0.         0.34811044 1.1875173  7.821462  ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [-0.22194633]\n",
      "\n",
      "\n",
      "Log-Std at step 4104: [1.87694 1.87694 1.87694 1.87694]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[ 5.2800646 -1.6332784  7.1520185 -5.1234202]]\n",
      "values: tensor([[-0.1625]])\n",
      "clipped_actions: [[5.2800646 0.        7.1520185 0.       ]]\n",
      "new_obs: [[ 1.871954   5.2800646  0.         0.06       0.        15.370173\n",
      "   0.         0.         0.        13.39785    0.        17.799267\n",
      "  14.10087   15.758581   0.         0.         1.       ]]\n",
      "rewards: [0.02994741]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 1.8720,  5.2801,  0.0000,  0.0600,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[-7.9328537 -2.5298438  3.6509078  1.9705869]]\n",
      "values: tensor([[3.7776]])\n",
      "clipped_actions: [[0.        0.        3.6509078 1.9705869]]\n",
      "new_obs: [[ 1.871954   5.2800646  0.         0.06       0.         0.\n",
      "   0.        13.39785    0.        17.799267  14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         2.       ]]\n",
      "rewards: [0.01296187]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 1.8720,  5.2801,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 2.342896   9.665168  -4.8410263 10.958192 ]]\n",
      "values: tensor([[2.7708]])\n",
      "clipped_actions: [[ 2.342896  9.665168  0.       10.958192]]\n",
      "new_obs: [[ 0.         5.2800646  0.         0.06       0.        13.39785\n",
      "   0.        17.799267  14.10087   15.758581   0.         0.\n",
      "  14.640791   0.         0.        15.826144   3.       ]]\n",
      "rewards: [-0.09835134]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  5.2801,  0.0000,  0.0600,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[ 9.878856   6.8814273  1.3900311 -2.1621838]]\n",
      "values: tensor([[2.2792]])\n",
      "clipped_actions: [[9.878856  6.8814273 1.3900311 0.       ]]\n",
      "new_obs: [[ 0.         0.         5.2800646  0.         0.        17.799267\n",
      "  14.10087   15.758581   0.         0.        14.640791   0.\n",
      "   0.        15.826144  15.974512  14.589509   4.       ]]\n",
      "rewards: [-0.10617512]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  5.2801,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[ 2.8017008 11.860682  -1.482894  -0.2248187]]\n",
      "values: tensor([[3.5177]])\n",
      "clipped_actions: [[ 2.8017008 11.860682   0.         0.       ]]\n",
      "new_obs: [[ 0.         0.         5.2800646  0.        14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         0.        15.826144\n",
      "  15.974512  14.589509  15.715956  15.271763   5.       ]]\n",
      "rewards: [-0.18631046]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  5.2801,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[  2.8734407 -10.0768175  -2.6736646  -9.717285 ]]\n",
      "values: tensor([[1.3211]])\n",
      "clipped_actions: [[2.8734407 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [-0.20745085]\n",
      "\n",
      "\n",
      "Log-Std at step 6150: [1.81556 1.81556 1.81556 1.81556]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[ 1.2342366  -5.3828053   0.91855717  3.617029  ]]\n",
      "values: tensor([[0.6563]])\n",
      "clipped_actions: [[1.2342366  0.         0.91855717 3.617029  ]]\n",
      "new_obs: [[ 0.          0.91855717  0.          0.06        0.         15.370173\n",
      "   0.          0.          0.         13.39785     0.         17.799267\n",
      "  14.10087    15.758581    0.          0.          1.        ]]\n",
      "rewards: [-0.01374068]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.9186,  0.0000,  0.0600,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[-0.7301904 -2.6752143  1.1025642  2.9830813]]\n",
      "values: tensor([[-0.3909]])\n",
      "clipped_actions: [[0.        0.        1.1025642 2.9830813]]\n",
      "new_obs: [[ 0.          0.91855717  0.          0.06        0.          0.\n",
      "   0.         13.39785     0.         17.799267   14.10087    15.758581\n",
      "   0.          0.         14.640791    0.          2.        ]]\n",
      "rewards: [-0.02945903]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.9186,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-1.9651815  6.1092906 -1.0492606  4.8810487]]\n",
      "values: tensor([[-0.0224]])\n",
      "clipped_actions: [[0.        6.1092906 0.        4.8810487]]\n",
      "new_obs: [[ 0.          0.          0.91855717  0.          0.         13.39785\n",
      "   0.         17.799267   14.10087    15.758581    0.          0.\n",
      "  14.640791    0.          0.         15.826144    3.        ]]\n",
      "rewards: [-0.05514111]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.9186,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[ 0.06593034  7.7375684   6.327657   14.546644  ]]\n",
      "values: tensor([[0.4127]])\n",
      "clipped_actions: [[ 0.06593034  7.7375684   6.327657   14.546644  ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       17.799267 14.10087\n",
      "  15.758581  0.        0.       14.640791  0.        0.       15.826144\n",
      "  15.974512 14.589509  4.      ]]\n",
      "rewards: [0.23508379]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[-9.10038   -4.520414   5.9513392 -5.5279646]]\n",
      "values: tensor([[-0.2277]])\n",
      "clipped_actions: [[0.        0.        5.9513392 0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.10087  15.758581  0.\n",
      "   0.       14.640791  0.        0.       15.826144 15.974512 14.589509\n",
      "  15.715956 15.271763  5.      ]]\n",
      "rewards: [0.22723585]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[ 3.2483859e+00 -1.2538808e+01 -2.1272111e+00 -9.3734264e-04]]\n",
      "values: tensor([[0.1192]])\n",
      "clipped_actions: [[3.248386 0.       0.       0.      ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [0.2062246]\n",
      "\n",
      "\n",
      "Log-Std at step 8202: [1.754 1.754 1.754 1.754]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[ 4.6697955 -2.114884  14.847498  -1.8205907]]\n",
      "values: tensor([[0.9828]])\n",
      "clipped_actions: [[ 4.6697955  0.        14.847498   0.       ]]\n",
      "new_obs: [[10.177702   4.6697955  0.         0.06       0.        15.370173\n",
      "   0.         0.         0.        13.39785    0.        17.799267\n",
      "  14.10087   15.758581   0.         0.         1.       ]]\n",
      "rewards: [0.05265917]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.1777,  4.6698,  0.0000,  0.0600,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 1.0771861   0.8580166  21.319405    0.18316516]]\n",
      "values: tensor([[4.6724]])\n",
      "clipped_actions: [[ 1.0771861   0.8580166  21.319405    0.18316516]]\n",
      "new_obs: [[ 9.100516   4.6697955  0.         0.06       0.         0.\n",
      "   0.        13.39785    0.        17.799267  14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         2.       ]]\n",
      "rewards: [-0.00495535]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 9.1005,  4.6698,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 4.748447   8.499349  -0.0545795  1.8487537]]\n",
      "values: tensor([[3.1908]])\n",
      "clipped_actions: [[4.748447  8.499349  0.        1.8487537]]\n",
      "new_obs: [[ 4.3520694  4.6697955  0.         0.06       0.        13.39785\n",
      "   0.        17.799267  14.10087   15.758581   0.         0.\n",
      "  14.640791   0.         0.        15.826144   3.       ]]\n",
      "rewards: [-0.09583135]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 4.3521,  4.6698,  0.0000,  0.0600,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[0.5201872 9.313206  5.5815945 4.0938816]]\n",
      "values: tensor([[1.6247]])\n",
      "clipped_actions: [[0.5201872 9.313206  5.5815945 4.0938816]]\n",
      "new_obs: [[ 3.8318822  4.6697955  0.         0.06       0.        17.799267\n",
      "  14.10087   15.758581   0.         0.        14.640791   0.\n",
      "   0.        15.826144  15.974512  14.589509   4.       ]]\n",
      "rewards: [-0.18166442]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 3.8319,  4.6698,  0.0000,  0.0600,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[ 3.7763224 -1.3523588 -6.1714334 -1.2230425]]\n",
      "values: tensor([[1.6314]])\n",
      "clipped_actions: [[3.7763224 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.05555981  8.446117    0.          0.08682645 14.10087    15.758581\n",
      "   0.          0.         14.640791    0.          0.         15.826144\n",
      "  15.974512   14.589509   15.715956   15.271763    5.        ]]\n",
      "rewards: [-0.16854212]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0556,  8.4461,  0.0000,  0.0868, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-5.5416985  -0.71033573  0.42123306 -2.7523987 ]]\n",
      "values: tensor([[1.1459]])\n",
      "clipped_actions: [[0.         0.         0.42123306 0.        ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [-0.16704299]\n",
      "\n",
      "\n",
      "Log-Std at step 10248: [1.69262 1.69262 1.69262 1.69262]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[ 1.2649379  1.6421459 11.713623   0.8272246]]\n",
      "values: tensor([[1.7073]])\n",
      "clipped_actions: [[ 1.2649379  1.6421459 11.713623   0.8272246]]\n",
      "new_obs: [[10.448685  0.        0.        0.        0.       15.370173  0.\n",
      "   0.        0.       13.39785   0.       17.799267 14.10087  15.758581\n",
      "   0.        0.        1.      ]]\n",
      "rewards: [-0.01495383]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.4487,  0.0000,  0.0000,  0.0000,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 5.673151    4.285302   -0.25595528 -4.2809405 ]]\n",
      "values: tensor([[1.3998]])\n",
      "clipped_actions: [[5.673151 4.285302 0.       0.      ]]\n",
      "new_obs: [[ 4.775534  0.        0.        0.        0.        0.        0.\n",
      "  13.39785   0.       17.799267 14.10087  15.758581  0.        0.\n",
      "  14.640791  0.        2.      ]]\n",
      "rewards: [-0.07966722]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 4.7755,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[10.978951  -5.0086555 -1.144433  10.2234745]]\n",
      "values: tensor([[0.4966]])\n",
      "clipped_actions: [[10.978951   0.         0.        10.2234745]]\n",
      "new_obs: [[ 0.        4.775534  0.        0.06      0.       13.39785   0.\n",
      "  17.799267 14.10087  15.758581  0.        0.       14.640791  0.\n",
      "   0.       15.826144  3.      ]]\n",
      "rewards: [-0.0974165]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  4.7755,  0.0000,  0.0600,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[-8.280523  9.620181  6.918966 -6.333603]]\n",
      "values: tensor([[1.8983]])\n",
      "clipped_actions: [[0.       9.620181 6.918966 0.      ]]\n",
      "new_obs: [[ 0.        0.        4.775534  0.        0.       17.799267 14.10087\n",
      "  15.758581  0.        0.       14.640791  0.        0.       15.826144\n",
      "  15.974512 14.589509  4.      ]]\n",
      "rewards: [-0.0801415]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  4.7755,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[1.9957522 1.6240485 6.6929483 2.8148026]]\n",
      "values: tensor([[3.3647]])\n",
      "clipped_actions: [[1.9957522 1.6240485 6.6929483 2.8148026]]\n",
      "new_obs: [[ 0.         0.         1.9607315  0.        14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         0.        15.826144\n",
      "  15.974512  14.589509  15.715956  15.271763   5.       ]]\n",
      "rewards: [0.7920342]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  1.9607,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-2.342936  6.66109  -2.810802  7.621798]]\n",
      "values: tensor([[1.2009]])\n",
      "clipped_actions: [[0.       6.66109  0.       7.621798]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [1.3868583]\n",
      "\n",
      "\n",
      "Log-Std at step 12294: [1.63124 1.63124 1.63124 1.63124]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[-3.0275273 -3.3001885  0.1399312  6.2105174]]\n",
      "values: tensor([[1.8634]])\n",
      "clipped_actions: [[0.        0.        0.1399312 6.2105174]]\n",
      "new_obs: [[ 0.1399312  0.         0.         0.         0.        15.370173\n",
      "   0.         0.         0.        13.39785    0.        17.799267\n",
      "  14.10087   15.758581   0.         0.         1.       ]]\n",
      "rewards: [-0.00537829]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.1399,  0.0000,  0.0000,  0.0000,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 5.4938455  -8.97135     0.22605908 -2.806846  ]]\n",
      "values: tensor([[-0.5777]])\n",
      "clipped_actions: [[5.4938455  0.         0.22605908 0.        ]]\n",
      "new_obs: [[ 0.         0.1399312  0.         0.06       0.         0.\n",
      "   0.        13.39785    0.        17.799267  14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         2.       ]]\n",
      "rewards: [-0.03231029]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.1399,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-1.314827  -1.7195547 -1.177533   2.6827033]]\n",
      "values: tensor([[0.6152]])\n",
      "clipped_actions: [[0.        0.        0.        2.6827033]]\n",
      "new_obs: [[ 0.         0.1399312  0.         0.06       0.        13.39785\n",
      "   0.        17.799267  14.10087   15.758581   0.         0.\n",
      "  14.640791   0.         0.        15.826144   3.       ]]\n",
      "rewards: [-0.03809464]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.1399,  0.0000,  0.0600,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[ 1.3908368 -2.2201064 -0.8149121  2.7737362]]\n",
      "values: tensor([[0.2318]])\n",
      "clipped_actions: [[1.3908368 0.        0.        2.7737362]]\n",
      "new_obs: [[ 0.         0.1399312  0.         0.06       0.        17.799267\n",
      "  14.10087   15.758581   0.         0.        14.640791   0.\n",
      "   0.        15.826144  15.974512  14.589509   4.       ]]\n",
      "rewards: [-0.0539739]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.1399,  0.0000,  0.0600,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[-4.505351   3.2494826 -1.7802267 -2.7472548]]\n",
      "values: tensor([[0.9752]])\n",
      "clipped_actions: [[0.        3.2494826 0.        0.       ]]\n",
      "new_obs: [[ 0.         0.         0.1399312  0.        14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         0.        15.826144\n",
      "  15.974512  14.589509  15.715956  15.271763   5.       ]]\n",
      "rewards: [-0.06782669]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.1399,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-3.8008232 -3.7507079 -2.6428094 -1.5558921]]\n",
      "values: tensor([[0.6621]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [-0.06782864]\n",
      "\n",
      "\n",
      "Log-Std at step 14346: [1.56968 1.56968 1.56968 1.56968]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[-2.7988505 -5.1498814  9.967814   2.204657 ]]\n",
      "values: tensor([[2.0445]])\n",
      "clipped_actions: [[0.       0.       9.967814 2.204657]]\n",
      "new_obs: [[ 9.967814  0.        0.        0.        0.       15.370173  0.\n",
      "   0.        0.       13.39785   0.       17.799267 14.10087  15.758581\n",
      "   0.        0.        1.      ]]\n",
      "rewards: [0.0209133]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 9.9678,  0.0000,  0.0000,  0.0000,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[14.803219   -0.14403152 -3.4385204  -3.427452  ]]\n",
      "values: tensor([[2.6051]])\n",
      "clipped_actions: [[14.803219  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.        9.967814  0.        0.06      0.        0.        0.\n",
      "  13.39785   0.       17.799267 14.10087  15.758581  0.        0.\n",
      "  14.640791  0.        2.      ]]\n",
      "rewards: [0.0288734]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  9.9678,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-4.465472   6.258901   0.9145107 -0.5214211]]\n",
      "values: tensor([[6.0680]])\n",
      "clipped_actions: [[0.        6.258901  0.9145107 0.       ]]\n",
      "new_obs: [[ 0.          3.7089133   6.258901   -0.04125178  0.         13.39785\n",
      "   0.         17.799267   14.10087    15.758581    0.          0.\n",
      "  14.640791    0.          0.         15.826144    3.        ]]\n",
      "rewards: [0.07264863]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  3.7089,  6.2589, -0.0413,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[-2.6120543  1.6692123  3.4473722 14.248083 ]]\n",
      "values: tensor([[6.1712]])\n",
      "clipped_actions: [[ 0.         1.6692123  3.4473722 14.248083 ]]\n",
      "new_obs: [[ 0.0000000e+00  2.0397010e+00  0.0000000e+00 -7.4929195e-03\n",
      "   0.0000000e+00  1.7799267e+01  1.4100870e+01  1.5758581e+01\n",
      "   0.0000000e+00  0.0000000e+00  1.4640791e+01  0.0000000e+00\n",
      "   0.0000000e+00  1.5826144e+01  1.5974512e+01  1.4589509e+01\n",
      "   4.0000000e+00]]\n",
      "rewards: [2.1330822]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000e+00,  2.0397e+00,  0.0000e+00, -7.4929e-03,  0.0000e+00,\n",
      "          1.7799e+01,  1.4101e+01,  1.5759e+01,  0.0000e+00,  0.0000e+00,\n",
      "          1.4641e+01,  0.0000e+00,  0.0000e+00,  1.5826e+01,  1.5975e+01,\n",
      "          1.4590e+01,  4.0000e+00]])\n",
      "actions: [[-2.70447    4.1808786  4.0486503 -1.2915211]]\n",
      "values: tensor([[1.3205]])\n",
      "clipped_actions: [[0.        4.1808786 4.0486503 0.       ]]\n",
      "new_obs: [[ 0.        0.        2.039701  0.       14.10087  15.758581  0.\n",
      "   0.       14.640791  0.        0.       15.826144 15.974512 14.589509\n",
      "  15.715956 15.271763  5.      ]]\n",
      "rewards: [2.1317017]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  2.0397,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-2.0148518  1.0634716 -6.543703   7.5669565]]\n",
      "values: tensor([[1.0769]])\n",
      "clipped_actions: [[0.        1.0634716 0.        7.5669565]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [2.6496172]\n",
      "\n",
      "\n",
      "Log-Std at step 16392: [1.5083 1.5083 1.5083 1.5083]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[-6.6632357 -4.696529  11.104301   1.251989 ]]\n",
      "values: tensor([[2.9211]])\n",
      "clipped_actions: [[ 0.        0.       11.104301  1.251989]]\n",
      "new_obs: [[11.104301  0.        0.        0.        0.       15.370173  0.\n",
      "   0.        0.       13.39785   0.       17.799267 14.10087  15.758581\n",
      "   0.        0.        1.      ]]\n",
      "rewards: [0.02054731]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[11.1043,  0.0000,  0.0000,  0.0000,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[11.5860815 -3.1479378  1.5653567 -0.3160422]]\n",
      "values: tensor([[3.5561]])\n",
      "clipped_actions: [[11.5860815  0.         1.5653567  0.       ]]\n",
      "new_obs: [[ 0.       11.104301  0.        0.06      0.        0.        0.\n",
      "  13.39785   0.       17.799267 14.10087  15.758581  0.        0.\n",
      "  14.640791  0.        2.      ]]\n",
      "rewards: [0.03526877]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000, 11.1043,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 0.62437034 10.266194   -4.6903434   1.1482048 ]]\n",
      "values: tensor([[6.5977]])\n",
      "clipped_actions: [[ 0.62437034 10.266194    0.          1.1482048 ]]\n",
      "new_obs: [[ 0.         0.8381071 10.266194  -0.6749558  0.        13.39785\n",
      "   0.        17.799267  14.10087   15.758581   0.         0.\n",
      "  14.640791   0.         0.        15.826144   3.       ]]\n",
      "rewards: [0.09389348]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.8381, 10.2662, -0.6750,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[ 3.490184  -6.6540413 -5.039704  15.380613 ]]\n",
      "values: tensor([[7.9612]])\n",
      "clipped_actions: [[ 3.490184  0.        0.       15.380613]]\n",
      "new_obs: [[ 0.         0.8381071  0.        -0.6749558  0.        17.799267\n",
      "  14.10087   15.758581   0.         0.        14.640791   0.\n",
      "   0.        15.826144  15.974512  14.589509   4.       ]]\n",
      "rewards: [2.3890555]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.8381,  0.0000, -0.6750,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[-1.0098524 -4.508599  -2.514997  -2.8345072]]\n",
      "values: tensor([[2.1503]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.         0.8381071  0.        -0.6749558 14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         0.        15.826144\n",
      "  15.974512  14.589509  15.715956  15.271763   5.       ]]\n",
      "rewards: [2.3877254]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.8381,  0.0000, -0.6750, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-9.6009035   0.17442897  8.642283    0.26891562]]\n",
      "values: tensor([[1.2945]])\n",
      "clipped_actions: [[0.         0.17442897 8.642283   0.26891562]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [2.4382293]\n",
      "\n",
      "\n",
      "Log-Std at step 18438: [1.44692 1.44692 1.44692 1.44692]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[ 2.2032351 -7.274461   9.180855   0.9309117]]\n",
      "values: tensor([[3.4169]])\n",
      "clipped_actions: [[2.2032351 0.        9.180855  0.9309117]]\n",
      "new_obs: [[ 6.9776196  2.2032351  0.         0.06       0.        15.370173\n",
      "   0.         0.         0.        13.39785    0.        17.799267\n",
      "  14.10087   15.758581   0.         0.         1.       ]]\n",
      "rewards: [0.01394091]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 6.9776,  2.2032,  0.0000,  0.0600,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 5.333087  -2.7398145 -6.5841846 -3.9038396]]\n",
      "values: tensor([[2.9748]])\n",
      "clipped_actions: [[5.333087 0.       0.       0.      ]]\n",
      "new_obs: [[ 1.6445327   7.536322    0.          0.10245907  0.          0.\n",
      "   0.         13.39785     0.         17.799267   14.10087    15.758581\n",
      "   0.          0.         14.640791    0.          2.        ]]\n",
      "rewards: [0.02424445]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 1.6445,  7.5363,  0.0000,  0.1025,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-5.560752   9.974202  -6.5327005 -4.7700076]]\n",
      "values: tensor([[4.2585]])\n",
      "clipped_actions: [[0.       9.974202 0.       0.      ]]\n",
      "new_obs: [[ 1.6445327  0.         7.536322   0.         0.        13.39785\n",
      "   0.        17.799267  14.10087   15.758581   0.         0.\n",
      "  14.640791   0.         0.        15.826144   3.       ]]\n",
      "rewards: [0.05982532]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 1.6445,  0.0000,  7.5363,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[-8.15456   -0.9184802 -3.0894024  8.722207 ]]\n",
      "values: tensor([[4.8588]])\n",
      "clipped_actions: [[0.       0.       0.       8.722207]]\n",
      "new_obs: [[ 1.6445327  0.         0.         0.         0.        17.799267\n",
      "  14.10087   15.758581   0.         0.        14.640791   0.\n",
      "   0.        15.826144  15.974512  14.589509   4.       ]]\n",
      "rewards: [1.539691]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 1.6445,  0.0000,  0.0000,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[-2.104846  -4.8642044 -7.5444393 -3.488168 ]]\n",
      "values: tensor([[2.1379]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 1.6445327  0.         0.         0.        14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         0.        15.826144\n",
      "  15.974512  14.589509  15.715956  15.271763   5.       ]]\n",
      "rewards: [1.5394228]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 1.6445,  0.0000,  0.0000,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-1.6881955 -4.030727  -1.3500509  3.759725 ]]\n",
      "values: tensor([[1.2243]])\n",
      "clipped_actions: [[0.       0.       0.       3.759725]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [1.5348125]\n",
      "\n",
      "\n",
      "Log-Std at step 20490: [1.38536 1.38536 1.38536 1.38536]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[ 1.5564151 -2.1788778 12.094977  -6.157505 ]]\n",
      "values: tensor([[3.3589]])\n",
      "clipped_actions: [[ 1.5564151  0.        12.094977   0.       ]]\n",
      "new_obs: [[10.538563   1.5564151  0.         0.06       0.        15.370173\n",
      "   0.         0.         0.        13.39785    0.        17.799267\n",
      "  14.10087   15.758581   0.         0.         1.       ]]\n",
      "rewards: [0.02077773]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.5386,  1.5564,  0.0000,  0.0600,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 11.640902  -13.029787   -1.2367439  -1.7730949]]\n",
      "values: tensor([[4.7649]])\n",
      "clipped_actions: [[11.640902  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.         12.094977    0.          0.11227904  0.          0.\n",
      "   0.         13.39785     0.         17.799267   14.10087    15.758581\n",
      "   0.          0.         14.640791    0.          2.        ]]\n",
      "rewards: [0.03348624]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000, 12.0950,  0.0000,  0.1123,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-5.1986556 10.797807  -0.8193655  2.123988 ]]\n",
      "values: tensor([[6.4099]])\n",
      "clipped_actions: [[ 0.       10.797807  0.        2.123988]]\n",
      "new_obs: [[ 0.         1.2971706 10.797807  -0.8223454  0.        13.39785\n",
      "   0.        17.799267  14.10087   15.758581   0.         0.\n",
      "  14.640791   0.         0.        15.826144   3.       ]]\n",
      "rewards: [0.08596349]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  1.2972, 10.7978, -0.8223,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[ 2.8237517  -6.634338   -0.07091236 16.823967  ]]\n",
      "values: tensor([[6.2701]])\n",
      "clipped_actions: [[ 2.8237517  0.         0.        16.823967 ]]\n",
      "new_obs: [[ 0.         1.2971706  0.        -0.8223454  0.        17.799267\n",
      "  14.10087   15.758581   0.         0.        14.640791   0.\n",
      "   0.        15.826144  15.974512  14.589509   4.       ]]\n",
      "rewards: [1.9408962]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  1.2972,  0.0000, -0.8223,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[-7.4793043 -4.7920966  4.114239  13.8598585]]\n",
      "values: tensor([[2.8930]])\n",
      "clipped_actions: [[ 0.         0.         4.114239  13.8598585]]\n",
      "new_obs: [[ 0.         1.2971706  0.        -0.8223454 14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         0.        15.826144\n",
      "  15.974512  14.589509  15.715956  15.271763   5.       ]]\n",
      "rewards: [1.9334406]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  1.2972,  0.0000, -0.8223, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[1.4450406 5.924217  0.3288262 3.5205393]]\n",
      "values: tensor([[1.2135]])\n",
      "clipped_actions: [[1.4450406 5.924217  0.3288262 3.5205393]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [1.8948594]\n",
      "\n",
      "\n",
      "Log-Std at step 22536: [1.32398 1.32398 1.32398 1.32398]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[  5.873252  -12.639262    8.930062   -1.8897803]]\n",
      "values: tensor([[3.6007]])\n",
      "clipped_actions: [[5.873252 0.       8.930062 0.      ]]\n",
      "new_obs: [[ 3.0568104  5.873252   0.         0.06       0.        15.370173\n",
      "   0.         0.         0.        13.39785    0.        17.799267\n",
      "  14.10087   15.758581   0.         0.         1.       ]]\n",
      "rewards: [0.01361065]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 3.0568,  5.8733,  0.0000,  0.0600,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[-11.498915  10.373662  -1.787643   4.898989]]\n",
      "values: tensor([[4.1527]])\n",
      "clipped_actions: [[ 0.       10.373662  0.        4.898989]]\n",
      "new_obs: [[ 3.0568104  0.         0.9742627  0.         0.         0.\n",
      "   0.        13.39785    0.        17.799267  14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         2.       ]]\n",
      "rewards: [0.7555336]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 3.0568,  0.0000,  0.9743,  0.0000,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[  6.1253457  -2.4393897  -4.1330376 -10.1143465]]\n",
      "values: tensor([[3.1251]])\n",
      "clipped_actions: [[6.1253457 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.         3.0568104  0.9742627  0.06       0.        13.39785\n",
      "   0.        17.799267  14.10087   15.758581   0.         0.\n",
      "  14.640791   0.         0.        15.826144   3.       ]]\n",
      "rewards: [0.7522743]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  3.0568,  0.9743,  0.0600,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[ 1.1889164 12.093079  -2.9548862  4.0155773]]\n",
      "values: tensor([[1.9187]])\n",
      "clipped_actions: [[ 1.1889164 12.093079   0.         4.0155773]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 1.5495777e-02 0.0000000e+00 0.0000000e+00\n",
      "  1.7799267e+01 1.4100870e+01 1.5758581e+01 0.0000000e+00 0.0000000e+00\n",
      "  1.4640791e+01 0.0000000e+00 0.0000000e+00 1.5826144e+01 1.5974512e+01\n",
      "  1.4589509e+01 4.0000000e+00]]\n",
      "rewards: [1.3441573]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 1.5496e-02, 0.0000e+00, 0.0000e+00, 1.7799e+01,\n",
      "         1.4101e+01, 1.5759e+01, 0.0000e+00, 0.0000e+00, 1.4641e+01, 0.0000e+00,\n",
      "         0.0000e+00, 1.5826e+01, 1.5975e+01, 1.4590e+01, 4.0000e+00]])\n",
      "actions: [[ 3.7630606   0.67566395 -4.176994    4.056084  ]]\n",
      "values: tensor([[2.3941]])\n",
      "clipped_actions: [[3.7630606  0.67566395 0.         4.056084  ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.10087  15.758581  0.\n",
      "   0.       14.640791  0.        0.       15.826144 15.974512 14.589509\n",
      "  15.715956 15.271763  5.      ]]\n",
      "rewards: [1.329793]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-1.3180897  1.2888353  2.479896  -2.2624724]]\n",
      "values: tensor([[1.1674]])\n",
      "clipped_actions: [[0.        1.2888353 2.479896  0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [1.327732]\n",
      "\n",
      "\n",
      "Log-Std at step 24582: [1.2626 1.2626 1.2626 1.2626]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[-2.8042984  -3.280977   12.600348   -0.07927884]]\n",
      "values: tensor([[3.7986]])\n",
      "clipped_actions: [[ 0.        0.       12.600348  0.      ]]\n",
      "new_obs: [[12.600348  0.        0.        0.        0.       15.370173  0.\n",
      "   0.        0.       13.39785   0.       17.799267 14.10087  15.758581\n",
      "   0.        0.        1.      ]]\n",
      "rewards: [0.01735501]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[12.6003,  0.0000,  0.0000,  0.0000,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[16.00541  -4.508149  4.650727 -1.84076 ]]\n",
      "values: tensor([[4.8845]])\n",
      "clipped_actions: [[16.00541   0.        4.650727  0.      ]]\n",
      "new_obs: [[ 0.       12.600348  0.        0.06      0.        0.        0.\n",
      "  13.39785   0.       17.799267 14.10087  15.758581  0.        0.\n",
      "  14.640791  0.        2.      ]]\n",
      "rewards: [0.02437336]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000, 12.6003,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-10.068939   11.462751   -3.5802486   2.0499644]]\n",
      "values: tensor([[5.3899]])\n",
      "clipped_actions: [[ 0.        11.462751   0.         2.0499644]]\n",
      "new_obs: [[ 0.          1.1375971  11.462751   -0.54457706  0.         13.39785\n",
      "   0.         17.799267   14.10087    15.758581    0.          0.\n",
      "  14.640791    0.          0.         15.826144    3.        ]]\n",
      "rewards: [0.06884727]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  1.1376, 11.4628, -0.5446,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[ 2.5701904  1.5073795 -4.356393  21.736004 ]]\n",
      "values: tensor([[5.2879]])\n",
      "clipped_actions: [[ 2.5701904  1.5073795  0.        21.736004 ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       17.799267 14.10087\n",
      "  15.758581  0.        0.       14.640791  0.        0.       15.826144\n",
      "  15.974512 14.589509  4.      ]]\n",
      "rewards: [1.7874548]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[ 3.739808  -0.9932374  5.794954   2.8240907]]\n",
      "values: tensor([[2.2098]])\n",
      "clipped_actions: [[3.739808  0.        5.794954  2.8240907]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.10087  15.758581  0.\n",
      "   0.       14.640791  0.        0.       15.826144 15.974512 14.589509\n",
      "  15.715956 15.271763  5.      ]]\n",
      "rewards: [1.7735869]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[ 1.6683525  1.2431607 -0.8272588 -4.65557  ]]\n",
      "values: tensor([[1.2263]])\n",
      "clipped_actions: [[1.6683525 1.2431607 0.        0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [1.7630119]\n",
      "\n",
      "\n",
      "Log-Std at step 26634: [1.20104 1.20104 1.20104 1.20104]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[-1.6755221 -6.3810186  5.8380785 -3.3916113]]\n",
      "values: tensor([[3.8751]])\n",
      "clipped_actions: [[0.        0.        5.8380785 0.       ]]\n",
      "new_obs: [[ 5.8380785  0.         0.         0.         0.        15.370173\n",
      "   0.         0.         0.        13.39785    0.        17.799267\n",
      "  14.10087   15.758581   0.         0.         1.       ]]\n",
      "rewards: [0.00730483]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 5.8381,  0.0000,  0.0000,  0.0000,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[12.700259  -1.5012398 -0.7822988  0.9004934]]\n",
      "values: tensor([[2.2324]])\n",
      "clipped_actions: [[12.700259   0.         0.         0.9004934]]\n",
      "new_obs: [[ 0.         5.8380785  0.         0.06       0.         0.\n",
      "   0.        13.39785    0.        17.799267  14.10087   15.758581\n",
      "   0.         0.        14.640791   0.         2.       ]]\n",
      "rewards: [0.00089337]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  5.8381,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-3.6660876 11.292841  -6.2637362  5.7817173]]\n",
      "values: tensor([[2.1776]])\n",
      "clipped_actions: [[ 0.        11.292841   0.         5.7817173]]\n",
      "new_obs: [[ 0.         0.         5.8380785  0.         0.        13.39785\n",
      "   0.        17.799267  14.10087   15.758581   0.         0.\n",
      "  14.640791   0.         0.        15.826144   3.       ]]\n",
      "rewards: [0.01085286]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  5.8381,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[ 1.6851591 -1.9736443 -2.0233269 14.572892 ]]\n",
      "values: tensor([[2.2907]])\n",
      "clipped_actions: [[ 1.6851591  0.         0.        14.572892 ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       17.799267 14.10087\n",
      "  15.758581  0.        0.       14.640791  0.        0.       15.826144\n",
      "  15.974512 14.589509  4.      ]]\n",
      "rewards: [0.7316267]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[ 4.7014728 -3.4635708 -5.9189906 -3.581615 ]]\n",
      "values: tensor([[2.1488]])\n",
      "clipped_actions: [[4.7014728 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.10087  15.758581  0.\n",
      "   0.       14.640791  0.        0.       15.826144 15.974512 14.589509\n",
      "  15.715956 15.271763  5.      ]]\n",
      "rewards: [0.72311544]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[-3.4486809  0.3755821  5.27038   -1.3181635]]\n",
      "values: tensor([[1.3050]])\n",
      "clipped_actions: [[0.        0.3755821 5.27038   0.       ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [0.7265871]\n",
      "\n",
      "\n",
      "Log-Std at step 28680: [1.13966 1.13966 1.13966 1.13966]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 15.3241, 14.8031,  0.0000, 15.3702,\n",
      "          0.0000,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000]])\n",
      "actions: [[ 0.53913563 -4.3627634   8.618186   -3.6657321 ]]\n",
      "values: tensor([[3.8005]])\n",
      "clipped_actions: [[0.53913563 0.         8.618186   0.        ]]\n",
      "new_obs: [[ 8.07905     0.53913563  0.          0.06        0.         15.370173\n",
      "   0.          0.          0.         13.39785     0.         17.799267\n",
      "  14.10087    15.758581    0.          0.          1.        ]]\n",
      "rewards: [0.00986008]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 8.0791,  0.5391,  0.0000,  0.0600,  0.0000, 15.3702,  0.0000,  0.0000,\n",
      "          0.0000, 13.3979,  0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[12.023862   -5.9599724  -3.863677   -0.06091228]]\n",
      "values: tensor([[3.0370]])\n",
      "clipped_actions: [[12.023862  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.          8.618186    0.          0.11624653  0.          0.\n",
      "   0.         13.39785     0.         17.799267   14.10087    15.758581\n",
      "   0.          0.         14.640791    0.          2.        ]]\n",
      "rewards: [0.01221494]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  8.6182,  0.0000,  0.1162,  0.0000,  0.0000,  0.0000, 13.3979,\n",
      "          0.0000, 17.7993, 14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-3.8560624 11.705665   2.8089647  7.1388626]]\n",
      "values: tensor([[3.1590]])\n",
      "clipped_actions: [[ 0.        11.705665   2.8089647  7.1388626]]\n",
      "new_obs: [[ 0.        0.        8.618186  0.        0.       13.39785   0.\n",
      "  17.799267 14.10087  15.758581  0.        0.       14.640791  0.\n",
      "   0.       15.826144  3.      ]]\n",
      "rewards: [0.03150831]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  8.6182,  0.0000,  0.0000, 13.3979,  0.0000, 17.7993,\n",
      "         14.1009, 15.7586,  0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261,\n",
      "          3.0000]])\n",
      "actions: [[-4.6285014  1.1700366 -1.9737338 11.675781 ]]\n",
      "values: tensor([[3.3100]])\n",
      "clipped_actions: [[ 0.         1.1700366  0.        11.675781 ]]\n",
      "new_obs: [[ 0.        0.        0.        0.        0.       17.799267 14.10087\n",
      "  15.758581  0.        0.       14.640791  0.        0.       15.826144\n",
      "  15.974512 14.589509  4.      ]]\n",
      "rewards: [1.0230284]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 17.7993, 14.1009, 15.7586,\n",
      "          0.0000,  0.0000, 14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895,\n",
      "          4.0000]])\n",
      "actions: [[ 3.7085311   0.9085406  -3.5271401   0.97789466]]\n",
      "values: tensor([[2.3524]])\n",
      "clipped_actions: [[3.7085311  0.9085406  0.         0.97789466]]\n",
      "new_obs: [[ 0.        0.        0.        0.       14.10087  15.758581  0.\n",
      "   0.       14.640791  0.        0.       15.826144 15.974512 14.589509\n",
      "  15.715956 15.271763  5.      ]]\n",
      "rewards: [1.0104808]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000, 14.1009, 15.7586,  0.0000,  0.0000,\n",
      "         14.6408,  0.0000,  0.0000, 15.8261, 15.9745, 14.5895, 15.7160, 15.2718,\n",
      "          5.0000]])\n",
      "actions: [[ 0.28366888  2.8721683  -1.6040547   3.0679455 ]]\n",
      "values: tensor([[1.4283]])\n",
      "clipped_actions: [[0.28366888 2.8721683  0.         3.0679455 ]]\n",
      "new_obs: [[ 0.        0.        0.        0.       15.324099 14.803132  0.\n",
      "  15.370173  0.        0.        0.       13.39785   0.       17.799267\n",
      "  14.10087  15.758581  0.      ]]\n",
      "rewards: [0.99955857]\n",
      "\n",
      "\n",
      "Log-Std at step 30726: [1.07828 1.07828 1.07828 1.07828]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m logpath \u001b[38;5;241m=\u001b[39m model_name[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m):]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogging at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:316\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    309\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:179\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 179\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\policies.py:647\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    645\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_features(obs)\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 647\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    649\u001b[0m     pi_features, vf_features \u001b[38;5;241m=\u001b[39m features\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:222\u001b[0m, in \u001b[0;36mMlpExtractor.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[th\u001b[38;5;241m.\u001b[39mTensor, th\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    :return: latent_policy, latent_value of the specified network.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m        If all layers are shared, then ``latent_policy == latent_value``\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_actor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_critic(features)\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\torch_layers.py:225\u001b[0m, in \u001b[0;36mMlpExtractor.forward_actor\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_actor\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\nn\\functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logpath = model_name[len(\"models/\"):]\n",
    "print(f\"logging at {logpath}\")\n",
    "model.learn(total_timesteps = total_timesteps,\n",
    "            progress_bar = False,\n",
    "            tb_log_name = logpath,\n",
    "            callback = callback,\n",
    "            reset_num_timesteps = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def save_next_file(directory, model_name):\n",
    "    base_pattern = re.compile(model_name + r\"_(\\d+)\\.zip\")\n",
    "    \n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    max_number = 0\n",
    "    for file in files:\n",
    "        match = base_pattern.match(file)\n",
    "        if match:\n",
    "            number = int(match.group(1))\n",
    "            max_number = max(max_number, number)\n",
    "    \n",
    "    # Generate the next filename\n",
    "    next_file_number = max_number + 1\n",
    "    next_file_name = f\"{model_name}_{next_file_number}\"\n",
    "    next_file_path = os.path.join(directory, next_file_name)\n",
    "    \n",
    "    model.save(next_file_path)\n",
    "    \n",
    "save_next_file(os.path.dirname(model_name), os.path.basename(model_name) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M,Q,P,B,Z,D = 10, 0, 5, 5, 1, 0\n",
    "M, Q, P, B, Z, D  = cfg[\"env\"][\"M\"], cfg[\"env\"][\"Q\"], cfg[\"env\"][\"P\"], cfg[\"env\"][\"B\"], cfg[\"env\"][\"Z\"], 0\n",
    "# M,Q,P,B,Z,D = 0, 0, 0, 0, 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = True, \n",
    "               D = cfg[\"env\"][\"D\"], \n",
    "               Q = cfg[\"env\"][\"Q\"], \n",
    "               P = cfg[\"env\"][\"P\"], \n",
    "               B = cfg[\"env\"][\"B\"], \n",
    "               Z = cfg[\"env\"][\"Z\"], \n",
    "               M = cfg[\"env\"][\"M\"],\n",
    "               reg = cfg[\"env\"][\"reg\"],\n",
    "               reg_lambda = cfg[\"env\"][\"reg_lambda\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"],\n",
    "               alpha = cfg[\"env\"][\"alpha\"],\n",
    "               beta = cfg[\"env\"][\"beta\"],\n",
    "               connections = connections, \n",
    "               action_sample = action_sample,\n",
    "               tau0 = tau0,delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 0, 'B': 0, 'P': 0, 'reg': 0}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.13335086}}, 'tau': {'s1': 0.106610715}, 'delta': {'p1': 0.022796683}}\n",
      "[PEN] t1; p1:\t\t\tsold too much (more than demand)\n",
      "Increased reward by 0.10661071538925171 through tank population in s1\n",
      "j1: inv: 0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.13335086405277252\n",
      "j1: b: 0.0\n",
      "[PEN] t1; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.106610715} {'j1': 0.0} {'p1': 0.0}\n",
      "    -10.04953682422638\n",
      "{'M': 0, 'B': -5, 'P': -5, 'reg': -0.2627582550048828}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.10351893}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.10351893305778503\n",
      "j1: b: 0.0\n",
      "[PEN] t2; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.106610715} {'j1': 0.0} {'p1': 0.0}\n",
      "    -15.153055757284164\n",
      "{'M': 0, 'B': -5, 'P': -10, 'reg': -0.36627718806266785}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.11744881}, 'delta': {'p1': 0.021353317}}\n",
      "[PEN] t3; p1:\t\t\tsold too much (more than demand)\n",
      "Increased reward by 0.11744880676269531 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.22405952} {'j1': 0.0} {'p1': 0.0}\n",
      "    -20.05696026980877\n",
      "{'M': 0, 'B': -10, 'P': -10, 'reg': -0.5050793141126633}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 0.06863385}}\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "p1: newdelta: 0.0\n",
      "[PEN] t4; p1:\t\t\tsold too much (resulting amount less than demand tank LB)\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.22405952} {'j1': 0.0} {'p1': 0.0}\n",
      "    -25.12559411674738\n",
      "{'M': 0, 'B': -15, 'P': -10, 'reg': -0.5737131610512733}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.07955089}, 'delta': {'p1': 0.047517113}}\n",
      "[PEN] t5; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "p1: newdelta: 0.0\n",
      "[PEN] t5; p1:\t\t\tsold too much (resulting amount less than demand tank LB)\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.22405952} {'j1': 0.0} {'p1': 0.0}\n",
      "    -35.252662129700184\n",
      "{'M': 0, 'B': -25, 'P': -10, 'reg': -0.7007811740040779}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.15008144}}, 'blend_demand': {'j1': {'p1': 0.1658755}}, 'tau': {'s1': 0.16930379}, 'delta': {'p1': 0.031857736}}\n",
      "[PEN] t6; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.15008144080638885, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.16587549448013306\n",
      "[PEN] t6; j1:\t\t\tIn and out flow both non-zero (in: 0.15, out: 0.17)\n",
      "p1: newdelta: 0.0\n",
      "[PEN] t6; p1:\t\t\tsold too much (resulting amount less than demand tank LB)\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.07397808} {'j1': 0.0} {'p1': 0.0}\n",
      "    -55.769780583679676\n"
     ]
    }
   ],
   "source": [
    "with th.autograd.set_detect_anomaly(True):\n",
    "    obs = env.reset()\n",
    "    obs, obs_dict = obs\n",
    "    for k in range(env.T):\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        print(env.pen_tracker)\n",
    "        print(\"\\n\\n   \",reconstruct_dict(action, env.mapping_act))\n",
    "        obs, reward, done, term, _ = env.step(action)\n",
    "        dobs = reconstruct_dict(obs, env.mapping_obs)\n",
    "        print(\"\\n    >>     \",dobs[\"sources\"], dobs[\"blenders\"], dobs[\"demands\"])\n",
    "        print(\"   \" ,reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0336, 0.0893, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.Tensor(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 (only once per episode)\n",
    "episode_rewards = []\n",
    "obs = env.reset()\n",
    "obs, obs_dict = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 1 Get first action\n",
    "print(env.t)\n",
    "action, _ = model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'s1': 17.46205}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_blend': {'s1': {'j1': 0.0}},\n",
       " 'blend_demand': {'j1': {'p1': 30.307917}},\n",
       " 'tau': {'s1': 8.731916},\n",
       " 'delta': {'p1': 17.08481}}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Visualize action\n",
    "print(env.t)\n",
    "reconstruct_dict(action, env.mapping_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Step once: get 2nd action\n",
    "print(env.t)\n",
    "obs, reward, done, term, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'s1': 26.193966}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# 4 Visualize new state\n",
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error [from server]: \n****************************************************************\n****************************************************************\n****************************************************************\n\nERROR: TensorBoard.dev has been shut down.\n\nThis command is no longer operational and will be removed.\n\nSee the FAQ at https://tensorboard.dev.\n\n****************************************************************\n****************************************************************\n****************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc1KCv3X3QvGwaXfgX1c4tg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[43mtb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExperimentFromDev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39mget_scalars()\n\u001b[0;32m      4\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\tensorboard\\data\\experimental\\experiment_from_dev.py:65\u001b[0m, in \u001b[0;36mExperimentFromDev.__init__\u001b[1;34m(self, experiment_id, api_endpoint)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_id \u001b[38;5;241m=\u001b[39m experiment_id\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client \u001b[38;5;241m=\u001b[39m \u001b[43mget_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_endpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_endpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\tensorboard\\data\\experimental\\experiment_from_dev.py:129\u001b[0m, in \u001b[0;36mget_api_client\u001b[1;34m(api_endpoint)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_api_client\u001b[39m(api_endpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m     server_info \u001b[38;5;241m=\u001b[39m _get_server_info(api_endpoint\u001b[38;5;241m=\u001b[39mapi_endpoint)\n\u001b[1;32m--> 129\u001b[0m     \u001b[43m_handle_server_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     channel_creds \u001b[38;5;241m=\u001b[39m grpc\u001b[38;5;241m.\u001b[39mssl_channel_credentials()\n\u001b[0;32m    131\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m auth\u001b[38;5;241m.\u001b[39mCredentialsStore()\u001b[38;5;241m.\u001b[39mread_credentials()\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\tensorboard\\data\\experimental\\experiment_from_dev.py:159\u001b[0m, in \u001b[0;36m_handle_server_info\u001b[1;34m(info)\u001b[0m\n\u001b[0;32m    157\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mverdict \u001b[38;5;241m==\u001b[39m server_info_pb2\u001b[38;5;241m.\u001b[39mVERDICT_ERROR:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError [from server]: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m compat\u001b[38;5;241m.\u001b[39mdetails)\n",
      "\u001b[1;31mValueError\u001b[0m: Error [from server]: \n****************************************************************\n****************************************************************\n****************************************************************\n\nERROR: TensorBoard.dev has been shut down.\n\nThis command is no longer operational and will be removed.\n\nSee the FAQ at https://tensorboard.dev.\n\n****************************************************************\n****************************************************************\n****************************************************************\n"
     ]
    }
   ],
   "source": [
    "experiment_id = \"c1KCv3X3QvGwaXfgX1c4tg\"\n",
    "experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "df = experiment.get_scalars()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blendv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
