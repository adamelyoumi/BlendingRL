{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increasingly difficult Environment\n",
    "- Positive reward for populating increasingly \"deep\" blending tanks ?\n",
    "- RL for chem sched paper (https://arxiv.org/pdf/2203.00636)\n",
    "- Masking (https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html, https://arxiv.org/pdf/2006.14171)\n",
    "    - Adding binary decision variables ?g  \n",
    "    - Requires discrete action space (only integer flows -> treated as categories ?)\n",
    "    - masking: disable incoming flows (resp. outgoing flows) for tanks at UB inv limit (resp. LB inv. limit), disable selling/buying when available = 0\n",
    "    - multiple envs with multiple agents ? (MARL, https://arxiv.org/pdf/2103.01955)\n",
    "        - Predict successive pipelines (\"source > blend\" then \"blend > blend\" (as many as required) then \"blend > demand\")\n",
    "        - Each agent has access to the whole state\n",
    "        - Action mask is derived from the previous agent's actions (0 if inventory at bounds or incoming flow already reserved, else 1)\n",
    "        - https://github.com/Rohan138/marl-baselines3/blob/main/marl_baselines3/independent_ppo.py\n",
    "- Safe RL: (https://proceedings.mlr.press/v119/wachi20a/wachi20a.pdf)\n",
    "    - \"Unsafe state\" ? > Do not enforce constraints strictly, instead opt for early episode termination to show which states are unsafe ? \n",
    "    - Implementations:\n",
    "        - https://pypi.org/project/fast-safe-rl/#description (Policy optimizers)\n",
    "        - https://github.com/PKU-Alignment/safety-gymnasium/tree/main/safety_gymnasium (environments; \"cost\" ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masking: Discretization of action space is too slow/might not work -> Need to implement masking for continuous action space\n",
    "- Recurrent policy makes the most sense ? (window of demand forecasts)\n",
    "- https://www.reddit.com/r/reinforcementlearning/comments/17l5b47/invalid_action_masking_when_action_space_is/\n",
    "    - Suggestion of autoregressive model for having constraints respected: one predicted action is input to a second model\n",
    "    - Suggestion of editing the distribution in such a way that the constraint is respected\n",
    "- https://www.sciencedirect.com/science/article/pii/S0098135420301599\n",
    "    - Choice of ELU activation ?\n",
    "    - Choice of NN size ?\n",
    "    - \"The feature engineering in the net inventory means the network does not have to learn these relationships itself, which did help speed training.\" ?\n",
    "- Simplify the problem (remove tanks 5 to 8), find the optimal solution with Gurobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proportional penalty instead of flat\n",
    "- Solution pool from Gurobi for data generation\n",
    "- Uniformize the distribution profile\n",
    "    - Idea is to remove start/end of episode effects to make the distribution simpler (see photo)\n",
    "    - -> Simulate infinite sup/dem profile\n",
    "    - -> simulate env with 12 time periods, only use the first 6 for the data, then do the same by shifting by 12.\n",
    "        - -> Need to implement non-zero initial inv states for both gym and gurobi\n",
    "\n",
    "- Clarify how DT works at inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "try:\n",
    "    print(curr_dir)\n",
    "except:\n",
    "    curr_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "    os.chdir(curr_dir)\n",
    "    print(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO, DDPG, SAC, TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import *\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.utils import safe_mean\n",
    "\n",
    "from envs import BlendEnv, flatten_and_track_mappings, reconstruct_dict\n",
    "from models import *\n",
    "from utils import *\n",
    "from math import exp, log\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(41_1111)|(42_1111)|(43_1111)|(44_1111)|(45_1111)|(46_1111)|(47_1111)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regex for tensorboard\n",
    "# Gives the current day's runs of the given config list\n",
    "\n",
    "L = []\n",
    "cfg_list = range(41, 48)\n",
    "for cfg in cfg_list:\n",
    "    L.append(\"(\" + str(cfg) + \"_\" + datetime.now().strftime('%m%d') + \")\")\n",
    "\"|\".join(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(/41/)|(/42/)|(/43/)|(/44/)|(/45/)|(/46/)|(/47/)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = []\n",
    "cfg_list = range(41, 48)\n",
    "for cfg in cfg_list:\n",
    "    L.append(\"(/\" + str(cfg) + \"/)\")\n",
    "\"|\".join(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## ENV CONFIGURATION ##############\n",
    "CONFIG = 34         # See /configs\n",
    "layout = \"simple\" # See /img\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./configs/{CONFIG}.yaml\", \"r\") as f:\n",
    "    s = \"\".join(f.readlines())\n",
    "    cfg = yaml.load(s, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"custom_softmax\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_softmax\n",
    "elif cfg[\"policytype\"] == \"MLP\":\n",
    "    policytype = \"MlpPolicy\"\n",
    "elif cfg[\"policytype\"] == \"MLPtanh\":\n",
    "    policytype = CustomMLP_ACP_simplest_tanh\n",
    "    \n",
    "optimizer_cls = eval(cfg[\"optimizer\"])\n",
    "\n",
    "if cfg[\"model\"][\"act_fn\"] == \"ReLU\":\n",
    "    act_cls = th.nn.ReLU\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"tanh\":\n",
    "    act_cls = th.nn.Tanh\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"sigmoid\":\n",
    "    act_cls = th.nn.Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections, action_sample = get_jsons(layout)\n",
    "sources, blenders, demands = get_sbp(connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 6\n",
    "if layout == \"base\":\n",
    "    sigma = {\"s1\":{\"q1\": 0.06}, \"s2\":{\"q1\": 0.26}}\n",
    "    sigma_ub = {\"p1\":{\"q1\": 0.16}, \"p2\":{\"q1\": 1}}\n",
    "    sigma_lb = {\"p1\":{\"q1\": 0}, \"p2\":{\"q1\": 0}}\n",
    "else:\n",
    "    sigma = {s:{\"q1\": 0.06} for s in sources}\n",
    "    sigma_ub = {d:{\"q1\": 0.16} for d in demands}\n",
    "    sigma_lb = {d:{\"q1\": 0} for d in demands}\n",
    "    \n",
    "s_inv_lb = {s: 0 for s in sources}\n",
    "s_inv_ub = {s: 999 for s in sources}\n",
    "d_inv_lb = {d: 0 for d in demands}\n",
    "d_inv_ub = {d: 999 for d in demands}\n",
    "betaT_d = {d: 1 for d in demands} # Price of sold products\n",
    "b_inv_ub = {j: 30 for j in blenders} \n",
    "b_inv_lb = {j: 0 for j in blenders}\n",
    "betaT_s = {s: cfg[\"env\"][\"product_cost\"]  for s in sources} # Cost of bought products\n",
    "\n",
    "if cfg[\"env\"][\"uniform_data\"]:\n",
    "    if cfg[\"env\"][\"max_pen_violations\"] < 999:\n",
    "        max_ep_length = 50\n",
    "        tau0   = {s: [np.random.normal(20, 3) for _ in range(max_ep_length)] for s in sources}\n",
    "        delta0 = {d: [np.random.normal(20, 3) for _ in range(max_ep_length)] for d in demands}\n",
    "        T = max_ep_length\n",
    "        \n",
    "    else:\n",
    "        tau0   = {s: [np.random.normal(20, 3) for _ in range(13)] for s in sources}\n",
    "        delta0 = {d: [np.random.normal(20, 3) for _ in range(13)] for d in demands}\n",
    "else:\n",
    "    tau0   = {s: [10, 10, 10, 0, 0, 0] for s in sources}\n",
    "    delta0 = {d: [0, 0, 0, 10, 10, 10] for d in demands}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = False, T = T,\n",
    "               D = cfg[\"env\"][\"D\"], Q = cfg[\"env\"][\"Q\"], P = cfg[\"env\"][\"P\"], B = cfg[\"env\"][\"B\"], Z = cfg[\"env\"][\"Z\"], M = cfg[\"env\"][\"M\"],\n",
    "               reg = cfg[\"env\"][\"reg\"], reg_lambda = cfg[\"env\"][\"reg_lambda\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"], alpha = cfg[\"env\"][\"alpha\"], beta = cfg[\"env\"][\"beta\"], \n",
    "               max_pen_violations = cfg[\"env\"][\"max_pen_violations\"], illeg_act_handling = cfg[\"env\"][\"illeg_act_handling\"],\n",
    "               connections = connections, action_sample = action_sample, \n",
    "               tau0 = tau0, delta0 = delta0, sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, \n",
    "                   norm_obs=cfg[\"obs_normalizer\"], \n",
    "                   norm_reward=cfg[\"reward_normalizer\"])\n",
    "# env = VecCheckNan(env, raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"], \n",
    "                   vf = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"])],\n",
    "    activation_fn = act_cls,\n",
    "    log_std_init = cfg[\"model\"][\"log_std_init\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpPolicy\n"
     ]
    }
   ],
   "source": [
    "print(policytype)\n",
    "\n",
    "if optimizer_cls == PPO:\n",
    "    kwa = dict(policy = policytype, \n",
    "                env = env,\n",
    "                tensorboard_log = \"./logs\",\n",
    "                clip_range = cfg[\"model\"][\"clip_range\"],\n",
    "                learning_rate = cfg[\"model\"][\"lr\"] if not cfg[\"model\"][\"lr_sched\"] else (lambda p: cfg[\"model\"][\"lr\"] + (cfg[\"model\"][\"lr_end\"] - cfg[\"model\"][\"lr\"]) * p),\n",
    "                ent_coef = cfg[\"model\"][\"ent_coef\"],\n",
    "                use_sde = cfg[\"model\"][\"use_sde\"],\n",
    "                batch_size = cfg[\"model\"][\"batch_size\"],\n",
    "                policy_kwargs = policy_kwargs)\n",
    "    \n",
    "else:\n",
    "    kwa = dict(policy = policytype, \n",
    "                env = env,\n",
    "                tensorboard_log = \"./logs\",\n",
    "                batch_size = cfg[\"model\"][\"batch_size\"],\n",
    "                learning_rate = cfg[\"model\"][\"lr\"])\n",
    "\n",
    "model = optimizer_cls(**kwa)\n",
    "\n",
    "if cfg[\"starting_point\"]:\n",
    "    try:\n",
    "        cfg_start = int(cfg[\"starting_point\"])\n",
    "        bin_ = get_bin(cfg_start)\n",
    "        directory = f\"C:\\\\Users\\\\adame\\\\OneDrive\\\\Bureau\\\\CODE\\\\BlendingRL\\\\models\\\\{layout}\\\\{bin_}\\\\{cfg_start}\"\n",
    "        chosen, mod_chosen = \"\", 0\n",
    "        for f in os.listdir(directory):\n",
    "            mod_time = os.path.getmtime(os.path.join(directory, f))\n",
    "            if mod_time > mod_chosen:\n",
    "                chosen = os.path.join(f\"models\\\\{layout}\\\\{bin_}\\\\{cfg_start}\", f)\n",
    "        model.set_parameters(chosen)\n",
    "        \n",
    "    except ValueError:\n",
    "        model.set_parameters(cfg[\"starting_point\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO(learning_rate=lambda p: a + (b-a)* p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If batch_size = 64 and n_steps = 2048, then 1 epoch = 2048/64 = 32 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/simple/25-36/34/34_1023-2149'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_ = get_bin(cfg['id'])\n",
    "entcoef = str(model.ent_coef) if type(model) == PPO else \"\"\n",
    "cliprange = str(model.clip_range(0)) if type(model) == PPO else \"\"\n",
    "model_name = f\"models/{layout}/{bin_}/{cfg['id']}/{cfg['id']}_{datetime.now().strftime('%m%d-%H%M')}\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallbackPPO(BaseCallback):\n",
    "    def __init__(self, schedule_timesteps, start_log_std=2, end_log_std=-1, std_control = cfg[\"clipped_std\"]):\n",
    "        super().__init__(verbose = 0)\n",
    "        self.print_flag = False\n",
    "        self.std_control = std_control\n",
    "        \n",
    "        self.start_log_std = start_log_std\n",
    "        self.end_log_std = end_log_std\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg, self.pen_nv = [], [], [], [], []\n",
    "        self.n_pen_M, self.n_pen_B, self.n_pen_P = [], [], []\n",
    "        self.units_sold, self.units_bought, self.rew_sold = [], [], []\n",
    "        \n",
    "        \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        self.logger.record(\"penalties/in_out\",              sum(self.pen_M)/len(self.pen_M))\n",
    "        self.logger.record(\"penalties/buysell_bounds\",      sum(self.pen_B)/len(self.pen_B))\n",
    "        self.logger.record(\"penalties/tank_bounds\",         sum(self.pen_P)/len(self.pen_P))\n",
    "        \n",
    "        self.logger.record(\"penalties/n_in_out\",            sum(self.n_pen_M)/len(self.n_pen_M))\n",
    "        self.logger.record(\"penalties/n_buysell_bounds\",    sum(self.n_pen_B)/len(self.n_pen_B))\n",
    "        self.logger.record(\"penalties/n_tank_bounds\",       sum(self.n_pen_P)/len(self.n_pen_P))\n",
    "        self.logger.record(\"penalties/n_vltn\",              sum(self.pen_nv)/len(self.pen_nv))\n",
    "        \n",
    "        self.logger.record(\"penalties/units_sold\",          sum(self.units_sold)/len(self.units_sold))\n",
    "        self.logger.record(\"penalties/units_bought\",        sum(self.units_bought)/len(self.units_bought))\n",
    "        self.logger.record(\"penalties/rew_sold\",            sum(self.rew_sold)/len(self.rew_sold))\n",
    "        \n",
    "        print(self.current_step, \":\", safe_mean([ep_info[\"r\"] for ep_info in model.ep_info_buffer]))\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg, self.pen_nv = [], [], [], [], []\n",
    "        self.n_pen_M, self.n_pen_B, self.n_pen_P = [], [], []\n",
    "        self.units_sold, self.units_bought, self.rew_sold = [], [], []\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        log_std: th.Tensor = self.model.policy.log_std\n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        \n",
    "        if self.locals[\"infos\"][0][\"terminated\"] or self.locals[\"infos\"][0][\"truncated\"]: # record info at each episode end\n",
    "            self.pen_M.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"M\"])\n",
    "            self.pen_B.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"B\"])\n",
    "            self.pen_P.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"P\"])\n",
    "            \n",
    "            self.n_pen_M.append(-self.locals[\"infos\"][0][\"pen_tracker\"][\"M\"]/cfg[\"env\"][\"M\"])\n",
    "            self.n_pen_B.append(-self.locals[\"infos\"][0][\"pen_tracker\"][\"B\"]/cfg[\"env\"][\"P\"])\n",
    "            self.n_pen_P.append(-self.locals[\"infos\"][0][\"pen_tracker\"][\"P\"]/cfg[\"env\"][\"B\"])\n",
    "            self.pen_nv.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"n_violations\"])\n",
    "            \n",
    "            self.units_sold.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"units_sold\"])\n",
    "            self.units_bought.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"units_bought\"])\n",
    "            self.rew_sold.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"rew_sold\"])\n",
    "        \n",
    "        # if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "        #     self.print_flag = True\n",
    "            \n",
    "        # if self.print_flag:\n",
    "        #     print(\"\\nt:\", t)\n",
    "        #     if np.isnan(self.locals['rewards'][0]) or np.isinf(self.locals['rewards'][0]):\n",
    "        #         print(f\"is invalid reward {self.locals['rewards'][0]}\")\n",
    "        #     for i in ['obs_tensor', 'actions', 'values', 'clipped_actions', 'new_obs', 'rewards']:\n",
    "        #         if i in self.locals:\n",
    "        #             print(f\"{i}: \" + str(self.locals[i]))\n",
    "        #     if t == 6:\n",
    "        #         self.print_flag = False\n",
    "        #         print(f\"\\n\\nLog-Std at step {self.num_timesteps}: {log_std.detach().cpu().numpy()}\")\n",
    "        #         print(self.locals[\"infos\"][0][\"pen_tracker\"])\n",
    "        #         print(\"\\n\\n\\n\\n\\n\")\n",
    "                \n",
    "        if self.std_control:\n",
    "            progress = self.current_step / self.schedule_timesteps\n",
    "            new_log_std = self.start_log_std + progress * (self.end_log_std - self.start_log_std)\n",
    "            self.model.policy.log_std.data.fill_(new_log_std)\n",
    "            self.current_step += 1\n",
    "                \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/simple/25-36/34/34_1023-2149'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_timesteps = 1e5\n",
    "log_callback = CustomLoggingCallbackPPO(schedule_timesteps=total_timesteps) if optimizer_cls == PPO else CustomLoggingCallbackDDPG()\n",
    "callback = CallbackList([log_callback])\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging at simple/25-36/34/34_1023-2149\n",
      "2048 : -38262.387173309995\n",
      "4096 : -35037.36655872\n",
      "6144 : -31934.28033633\n",
      "8192 : -26427.54269349\n",
      "10240 : -22356.47988014\n",
      "12288 : -19999.82570883\n",
      "14336 : -17064.042057069997\n",
      "16384 : -13799.584824749998\n",
      "18432 : -12075.939521870001\n",
      "20480 : -9942.88496025\n",
      "22528 : -8209.231710719998\n",
      "24576 : -6876.682527270002\n",
      "26624 : -5369.016219349999\n",
      "28672 : -4985.15545598\n",
      "30720 : -3927.5333796699997\n",
      "32768 : -2957.8563028500002\n",
      "34816 : -2561.7407125099994\n",
      "36864 : -2102.9350889700004\n",
      "38912 : -1468.6648103099997\n",
      "40960 : -1306.8101320300002\n",
      "43008 : -1103.7874256699997\n",
      "45056 : -809.20454111\n",
      "47104 : -581.8742708600001\n",
      "49152 : -481.5362704600001\n",
      "51200 : -310.08267188\n",
      "53248 : -174.79387455999998\n",
      "55296 : -91.19813708\n",
      "57344 : -9.051597730000003\n",
      "59392 : 52.89979792\n",
      "61440 : 101.94062487000001\n",
      "63488 : 127.89303318\n",
      "65536 : 158.61854684\n",
      "67584 : 166.65592941\n",
      "69632 : 179.93371213000003\n",
      "71680 : 186.37560455000002\n",
      "73728 : 192.88894446999998\n",
      "75776 : 195.36590621000002\n",
      "77824 : 200.09567018\n",
      "79872 : 211.97240551000002\n",
      "81920 : 217.22731166999998\n",
      "83968 : 220.89877489000003\n",
      "86016 : 220.99135057000007\n",
      "88064 : 227.93452644\n",
      "90112 : 231.49900805\n",
      "92160 : 235.30646333\n",
      "94208 : 239.50662777000005\n",
      "96256 : 253.45815829\n",
      "98304 : 266.96565509\n",
      "100352 : 283.89812062000004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x220bb68d9d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logpath = model_name[len(\"models/\"):]\n",
    "print(f\"logging at {logpath}\")\n",
    "model.learn(total_timesteps = total_timesteps,\n",
    "            progress_bar = False,\n",
    "            tb_log_name = logpath,\n",
    "            callback = callback,\n",
    "            reset_num_timesteps = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def save_next_file(directory, model_name):\n",
    "    base_pattern = re.compile(model_name + r\"_(\\d+)\\.zip\")\n",
    "    \n",
    "    try:\n",
    "        files = os.listdir(directory)\n",
    "    except:\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "        files = os.listdir(directory)\n",
    "        \n",
    "    max_number = 0\n",
    "    for file in files:\n",
    "        match = base_pattern.match(file)\n",
    "        if match:\n",
    "            number = int(match.group(1))\n",
    "            max_number = max(max_number, number)\n",
    "    \n",
    "    # Generate the next filename\n",
    "    next_file_number = max_number + 1\n",
    "    next_file_name = f\"{model_name}_{next_file_number}\"\n",
    "    next_file_path = os.path.join(directory, next_file_name)\n",
    "    \n",
    "    model.save(next_file_path)\n",
    "    \n",
    "save_next_file(os.path.dirname(model_name), os.path.basename(model_name) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"models\\\\simplest\\\\25-36\\\\30\\\\30_0930-1359_1.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M,Q,P,B,Z,D = 10, 0, 5, 5, 1, 0\n",
    "M, Q, P, B, Z, D  = cfg[\"env\"][\"M\"], cfg[\"env\"][\"Q\"], cfg[\"env\"][\"P\"], cfg[\"env\"][\"B\"], cfg[\"env\"][\"Z\"], 0\n",
    "# M,Q,P,B,Z,D = 0, 0, 0, 0, 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"env\"][\"uniform_data\"]:\n",
    "    tau0   = {s: [np.random.binomial(1, 0.7) * np.random.normal(15, 2) for _ in range(20)] for s in sources}\n",
    "    delta0 = {d: [np.random.binomial(1, 0.7) * np.random.normal(15, 2) for _ in range(20)] for d in demands}\n",
    "else:\n",
    "    tau0   = {s: [10, 10, 10, 0, 0, 0] for s in sources}\n",
    "    delta0 = {d: [0, 0, 0, 10, 10, 10] for d in demands}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = True, \n",
    "               D = cfg[\"env\"][\"D\"], \n",
    "               Q = cfg[\"env\"][\"Q\"], \n",
    "               P = cfg[\"env\"][\"P\"], \n",
    "               B = cfg[\"env\"][\"B\"], \n",
    "               Z = cfg[\"env\"][\"Z\"], \n",
    "               M = cfg[\"env\"][\"M\"],\n",
    "               reg = cfg[\"env\"][\"reg\"],\n",
    "               reg_lambda = cfg[\"env\"][\"reg_lambda\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"],\n",
    "               alpha = cfg[\"env\"][\"alpha\"],\n",
    "               beta = cfg[\"env\"][\"beta\"],\n",
    "               connections = connections, \n",
    "               action_sample = action_sample,\n",
    "               tau0 = tau0,delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " {'M': 0, 'B': 0, 'P': 0, 'Q': 0, 'reg': 0, 'n_violations': 0}\n",
      "[ 2.2490938  0.        10.549076   0.       ]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 2.2490938}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 10.549076}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 8.29998230934143 through tank population in s1\n",
      "j1: inv: 0, in_flow_sources: 2.249093770980835, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 4.49818754196167 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "[ 8.299982   2.2490938  0.         0.06      19.031122  17.232286\n",
      " 12.631584  20.010368  13.486182  11.07766   16.463406   0.\n",
      " 17.160105   0.        16.847889   0.         1.       ]\n",
      "\n",
      "    >>      {'s1': 8.299982} {'j1': 2.2490938} {'p1': 0.0}\n",
      "    10.449076080322266\n",
      "\n",
      "\n",
      " {'M': 0, 'B': 0, 'P': 0, 'Q': 0, 'reg': 0, 'n_violations': 0}\n",
      "[5.444898  0.        3.9590578 0.       ]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 5.444898}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 3.9590578}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 2.249093770980835, in_flow_sources: 5.4448981285095215, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 10.889796257019043 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "[ 6.814142    7.6939917   0.          0.10246091 12.631584   20.010368\n",
      " 13.486182   11.07766    16.463406    0.         17.160105    0.\n",
      " 16.847889    0.          0.         15.201728    2.        ]\n",
      "\n",
      "    >>      {'s1': 6.814142} {'j1': 7.6939917} {'p1': 0.0}\n",
      "    15.793974208831788\n",
      "\n",
      "\n",
      " {'M': 0, 'B': 0, 'P': 0, 'Q': 0, 'reg': 0, 'n_violations': 0}\n",
      "[0.        5.281314  0.9563463 0.       ]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 5.281314}}, 'tau': {'s1': 0.9563463}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 0.9563462734222412 through tank population in s1\n",
      "j1: inv: 7.6939918994903564, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 5.281313896179199\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 21.125255584716797 through tank population in p1\n",
      "[PEN] t3; p1; q1; j1:\t\t\tSold qualities out of bounds (-0.12)\n",
      "[ 7.7704883  2.412678   5.281314  -0.1218244 13.486182  11.07766\n",
      " 16.463406   0.        17.160105   0.        16.847889   0.\n",
      "  0.        15.201728  10.047319  15.311101   3.       ]\n",
      "\n",
      "    >>      {'s1': 7.7704883} {'j1': 2.412678} {'p1': 5.281314}\n",
      "    32.49426217079163\n",
      "\n",
      "\n",
      " {'M': 0, 'B': 0, 'P': 0, 'Q': 0, 'reg': 0, 'n_violations': 1}\n",
      "[0.         0.47993773 2.0407162  0.        ]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.47993773}}, 'tau': {'s1': 2.0407162}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 2.0407161712646484 through tank population in s1\n",
      "j1: inv: 2.4126780033111572, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.47993773221969604\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 1.9197509288787842 through tank population in p1\n",
      "[PEN] t4; p1; q1; j1:\t\t\tSold qualities out of bounds (-0.09)\n",
      "[ 9.811205    1.9327402   5.7612514  -0.09157299 16.463406    0.\n",
      " 17.160105    0.         16.847889    0.          0.         15.201728\n",
      " 10.047319   15.311101   14.976164   17.884268    4.        ]\n",
      "\n",
      "    >>      {'s1': 9.811205} {'j1': 1.9327402} {'p1': 5.7612514}\n",
      "    35.874791538715364\n",
      "\n",
      "\n",
      " {'M': 0, 'B': 0, 'P': 0, 'Q': 0, 'reg': 0, 'n_violations': 2}\n",
      "[0.         0.         0.6370637  0.22133237]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.6370637}, 'delta': {'p1': 0.22133237}}\n",
      "[PEN] t5; p1:\t\t\tsold too much (more than demand)\n",
      "Increased reward by 0.6370636820793152 through tank population in s1\n",
      "j1: inv: 1.9327402710914612, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "[10.448268    1.9327402   5.7612514  -0.09157299 17.160105    0.\n",
      " 16.847889    0.          0.         15.201728   10.047319   15.311101\n",
      " 14.976164   17.884268   12.588721   14.116285    5.        ]\n",
      "\n",
      "    >>      {'s1': 10.448268} {'j1': 1.9327402} {'p1': 5.7612514}\n",
      "    -63.48814477920533\n",
      "\n",
      "\n",
      " {'M': 0, 'B': -100, 'P': 0, 'Q': 0, 'reg': 0, 'n_violations': 3}\n",
      "[0.        0.        3.8217669 2.2020528]\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 3.8217669}, 'delta': {'p1': 2.2020528}}\n",
      "[PEN] t6; p1:\t\t\tsold too much (more than demand)\n",
      "Increased reward by 3.8217668533325195 through tank population in s1\n",
      "j1: inv: 1.9327402710914612, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "[14.270035    1.9327402   5.7612514  -0.09157299 16.847889    0.\n",
      "  0.         15.201728   10.047319   15.311101   14.976164   17.884268\n",
      " 12.588721   14.116285    0.         15.300578    6.        ]\n",
      "\n",
      "    >>      {'s1': 14.270035} {'j1': 1.9327402} {'p1': 5.7612514}\n",
      "    -159.6663779258728\n"
     ]
    }
   ],
   "source": [
    "with th.autograd.set_detect_anomaly(True):\n",
    "    obs = env.reset()\n",
    "    obs, obs_dict = obs\n",
    "    for k in range(env.T):\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        print(\"\\n\\n\",env.pen_tracker)\n",
    "        print(action)\n",
    "        print(\"\\n\\n   \",reconstruct_dict(action, env.mapping_act))\n",
    "        obs, reward, done, term, _ = env.step(action)\n",
    "        print(obs)\n",
    "        dobs = reconstruct_dict(obs, env.mapping_obs)\n",
    "        print(\"\\n    >>     \",dobs[\"sources\"], dobs[\"blenders\"], dobs[\"demands\"])\n",
    "        print(\"   \" ,reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 (only once per episode)\n",
    "episode_rewards = []\n",
    "obs = env.reset()\n",
    "obs, obs_dict = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 1 Get first action\n",
    "print(env.t)\n",
    "action, _ = model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'s1': 17.46205}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_blend': {'s1': {'j1': 0.0}},\n",
       " 'blend_demand': {'j1': {'p1': 30.307917}},\n",
       " 'tau': {'s1': 8.731916},\n",
       " 'delta': {'p1': 17.08481}}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Visualize action\n",
    "print(env.t)\n",
    "reconstruct_dict(action, env.mapping_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Step once: get 2nd action\n",
    "print(env.t)\n",
    "obs, reward, done, term, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'s1': 26.193966}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# 4 Visualize new state\n",
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blendv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
