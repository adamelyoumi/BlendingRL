{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN policy ?\n",
    "- grid search for HP tuning (OK)\n",
    "- Increasingly difficult Environment\n",
    "- Positive reward for populating increasingly \"deep\" blending tanks ?\n",
    "- RL for chem sched paper (https://arxiv.org/pdf/2203.00636)\n",
    "- Masking (https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html, https://arxiv.org/pdf/2006.14171)\n",
    "    - Adding binary decision variables ?g  \n",
    "    - Requires discrete action space (only integer flows -> treated as categories ?)\n",
    "    - masking: disable incoming flows (resp. outgoing flows) for tanks at UB inv limit (resp. LB inv. limit), disable selling/buying when available = 0\n",
    "    - multiple envs with multiple agents ? (MARL, https://arxiv.org/pdf/2103.01955)\n",
    "        - Predict successive pipelines (\"source > blend\" then \"blend > blend\" (as many as required) then \"blend > demand\")\n",
    "        - Each agent has access to the whole state\n",
    "        - Action mask is derived from the previous agent's actions (0 if inventory at bounds or incoming flow already reserved, else 1)\n",
    "        - https://github.com/Rohan138/marl-baselines3/blob/main/marl_baselines3/independent_ppo.py\n",
    "- Safe RL: (https://proceedings.mlr.press/v119/wachi20a/wachi20a.pdf)\n",
    "    - \"Unsafe state\" ? > Do not enforce constraints strictly, instead opt for early episode termination to show which states are unsafe ? \n",
    "    - Implementations:\n",
    "        - https://pypi.org/project/fast-safe-rl/#description (Policy optimizers)\n",
    "        - https://github.com/PKU-Alignment/safety-gymnasium/tree/main/safety_gymnasium (environments; \"cost\" ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try other learning rates/CNN policies\n",
    "2. Implement Masking with single agent\n",
    "3. Try other ways to tell the model what are illegal/unsafe states (safe RL)\n",
    "4. Try multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masking: Discretization of action space is too slow/might not work -> Need to implement masking for continuous action space\n",
    "- Recurrent policy makes the most sense ? (window of demand forecasts)\n",
    "- https://www.reddit.com/r/reinforcementlearning/comments/17l5b47/invalid_action_masking_when_action_space_is/\n",
    "    - Suggestion of autoregressive model for having constraints respected: one predicted action is input to a second model\n",
    "    - Suggestion of editing the distribution in such a way that the constraint is respected\n",
    "- https://www.sciencedirect.com/science/article/pii/S0098135420301599\n",
    "    - Choice of ELU activation ?\n",
    "    - Choice of NN size ?\n",
    "    - \"The feature engineering in the net inventory means the network does not have to learn these relationships itself, which did help speed training.\" ?\n",
    "- Simplify the problem (remove tanks 5 to 8), find the optimal solution with Gurobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove all constraints except in/out\n",
    "- Semantic Loss : https://arxiv.org/pdf/1711.11157\n",
    "- MultiplexNet : https://arxiv.org/pdf/2111.01564\n",
    "- Softmax with large coef to produce action mask\n",
    "- Graph convolution NN instead of RNN ?\n",
    "    - https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "    - Graph rep. learning - William L Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPG\n",
    "- Softmax\n",
    "- ~~Remove non-selling rewards~~\n",
    "- MultiplexNet\n",
    "- Why softmax doesn't work ? -> gradient doesn't compute properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finalize adjustment of flows\n",
    "- Add more difficulty (bigger env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "import json\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import *\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
    "\n",
    "from envs import BlendEnv, flatten_and_track_mappings, reconstruct_dict\n",
    "from models import *\n",
    "from math import exp, log\n",
    "import yaml\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( Regexp for Tensorboard coloring )\n",
    "\n",
    "(1\\\\|2\\\\|3\\\\|4\\\\|5\\\\|6\\\\|7\\\\|8\\\\|9\\\\|10\\\\|11\\\\|12\\\\|13\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/3.yaml\", \"r\") as f:\n",
    "    s = \"\".join(f.readlines())\n",
    "    cfg = yaml.load(s, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](../simple.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"clipped_std\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_std\n",
    "elif cfg[\"custom_softmax\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_softmax\n",
    "elif cfg[\"policytype\"] == \"MLP\":\n",
    "    policytype = \"MlpPolicy\"\n",
    "elif cfg[\"policytype\"] == \"MLPtanh\":\n",
    "    policytype = CustomMLP_ACP_simplest_tanh\n",
    "    \n",
    "if cfg[\"optimizer\"] == \"PPO\":\n",
    "    optimizer_cls = PPO\n",
    "elif cfg[\"optimizer\"] == \"DDPG\":\n",
    "    optimizer_cls = DDPG\n",
    "\n",
    "if cfg[\"model\"][\"act_fn\"] == \"ReLU\":\n",
    "    act_cls = th.nn.ReLU\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"tanh\":\n",
    "    act_cls = th.nn.Tanh\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"sigmoid\":\n",
    "    act_cls = th.nn.Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = {\n",
    "    \"source_blend\": {\n",
    "        \"s1\": [\"j1\", \"j2\", \"j3\", \"j4\"],\n",
    "        \"s2\": [\"j1\", \"j2\", \"j3\", \"j4\"]\n",
    "    },\n",
    "    \"blend_blend\": {\"j1\": [], \"j2\": [], \"j3\": [], \"j4\": []},\n",
    "    \"blend_demand\": {\"j1\": [\"p1\", \"p2\"],\n",
    "        \"j2\": [\"p1\", \"p2\"],\n",
    "        \"j3\": [\"p1\", \"p2\"],\n",
    "        \"j4\": [\"p1\", \"p2\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sample = {\n",
    "    'source_blend':{\n",
    "        's1': {'j1':1, 'j2':1, 'j3':1, 'j4':0}, # From s1 to b1, from s1 to b2 etc...\n",
    "        's2': {'j1':1, 'j2':1, 'j3':0, 'j4':1},\n",
    "    },\n",
    "    \n",
    "    'blend_blend':{},\n",
    "    \n",
    "    'blend_demand':{\n",
    "        'j1': {'p1':1, 'p2':0},\n",
    "        'j2': {'p1':1, 'p2':2},\n",
    "        'j3': {'p1':1, 'p2':2},\n",
    "        'j4': {'p1':1, 'p2':2}\n",
    "    },\n",
    "    \n",
    "    \"tau\": {\"s1\": 10, \"s2\": 10},\n",
    "    \n",
    "    \"delta\": {\"p1\": 0, \"p2\": 0}\n",
    "}\n",
    "action_sample_flat, _ = flatten_and_track_mappings(action_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau0   = {'s1': [10, 10, 10, 0, 0, 0], 's2': [10, 10, 10, 0, 0, 0]}\n",
    "delta0 = {'p1': [0, 0, 0, 10, 10, 10], 'p2': [0, 0, 0, 10, 10, 10]}\n",
    "sigma = {\"s1\":{\"q1\": 0.06}, \"s2\":{\"q1\": 0.26}} # Source concentrations\n",
    "sigma_ub = {\"p1\":{\"q1\": 0.16}, \"p2\":{\"q1\": 0.16}} # Demand concentrations UBs/LBs\n",
    "sigma_lb = {\"p1\":{\"q1\": 0}, \"p2\":{\"q1\": 0}}\n",
    "s_inv_lb = {'s1': 0, 's2': 0}\n",
    "s_inv_ub = {'s1': 999, 's2': 999}\n",
    "d_inv_lb = {'p1': 0, 'p2': 0}\n",
    "d_inv_ub = {'p1': 999, 'p2': 999}\n",
    "betaT_d = {'p1': 1, 'p2': 1} # Price of sold products\n",
    "betaT_s = {'s1': cfg[\"env\"][\"product_cost\"], 's2': cfg[\"env\"][\"product_cost\"]} # Cost of bought products\n",
    "b_inv_ub = {\"j1\": 30, \"j2\": 30, \"j3\": 30, \"j4\": 30} \n",
    "b_inv_lb = {j:0 for j in b_inv_ub.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = False, \n",
    "               D = cfg[\"env\"][\"D\"], \n",
    "               Q = cfg[\"env\"][\"Q\"], \n",
    "               P = cfg[\"env\"][\"P\"], \n",
    "               B = cfg[\"env\"][\"B\"], \n",
    "               Z = cfg[\"env\"][\"Z\"], \n",
    "               M = cfg[\"env\"][\"M\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"],\n",
    "               alpha = cfg[\"env\"][\"alpha\"],\n",
    "               beta = cfg[\"env\"][\"beta\"],\n",
    "               connections = connections, \n",
    "               action_sample = action_sample,\n",
    "               tau0 = tau0,delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "\n",
    "env = Monitor(env)\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "# env = VecNormalize(env, \n",
    "#                    norm_obs=cfg[\"obs_normalizer\"], \n",
    "#                    norm_reward=cfg[\"reward_normalizer\"])\n",
    "# env = VecCheckNan(env, raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ['sources', 's1']),\n",
       " (1, ['sources', 's2']),\n",
       " (2, ['blenders', 'j1']),\n",
       " (3, ['blenders', 'j2']),\n",
       " (4, ['blenders', 'j3']),\n",
       " (5, ['blenders', 'j4']),\n",
       " (6, ['demands', 'p1']),\n",
       " (7, ['demands', 'p2']),\n",
       " (8, ['properties', 'j1', 'q1']),\n",
       " (9, ['properties', 'j2', 'q1']),\n",
       " (10, ['properties', 'j3', 'q1']),\n",
       " (11, ['properties', 'j4', 'q1']),\n",
       " (12, ['sources_avail_next_0', 's1']),\n",
       " (13, ['sources_avail_next_0', 's2']),\n",
       " (14, ['demands_avail_next_0', 'p1']),\n",
       " (15, ['demands_avail_next_0', 'p2']),\n",
       " (16, ['sources_avail_next_1', 's1']),\n",
       " (17, ['sources_avail_next_1', 's2']),\n",
       " (18, ['demands_avail_next_1', 'p1']),\n",
       " (19, ['demands_avail_next_1', 'p2']),\n",
       " (20, ['sources_avail_next_2', 's1']),\n",
       " (21, ['sources_avail_next_2', 's2']),\n",
       " (22, ['demands_avail_next_2', 'p1']),\n",
       " (23, ['demands_avail_next_2', 'p2']),\n",
       " (24, ['sources_avail_next_3', 's1']),\n",
       " (25, ['sources_avail_next_3', 's2']),\n",
       " (26, ['demands_avail_next_3', 'p1']),\n",
       " (27, ['demands_avail_next_3', 'p2']),\n",
       " (28, ['sources_avail_next_4', 's1']),\n",
       " (29, ['sources_avail_next_4', 's2']),\n",
       " (30, ['demands_avail_next_4', 'p1']),\n",
       " (31, ['demands_avail_next_4', 'p2']),\n",
       " (32, ['sources_avail_next_5', 's1']),\n",
       " (33, ['sources_avail_next_5', 's2']),\n",
       " (34, ['demands_avail_next_5', 'p1']),\n",
       " (35, ['demands_avail_next_5', 'p2']),\n",
       " (36, ['t'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.mapping_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"], \n",
    "                   vf = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"])],\n",
    "    activation_fn = act_cls,\n",
    "    log_std_init = cfg[\"model\"][\"log_std_init\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpPolicy\n"
     ]
    }
   ],
   "source": [
    "print(policytype)\n",
    "model = optimizer_cls(policytype, \n",
    "                    env,\n",
    "                    tensorboard_log = \"./logs\",\n",
    "                    clip_range = cfg[\"model\"][\"clip_range\"],\n",
    "                    learning_rate = cfg[\"model\"][\"lr\"],\n",
    "                    ent_coef = cfg[\"model\"][\"ent_coef\"],\n",
    "                    policy_kwargs = policy_kwargs)\n",
    "\n",
    "if cfg[\"starting_point\"]:\n",
    "    model.set_parameters(cfg[\"starting_point\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "if type(model.policy) == CustomRNN_ACP:\n",
    "    policytype = \"CRNN\"\n",
    "elif type(model.policy) == CustomMLP_ACP_simplest_std:\n",
    "    policytype = \"CMLP\"\n",
    "else:\n",
    "    policytype = \"MLP\"\n",
    "    \n",
    "entcoef = str(model.ent_coef) if type(model) == PPO else \"\"\n",
    "cliprange = str(model.clip_range(0)) if type(model) == PPO else \"\"\n",
    "model_name = f\"models/simple/{cfg['id']}/{cfg['id']}_{datetime.datetime.now().strftime('%m%d-%H%M')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_stds = []\n",
    "        self.total_rewards = []\n",
    "        self.signal = True\n",
    "        self.update1 = True\n",
    "        self.print_flag = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        log_std: th.Tensor = self.model.policy.log_std\n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        \n",
    "        if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "            self.print_flag = True\n",
    "            \n",
    "        if self.print_flag:\n",
    "            print(\"\\nt:\", t)\n",
    "            if np.isnan(self.locals['rewards'][0]) or np.isinf(self.locals['rewards'][0]):\n",
    "                print(f\"is invalid reward {self.locals['rewards'][0]}\")\n",
    "            for i in ['obs_tensor', 'actions', 'values', 'clipped_actions', 'new_obs', 'rewards']:\n",
    "                if i in self.locals:\n",
    "                    print(f\"{i}: \" + str(self.locals[i]))\n",
    "            if t == 6:\n",
    "                stds = th.exp(self.model.policy.log_std).mean().item()\n",
    "        \n",
    "                if stds > 50:\n",
    "                    print(\"clipping log-stds\")\n",
    "                    print(\"before: \", self.model.policy.log_std)\n",
    "                    self.model.policy.log_std = nn.Parameter( 2*th.ones(self.model.policy.log_std.shape, requires_grad=True) )\n",
    "                    print(\"after: \",  self.model.policy.log_std)\n",
    "                    \n",
    "                self.logger.record('train/learning_rate', self.model.learning_rate)\n",
    "                self.logger.record('train/clip_range', self.model.clip_range(0))\n",
    "                self.logger.record(\"train/std\", th.exp(self.model.policy.log_std).mean().item())\n",
    "                \n",
    "                # if self.locals['rewards'][0] > 200 and self.update1:\n",
    "                #     # self.training_env.set_attr('', 1e2)\n",
    "                #     self.model.learning_rate = 1e2\n",
    "                #     self.model.clip_range = 5e-2\n",
    "                #     self.update1 = False\n",
    "                \n",
    "                self.print_flag = False\n",
    "                print(f\"\\n\\nLog-Std at step {self.num_timesteps}: {log_std.detach().numpy()}\")\n",
    "                self.log_stds.append(log_std.mean().item())\n",
    "                self.total_rewards.append(self.locals['rewards'][0])\n",
    "                print(f\"\\nAvg rewards over the last 100 episodes:{sum(self.total_rewards[-100:])/100} ; last reward: {self.total_rewards[-1]}\")\n",
    "                self.model.learning_rate\n",
    "                print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "                \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/simple/3/3_0705-1707'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_callback = CustomLoggingCallback()\n",
    "callback = CallbackList([log_callback])\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging at simple/3/3_0705-1707\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[-6.1984297e-02 -2.4606434e-01  2.5531482e-03  5.9051428e-02\n",
      "  -2.6110533e-01 -2.9601622e-01  1.5054165e-01  1.0394938e-02\n",
      "  -2.3558119e-01 -1.1127918e-01  1.0132976e-04  1.2141154e-01\n",
      "   2.7788594e-01 -1.5903588e-01 -1.0156837e-01 -1.2808603e-01\n",
      "  -2.7372128e-01 -6.1988190e-02  1.3421952e-02 -1.6627008e-01]]\n",
      "values: tensor([[1.1313]])\n",
      "clipped_actions: [[0.0000000e+00 0.0000000e+00 2.5531482e-03 5.9051428e-02 0.0000000e+00\n",
      "  0.0000000e+00 1.5054165e-01 1.0394938e-02 0.0000000e+00 0.0000000e+00\n",
      "  1.0132976e-04 1.2141154e-01 2.7788594e-01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3421952e-02 0.0000000e+00]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0.  0.  0.\n",
      "   1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0.,  0.,  0.,  1.]])\n",
      "actions: [[ 0.02082834 -0.13863204  0.1792112  -0.23830424  0.02875639 -0.0905029\n",
      "  -0.24343169 -0.26962337  0.02419415 -0.01066689 -0.00785924 -0.13499162\n",
      "   0.30079168  0.298557   -0.04649375 -0.22688521  0.03203183  0.01286172\n",
      "   0.05707626 -0.14817569]]\n",
      "values: tensor([[0.8448]])\n",
      "clipped_actions: [[0.02082834 0.         0.1792112  0.         0.02875639 0.\n",
      "  0.         0.         0.02419415 0.         0.         0.\n",
      "  0.30079168 0.298557   0.         0.         0.03203183 0.01286172\n",
      "  0.05707626 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.  0.  0.\n",
      "  10. 10.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   2.]]\n",
      "rewards: [-1.9999994]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0.,  0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0., 10., 10.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  2.]])\n",
      "actions: [[-0.04421809 -0.01581908  0.09795261 -0.13782424 -0.14741823  0.0347512\n",
      "  -0.01958043  0.09361301  0.02815229 -0.0279583   0.10655996 -0.07877267\n",
      "  -0.03105977 -0.14669336  0.16730115 -0.03116098  0.01040635 -0.16892552\n",
      "  -0.10479333  0.09023234]]\n",
      "values: tensor([[-1.5964]])\n",
      "clipped_actions: [[0.         0.         0.09795261 0.         0.         0.0347512\n",
      "  0.         0.09361301 0.02815229 0.         0.10655996 0.\n",
      "  0.         0.         0.16730115 0.         0.01040635 0.\n",
      "  0.         0.09023234]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.01040635  0.\n",
      "   0.          0.          0.          0.          0.06        0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.        ]]\n",
      "rewards: [-1.2302451]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0104,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  3.0000]])\n",
      "actions: [[-0.01190307  0.01062645  0.05494334  0.29727134  0.032943    0.1189036\n",
      "   0.16573901  0.14286986  0.02665925  0.08394554  0.04878376 -0.05235567\n",
      "  -0.2986863   0.10958961  0.13277942 -0.04032426  0.00676341  0.2140281\n",
      "  -0.13913235  0.16793452]]\n",
      "values: tensor([[-0.8485]])\n",
      "clipped_actions: [[0.         0.01062645 0.05494334 0.29727134 0.032943   0.1189036\n",
      "  0.16573901 0.14286986 0.02665925 0.08394554 0.04878376 0.\n",
      "  0.         0.10958961 0.13277942 0.         0.00676341 0.2140281\n",
      "  0.         0.16793452]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.\n",
      "  10. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   4.]]\n",
      "rewards: [-0.9353933]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.]])\n",
      "actions: [[-0.00063985 -0.04922804  0.13018246  0.08651102 -0.06696275 -0.18209732\n",
      "   0.11838713 -0.08661056 -0.17030597 -0.04189034 -0.07144004 -0.20604299\n",
      "   0.16515233  0.06040237 -0.08582824  0.03959009  0.13261919  0.11044271\n",
      "   0.01799345 -0.09579284]]\n",
      "values: tensor([[-0.5396]])\n",
      "clipped_actions: [[0.         0.         0.13018246 0.08651102 0.         0.\n",
      "  0.11838713 0.         0.         0.         0.         0.\n",
      "  0.16515233 0.06040237 0.         0.03959009 0.13261919 0.11044271\n",
      "  0.01799345 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   5.]]\n",
      "rewards: [-0.7356454]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.]])\n",
      "actions: [[ 0.04870961  0.15839517 -0.08549979 -0.06252139  0.06600268 -0.01343333\n",
      "  -0.13875686 -0.00318224 -0.21213585 -0.11048516 -0.10634792 -0.02402552\n",
      "  -0.1414264  -0.17883682  0.01347254 -0.23256908  0.09075078  0.02322539\n",
      "   0.14598367 -0.23201415]]\n",
      "values: tensor([[0.0975]])\n",
      "clipped_actions: [[0.04870961 0.15839517 0.         0.         0.06600268 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01347254 0.         0.09075078 0.02322539\n",
      "  0.14598367 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.6082398]\n",
      "\n",
      "\n",
      "Log-Std at step 6: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.006082398295402527 ; last reward: -0.6082398295402527\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[-0.06067218 -0.08123575 -0.08994604  0.04523395 -0.23951311 -0.18427835\n",
      "   0.14669965 -0.06071127 -0.01554761 -0.07318118 -0.0310696  -0.08661659\n",
      "   0.04480227  0.01403931  0.02494204 -0.142912    0.16971734 -0.17650613\n",
      "  -0.1104986  -0.14013258]]\n",
      "values: tensor([[-1149762.2500]])\n",
      "clipped_actions: [[0.         0.         0.         0.04523395 0.         0.\n",
      "  0.14669965 0.         0.         0.         0.         0.\n",
      "  0.04480227 0.01403931 0.02494204 0.         0.16971734 0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.12448339  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.12255165]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.1245,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[-0.05663754 -0.18268855 -0.06303484  0.08135032 -0.11049889 -0.05392573\n",
      "   0.06239394 -0.05150359  0.01656849 -0.06011531 -0.01851156 -0.17433578\n",
      "  -0.14571199  0.23428842 -0.05917434 -0.0315216  -0.23540913  0.25438243\n",
      "  -0.07525764  0.05816682]]\n",
      "values: tensor([[-1013255.3125]])\n",
      "clipped_actions: [[0.         0.         0.         0.08135032 0.         0.\n",
      "  0.06239394 0.         0.01656849 0.         0.         0.\n",
      "  0.         0.23428842 0.         0.         0.         0.25438243\n",
      "  0.         0.05816682]]\n",
      "new_obs: [[ 0.04313308  0.1919885   0.          0.          0.          0.08135032\n",
      "   0.          0.          0.          0.          0.          0.06\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   2.        ]]\n",
      "rewards: [-0.24227399]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0431,  0.1920,  0.0000,  0.0000,  0.0000,  0.0814,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0600, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  2.0000]])\n",
      "actions: [[ 0.08079313 -0.11855187  0.0804664   0.10720655 -0.16438031  0.23492302\n",
      "  -0.0057265  -0.11766318  0.01237891 -0.01261248 -0.283391    0.11333474\n",
      "  -0.02482973  0.1562339  -0.03254823  0.07183635  0.01372241  0.00204304\n",
      "  -0.12207645  0.01218722]]\n",
      "values: tensor([[-823544.4375]])\n",
      "clipped_actions: [[0.08079313 0.         0.0804664  0.10720655 0.         0.23492302\n",
      "  0.         0.         0.01237891 0.         0.         0.11333474\n",
      "  0.         0.1562339  0.         0.07183635 0.01372241 0.00204304\n",
      "  0.         0.01218722]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.08135032\n",
      "   0.          0.          0.          0.          0.          0.06\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.        ]]\n",
      "rewards: [-0.73889583]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0814,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0600,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  3.0000]])\n",
      "actions: [[-0.24538593 -0.02859851 -0.09868855 -0.20325233 -0.05410088  0.16389968\n",
      "   0.0025113  -0.23323806  0.17664948 -0.02494129 -0.05900735  0.08163939\n",
      "   0.01157049 -0.03071974 -0.16578107  0.16435422 -0.04515388  0.01166555\n",
      "   0.02975771 -0.10552437]]\n",
      "values: tensor([[-433869.1562]])\n",
      "clipped_actions: [[0.         0.         0.         0.         0.         0.16389968\n",
      "  0.0025113  0.         0.17664948 0.         0.         0.08163939\n",
      "  0.01157049 0.         0.         0.16435422 0.         0.01166555\n",
      "  0.02975771 0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.08135032  0.          0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   4.        ]]\n",
      "rewards: [-0.7349403]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0814,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  4.0000]])\n",
      "actions: [[-0.16956131 -0.17914656  0.22038981  0.1383742  -0.07511728 -0.22514592\n",
      "  -0.16088854  0.00906938 -0.08149615  0.05009404 -0.10634794 -0.2222269\n",
      "   0.00121924  0.24484381 -0.07192928  0.07011741 -0.04558799  0.09926019\n",
      "   0.03480573  0.23007096]]\n",
      "values: tensor([[-253240.9844]])\n",
      "clipped_actions: [[0.         0.         0.22038981 0.1383742  0.         0.\n",
      "  0.         0.00906938 0.         0.05009404 0.         0.\n",
      "  0.00121924 0.24484381 0.         0.07011741 0.         0.09926019\n",
      "  0.03480573 0.23007096]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   5.]]\n",
      "rewards: [-0.8356589]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.]])\n",
      "actions: [[ 0.04643224 -0.15785052  0.10922156  0.1436458   0.06368899 -0.23738188\n",
      "  -0.14086254 -0.00101855  0.04854566 -0.04746441  0.1674501  -0.08705477\n",
      "   0.16561693  0.1480584  -0.18182616  0.14544365  0.03163456 -0.10774377\n",
      "  -0.11115284  0.14828691]]\n",
      "values: tensor([[-101116.5391]])\n",
      "clipped_actions: [[0.04643224 0.         0.10922156 0.1436458  0.06368899 0.\n",
      "  0.         0.         0.04854566 0.         0.1674501  0.\n",
      "  0.16561693 0.1480584  0.         0.14544365 0.03163456 0.\n",
      "  0.         0.14828691]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.8347395]\n",
      "\n",
      "\n",
      "Log-Std at step 2058: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.014429793357849122 ; last reward: -0.8347395062446594\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.14010896  0.20646675 -0.05852996  0.10685797 -0.01307643 -0.1898124\n",
      "   0.18314679  0.17497258 -0.11994147  0.02593958  0.14414424 -0.18552318\n",
      "  -0.25209248 -0.21624495  0.21575996 -0.06242753  0.04294942  0.09854446\n",
      "   0.1323675  -0.4120134 ]]\n",
      "values: tensor([[-1258983.5000]])\n",
      "clipped_actions: [[0.14010896 0.20646675 0.         0.10685797 0.         0.\n",
      "  0.18314679 0.17497258 0.         0.02593958 0.14414424 0.\n",
      "  0.         0.         0.21575996 0.         0.04294942 0.09854446\n",
      "  0.1323675  0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.05039689  0.\n",
      "   0.          0.          0.          0.          0.26        0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.37026408]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0504,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.2600,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[-0.01234605 -0.05479921  0.17591839  0.00259446 -0.26272509  0.02223442\n",
      "  -0.03956933 -0.06268969  0.09317548  0.05611776 -0.3176449   0.09156834\n",
      "   0.20676138  0.15051714 -0.137355   -0.11306302  0.01062727 -0.10140791\n",
      "   0.03924046 -0.10340435]]\n",
      "values: tensor([[-1109667.1250]])\n",
      "clipped_actions: [[0.         0.         0.17591839 0.00259446 0.         0.02223442\n",
      "  0.         0.         0.09317548 0.05611776 0.         0.09156834\n",
      "  0.20676138 0.15051714 0.         0.         0.01062727 0.\n",
      "  0.03924046 0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.0396889e-02\n",
      "  1.5445387e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  2.5999999e-01 5.9999999e-02 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 2.0000000e+00]]\n",
      "rewards: [-0.49415606]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0397e-02, 1.5445e-04,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6000e-01, 6.0000e-02,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0000e+00]])\n",
      "actions: [[ 3.05835843e-01 -3.94177400e-02 -1.45246595e-01  2.03740187e-02\n",
      "  -8.08514357e-02 -1.24335289e-04  2.95202196e-01  2.93162651e-02\n",
      "  -3.65108997e-02 -1.85928255e-01  4.98531610e-02 -2.35251814e-01\n",
      "  -1.54814735e-01  1.97392032e-01  8.14130343e-03  6.42016083e-02\n",
      "   3.61898504e-02  1.04342885e-01  2.09998757e-01 -1.19177394e-01]]\n",
      "values: tensor([[-903626.1250]])\n",
      "clipped_actions: [[0.30583584 0.         0.         0.02037402 0.         0.\n",
      "  0.2952022  0.02931627 0.         0.         0.04985316 0.\n",
      "  0.         0.19739203 0.0081413  0.06420161 0.03618985 0.10434289\n",
      "  0.20999876 0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 3.3929549e-02 0.0000000e+00 5.0396889e-02\n",
      "  1.5445387e-04 0.0000000e+00 0.0000000e+00 5.9999999e-02 0.0000000e+00\n",
      "  2.5999999e-01 5.9999999e-02 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [-0.7410823]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 3.3930e-02, 0.0000e+00, 5.0397e-02, 1.5445e-04,\n",
      "         0.0000e+00, 0.0000e+00, 6.0000e-02, 0.0000e+00, 2.6000e-01, 6.0000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e+00]])\n",
      "actions: [[ 0.01430515  0.13829553 -0.0516582   0.21169461  0.10093999  0.05553652\n",
      "   0.14399026  0.05829513  0.11527895  0.19687158  0.14823201 -0.16118093\n",
      "   0.00936668  0.12697095  0.05217995  0.08940109  0.26106718  0.23151216\n",
      "   0.11312608  0.08361769]]\n",
      "values: tensor([[-487981.2188]])\n",
      "clipped_actions: [[0.01430515 0.13829553 0.         0.21169461 0.10093999 0.05553652\n",
      "  0.14399026 0.05829513 0.11527895 0.19687158 0.14823201 0.\n",
      "  0.00936668 0.12697095 0.05217995 0.08940109 0.26106718 0.23151216\n",
      "  0.11312608 0.08361769]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.\n",
      "  10. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   4.]]\n",
      "rewards: [-0.8456282]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.]])\n",
      "actions: [[ 0.12145872  0.01744191 -0.01077832  0.05178066 -0.01239577  0.12294361\n",
      "  -0.22273934 -0.0443122  -0.11071432 -0.12656319  0.06392767 -0.1107236\n",
      "   0.08403473  0.00212856 -0.03830121 -0.07045679  0.01889328  0.09380722\n",
      "  -0.09358761  0.21260062]]\n",
      "values: tensor([[-287095.4688]])\n",
      "clipped_actions: [[0.12145872 0.01744191 0.         0.05178066 0.         0.12294361\n",
      "  0.         0.         0.         0.         0.06392767 0.\n",
      "  0.08403473 0.00212856 0.         0.         0.01889328 0.09380722\n",
      "  0.         0.21260062]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   5.]]\n",
      "rewards: [-0.84523064]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.]])\n",
      "actions: [[-0.11432666 -0.27266723  0.00200739 -0.01884217  0.06615953 -0.19262487\n",
      "   0.19115306 -0.24133037  0.24857953  0.12238304  0.00425713  0.01758589\n",
      "  -0.03581119  0.00588792  0.05198454  0.0953422   0.13455921  0.23851496\n",
      "   0.20925009  0.23677993]]\n",
      "values: tensor([[-117190.2266]])\n",
      "clipped_actions: [[0.         0.         0.00200739 0.         0.06615953 0.\n",
      "  0.19115306 0.         0.24857953 0.12238304 0.00425713 0.01758589\n",
      "  0.         0.00588792 0.05198454 0.0953422  0.13455921 0.23851496\n",
      "  0.20925009 0.23677993]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.8443983]\n",
      "\n",
      "\n",
      "Log-Std at step 4104: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.02287377655506134 ; last reward: -0.8443983197212219\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.3107845  -0.10392127 -0.14535725  0.23849112 -0.18246077 -0.07718742\n",
      "   0.03666051  0.10095862 -0.06239756 -0.04088799  0.03188452 -0.04009136\n",
      "   0.14859077 -0.10248806  0.16626073 -0.2826069   0.00156506  0.04363304\n",
      "  -0.03189896 -0.10438752]]\n",
      "values: tensor([[-1258983.5000]])\n",
      "clipped_actions: [[0.3107845  0.         0.         0.23849112 0.         0.\n",
      "  0.03666051 0.10095862 0.         0.         0.03188452 0.\n",
      "  0.14859077 0.         0.16626073 0.         0.00156506 0.04363304\n",
      "  0.         0.        ]]\n",
      "new_obs: [[0.000000e+00 0.000000e+00 8.855247e-04 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 6.000000e-02 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 1.000000e+01 1.000000e+01 0.000000e+00\n",
      "  0.000000e+00 1.000000e+01 1.000000e+01 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 1.000000e+01 1.000000e+01 0.000000e+00\n",
      "  0.000000e+00 1.000000e+01 1.000000e+01 0.000000e+00 0.000000e+00\n",
      "  1.000000e+01 1.000000e+01 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 1.000000e+00]]\n",
      "rewards: [-0.25238338]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 8.8552e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00]])\n",
      "actions: [[ 0.09156401  0.12624449  0.00171534 -0.00147129 -0.20490663 -0.2556095\n",
      "   0.13669038  0.00960721  0.10657815 -0.3286238   0.29553732  0.00775725\n",
      "  -0.21627586  0.14067426  0.047685   -0.12669463  0.05286548 -0.14406748\n",
      "  -0.41374525 -0.18990754]]\n",
      "values: tensor([[-1108685.2500]])\n",
      "clipped_actions: [[0.09156401 0.12624449 0.00171534 0.         0.         0.\n",
      "  0.13669038 0.00960721 0.10657815 0.         0.29553732 0.00775725\n",
      "  0.         0.14067426 0.047685   0.         0.05286548 0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[6.9388939e-18 0.0000000e+00 8.8552473e-04 0.0000000e+00 5.4210109e-20\n",
      "  0.0000000e+00 0.0000000e+00 4.1308557e-04 5.9999999e-02 0.0000000e+00\n",
      "  4.5720504e+14 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 2.0000000e+00]]\n",
      "rewards: [-0.504806]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[6.9389e-18, 0.0000e+00, 8.8552e-04, 0.0000e+00, 5.4210e-20, 0.0000e+00,\n",
      "         0.0000e+00, 4.1309e-04, 6.0000e-02, 0.0000e+00, 4.5721e+14, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0000e+00]])\n",
      "actions: [[-3.7904302e+11 -1.7910974e+11 -4.0274792e+11  1.5817161e+11\n",
      "  -5.7285024e+11  1.9802395e+11  1.4912699e+11 -5.4878470e+11\n",
      "   2.1881885e+11 -1.9512495e+11  1.8524090e+10 -9.3351943e+11\n",
      "   5.6701655e+11  5.7285778e+11 -1.7366044e+11 -5.3167181e+11\n",
      "   2.5068495e+11 -5.5711515e+10  2.4330948e+11 -2.3458375e+11]]\n",
      "values: tensor([[-2.5515e+18]])\n",
      "clipped_actions: [[ 0.  0.  0. 50.  0. 50. 50.  0. 50.  0. 50.  0. 50. 50.  0.  0. 50.  0.\n",
      "  50.  0.]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 8.8552473e-04 4.1308557e-04 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 5.9999999e-02 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [-0.25235718]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         8.8552e-04, 4.1309e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.0000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e+00]])\n",
      "actions: [[-0.02647269  0.13186528  0.03716703  0.13384539 -0.01266574 -0.13128035\n",
      "   0.07664578 -0.16904522 -0.14790004 -0.03631947 -0.07623003 -0.1785077\n",
      "  -0.01910013  0.27024877  0.12333371  0.01241303  0.06818471  0.19915885\n",
      "   0.13699101  0.16194497]]\n",
      "values: tensor([[-479714.2812]])\n",
      "clipped_actions: [[0.         0.13186528 0.03716703 0.13384539 0.         0.\n",
      "  0.07664578 0.         0.         0.         0.         0.\n",
      "  0.         0.27024877 0.12333371 0.01241303 0.06818471 0.19915885\n",
      "  0.13699101 0.16194497]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          9.864253\n",
      "   0.          0.          0.          0.          0.          0.05917431\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   4.        ]]\n",
      "rewards: [-0.42536908]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  9.8643,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0592,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  4.0000]])\n",
      "actions: [[-0.03219832  0.00413774  0.01666875  0.14554796 -0.0737005   0.15931524\n",
      "  -0.07355139 -0.12228468 -0.03961422  0.05116432  0.08238889 -0.1596864\n",
      "  -0.11611459  0.00811687 -0.2784982   0.00843104 -0.11319767  0.09262306\n",
      "  -0.08852191 -0.03757501]]\n",
      "values: tensor([[-279804.0938]])\n",
      "clipped_actions: [[0.         0.00413774 0.01666875 0.14554796 0.         0.15931524\n",
      "  0.         0.         0.         0.05116432 0.08238889 0.\n",
      "  0.         0.00811687 0.         0.00843104 0.         0.09262306\n",
      "  0.         0.        ]]\n",
      "new_obs: [[0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  9.855823e+00 0.000000e+00 8.431044e-03 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 5.912369e-02 0.000000e+00 0.000000e+00 1.000000e+01\n",
      "  1.000000e+01 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 5.000000e+00]]\n",
      "rewards: [-0.42495662]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8558e+00,\n",
      "         0.0000e+00, 8.4310e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9124e-02,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.0000e+00]])\n",
      "actions: [[-0.16392095  0.13626261 -0.09196705 -0.02791991  0.06187785 -0.01627446\n",
      "   0.21352822  0.2820017  -0.15552008 -0.10931142 -0.41632053  0.04412305\n",
      "  -0.17412622 -0.07620668 -0.0222126   0.22255947 -0.03641035  0.22787341\n",
      "  -0.02559895 -0.1754733 ]]\n",
      "values: tensor([[-112841.1406]])\n",
      "clipped_actions: [[0.         0.13626261 0.         0.         0.06187785 0.\n",
      "  0.21352822 0.2820017  0.         0.         0.         0.04412305\n",
      "  0.         0.         0.         0.22255947 0.         0.22787341\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.4137055]\n",
      "\n",
      "\n",
      "Log-Std at step 6150: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.027010831534862518 ; last reward: -0.4137054979801178\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.18608399 -0.17780437  0.0369678   0.09231651  0.1360224  -0.03294476\n",
      "  -0.11787593 -0.03105669 -0.17837676  0.10135101 -0.03747032  0.06165372\n",
      "  -0.06778693  0.13911223 -0.0631853   0.22368829  0.10219152  0.17548075\n",
      "  -0.001523    0.0521124 ]]\n",
      "values: tensor([[-1258983.5000]])\n",
      "clipped_actions: [[0.18608399 0.         0.0369678  0.09231651 0.1360224  0.\n",
      "  0.         0.         0.         0.10135101 0.         0.06165372\n",
      "  0.         0.13911223 0.         0.22368829 0.10219152 0.17548075\n",
      "  0.         0.0521124 ]]\n",
      "new_obs: [[ 0.          0.03945835  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.37751913]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0395,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[ 3.92135739e-01  8.87533128e-02 -2.41737932e-01 -5.00440411e-02\n",
      "   7.33922124e-02  1.98517382e-01  3.32733840e-02 -1.03870854e-01\n",
      "  -2.90609092e-01  7.91709721e-02  1.27116195e-03  2.21654519e-01\n",
      "  -2.72868276e-01  3.79489735e-04  2.55530983e-01  5.58568351e-02\n",
      "   1.42272502e-01 -2.99313031e-02  1.08879760e-01 -7.05699474e-02]]\n",
      "values: tensor([[-1108555.3750]])\n",
      "clipped_actions: [[3.9213574e-01 8.8753313e-02 0.0000000e+00 0.0000000e+00 7.3392212e-02\n",
      "  1.9851738e-01 3.3273384e-02 0.0000000e+00 0.0000000e+00 7.9170972e-02\n",
      "  1.2711619e-03 2.2165452e-01 0.0000000e+00 3.7948973e-04 2.5553098e-01\n",
      "  5.5856835e-02 1.4227250e-01 0.0000000e+00 1.0887976e-01 0.0000000e+00]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.9225616e-03\n",
      "  0.0000000e+00 0.0000000e+00 3.7948973e-04 0.0000000e+00 0.0000000e+00\n",
      "  2.8515381e-01 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 2.0000000e+00]]\n",
      "rewards: [-0.6294504]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.9226e-03, 0.0000e+00,\n",
      "         0.0000e+00, 3.7949e-04, 0.0000e+00, 0.0000e+00, 2.8515e-01, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0000e+00]])\n",
      "actions: [[ 0.01624469  0.13029571  0.1463987  -0.09998537  0.01683509  0.00669308\n",
      "   0.12297107 -0.02394456  0.05812766 -0.02943694  0.04550522  0.03399648\n",
      "   0.025822    0.10596658 -0.12687     0.01624972 -0.20383787  0.13971813\n",
      "  -0.00142417  0.20834786]]\n",
      "values: tensor([[-903696.1250]])\n",
      "clipped_actions: [[0.01624469 0.13029571 0.1463987  0.         0.01683509 0.00669308\n",
      "  0.12297107 0.         0.05812766 0.         0.04550522 0.03399648\n",
      "  0.025822   0.10596658 0.         0.01624972 0.         0.13971813\n",
      "  0.         0.20834786]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.9225616e-03\n",
      "  0.0000000e+00 3.6536588e-03 3.1091005e-03 0.0000000e+00 0.0000000e+00\n",
      "  2.8515381e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [-0.8811714]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.9226e-03, 0.0000e+00,\n",
      "         3.6537e-03, 3.1091e-03, 0.0000e+00, 0.0000e+00, 2.8515e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e+00]])\n",
      "actions: [[ 0.21365762  0.07163622  0.10072763  0.2293494  -0.13871652 -0.05596608\n",
      "   0.11894304 -0.02967448  0.1512821  -0.04928257 -0.09515246 -0.2934845\n",
      "   0.15190169 -0.01770336 -0.11020466 -0.05821862 -0.17960046 -0.26449832\n",
      "  -0.15254393 -0.04496901]]\n",
      "values: tensor([[-487834.0312]])\n",
      "clipped_actions: [[0.21365762 0.07163622 0.10072763 0.2293494  0.         0.\n",
      "  0.11894304 0.         0.1512821  0.         0.         0.\n",
      "  0.15190169 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 7.5762202e-03 3.1091005e-03 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 4.0000000e+00]]\n",
      "rewards: [-0.8808733]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.5762e-03, 3.1091e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.0000e+00]])\n",
      "actions: [[ 0.0479576  -0.03284873  0.15308899 -0.12417866 -0.1639002   0.08889198\n",
      "  -0.12750885 -0.22271167 -0.18331575  0.2921594   0.15643823  0.2313264\n",
      "   0.08174951  0.18015307 -0.09782668 -0.21695365  0.04729241 -0.01611368\n",
      "   0.00047583  0.02407697]]\n",
      "values: tensor([[-287107.5000]])\n",
      "clipped_actions: [[0.0479576  0.         0.15308899 0.         0.         0.08889198\n",
      "  0.         0.         0.         0.2921594  0.15643823 0.2313264\n",
      "  0.08174951 0.18015307 0.         0.         0.04729241 0.\n",
      "  0.00047583 0.02407697]]\n",
      "new_obs: [[0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 7.100386e-03 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+01\n",
      "  1.000000e+01 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 5.000000e+00]]\n",
      "rewards: [-0.88509315]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         7.1004e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.0000e+00]])\n",
      "actions: [[ 0.07856338  0.11737433  0.19020246  0.20221677  0.04819803 -0.09932715\n",
      "  -0.25814918 -0.19480732  0.13163109 -0.15174776 -0.10190526 -0.0445147\n",
      "  -0.0277751  -0.08345082 -0.11895918  0.03253881  0.30765858  0.10761862\n",
      "  -0.1953211   0.15470463]]\n",
      "values: tensor([[-117198.9141]])\n",
      "clipped_actions: [[0.07856338 0.11737433 0.19020246 0.20221677 0.04819803 0.\n",
      "  0.         0.         0.13163109 0.         0.         0.\n",
      "  0.         0.         0.         0.03253881 0.30765858 0.10761862\n",
      "  0.         0.15470463]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.8845241]\n",
      "\n",
      "\n",
      "Log-Std at step 8202: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.03585607260465622 ; last reward: -0.8845241069793701\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.09584706 -0.05286031 -0.01430486 -0.0235768   0.20265299 -0.07989967\n",
      "  -0.09188797  0.16552155 -0.01756201  0.04226057 -0.02806235 -0.12035993\n",
      "  -0.05804556 -0.15056689  0.14318152 -0.11643763  0.1402004  -0.05832227\n",
      "  -0.12668952 -0.20763408]]\n",
      "values: tensor([[-1258983.5000]])\n",
      "clipped_actions: [[0.09584706 0.         0.         0.         0.20265299 0.\n",
      "  0.         0.16552155 0.         0.04226057 0.         0.\n",
      "  0.         0.         0.14318152 0.         0.1402004  0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.04435335  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.12474045]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0444,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[-0.07622519  0.09246828 -0.09953112 -0.04848842 -0.12003197 -0.10593552\n",
      "  -0.09357128 -0.09919748  0.20013973  0.01639463  0.17153268 -0.10416603\n",
      "  -0.12474549  0.03334371  0.15282997 -0.08438344  0.33079925  0.04706458\n",
      "   0.08172509 -0.09181398]]\n",
      "values: tensor([[-1108481.]])\n",
      "clipped_actions: [[0.         0.09246828 0.         0.         0.         0.\n",
      "  0.         0.         0.20013973 0.01639463 0.17153268 0.\n",
      "  0.         0.03334371 0.15282997 0.         0.33079925 0.04706458\n",
      "  0.08172509 0.        ]]\n",
      "new_obs: [[ 0.28268433  0.04706458  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   2.        ]]\n",
      "rewards: [-0.24646747]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.2827,  0.0471,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  2.0000]])\n",
      "actions: [[-0.07911198  0.11276356 -0.17674483 -0.0810423   0.03747829  0.10185257\n",
      "   0.08966751  0.20737237  0.23472476 -0.06634275 -0.05210692  0.02937175\n",
      "   0.12452257  0.0211544   0.06017966  0.12915076  0.00335373 -0.10577857\n",
      "   0.02461725  0.0718887 ]]\n",
      "values: tensor([[-902537.3750]])\n",
      "clipped_actions: [[0.         0.11276356 0.         0.         0.03747829 0.10185257\n",
      "  0.08966751 0.20737237 0.23472476 0.         0.         0.02937175\n",
      "  0.12452257 0.0211544  0.06017966 0.12915076 0.00335373 0.\n",
      "  0.02461725 0.0718887 ]]\n",
      "new_obs: [[1.7327449e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 1.2308880e-02 1.4043765e-03 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [-0.49640283]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[1.7327e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.2309e-02, 1.4044e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e+00]])\n",
      "actions: [[-0.1977714  -0.10200201  0.13869444 -0.1141707   0.09795821 -0.17938347\n",
      "  -0.10886141 -0.11418271  0.06872389 -0.14173636  0.07958119 -0.33989868\n",
      "  -0.15221074  0.06585292 -0.05787344 -0.20447466 -0.05797992 -0.08304244\n",
      "   0.10652269  0.01600224]]\n",
      "values: tensor([[-486534.9688]])\n",
      "clipped_actions: [[0.         0.         0.13869444 0.         0.09795821 0.\n",
      "  0.         0.         0.06872389 0.         0.07958119 0.\n",
      "  0.         0.06585292 0.         0.         0.         0.\n",
      "  0.10652269 0.01600224]]\n",
      "new_obs: [[ 0.03458006  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   4.        ]]\n",
      "rewards: [-0.6389165]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0346,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  4.0000]])\n",
      "actions: [[-0.00540646  0.1582112  -0.16332436 -0.05213235  0.11210514 -0.01389904\n",
      "   0.15897767  0.11025988 -0.03299891  0.0706397  -0.08938327 -0.13496506\n",
      "   0.20073754  0.12026574  0.02223526 -0.04261634  0.02779312  0.02062032\n",
      "  -0.23948646  0.2359206 ]]\n",
      "values: tensor([[-287101.6562]])\n",
      "clipped_actions: [[0.         0.1582112  0.         0.         0.11210514 0.\n",
      "  0.15897767 0.11025988 0.         0.0706397  0.         0.\n",
      "  0.20073754 0.12026574 0.02223526 0.         0.02779312 0.02062032\n",
      "  0.         0.2359206 ]]\n",
      "new_obs: [[ 0.          0.          0.          0.03458006  0.          0.\n",
      "   0.          0.          0.          0.06        0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   5.        ]]\n",
      "rewards: [-0.6380446]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0346,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0600,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  5.0000]])\n",
      "actions: [[-0.06360898 -0.05555978  0.10011031 -0.05784301  0.05845838 -0.351453\n",
      "   0.02531702 -0.07030876 -0.0439198  -0.14504011  0.07247432  0.10963121\n",
      "  -0.03100199 -0.08012693 -0.15056793 -0.02154767 -0.18857476 -0.20280683\n",
      "   0.01691743 -0.10422246]]\n",
      "values: tensor([[-117237.3359]])\n",
      "clipped_actions: [[0.         0.         0.10011031 0.         0.05845838 0.\n",
      "  0.02531702 0.         0.         0.         0.07247432 0.10963121\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01691743 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.65418464]\n",
      "\n",
      "\n",
      "Log-Std at step 10248: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.0423979189991951 ; last reward: -0.6541846394538879\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[-0.04939922 -0.18076658 -0.06306308  0.08100271 -0.03033944 -0.0672683\n",
      "   0.02953146  0.05098294 -0.00681221 -0.07934796 -0.0182986  -0.02098561\n",
      "  -0.16550015 -0.10556056 -0.16920382  0.0197658  -0.04811598  0.05376021\n",
      "  -0.03863209 -0.16333237]]\n",
      "values: tensor([[-193313.4844]])\n",
      "clipped_actions: [[0.         0.         0.         0.08100271 0.         0.\n",
      "  0.02953146 0.05098294 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.0197658  0.         0.05376021\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.01971843  0.\n",
      "   0.          0.          0.          0.          0.26        0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.12458506]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0197,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.2600,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[ 0.06712198  0.10543413 -0.0477726   0.07293833  0.10381793  0.3136132\n",
      "   0.26979443  0.0846335  -0.05774926 -0.11347026 -0.08995567 -0.13638377\n",
      "  -0.24219418  0.17118752  0.0133935  -0.02128752 -0.06862277 -0.1377655\n",
      "  -0.1281107   0.14588186]]\n",
      "values: tensor([[-161678.4531]])\n",
      "clipped_actions: [[0.06712198 0.10543413 0.         0.07293833 0.10381793 0.3136132\n",
      "  0.26979443 0.0846335  0.         0.         0.         0.\n",
      "  0.         0.17118752 0.0133935  0.         0.         0.\n",
      "  0.         0.14588186]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.01971843  0.          0.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   2.        ]]\n",
      "rewards: [-0.12360043]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0197,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  2.0000]])\n",
      "actions: [[-2.6808776e-02  2.9358519e-03 -7.8380004e-02 -1.0733115e-01\n",
      "  -1.4841482e-01  3.0281711e-01  1.7106533e-05  6.0351141e-02\n",
      "   9.7885028e-02  1.4257972e-01  2.3130527e-01 -9.1693446e-02\n",
      "   2.3511145e-01  9.0923481e-02  6.4553544e-02 -1.2625307e-01\n",
      "  -1.1787650e-03 -2.9425729e-02 -8.7792397e-02 -8.5003614e-02]]\n",
      "values: tensor([[-131189.1094]])\n",
      "clipped_actions: [[0.0000000e+00 2.9358519e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  3.0281711e-01 1.7106533e-05 6.0351141e-02 9.7885028e-02 1.4257972e-01\n",
      "  2.3130527e-01 0.0000000e+00 2.3511145e-01 9.0923481e-02 6.4553544e-02\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.01971843  0.          0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.        ]]\n",
      "rewards: [-0.12360322]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0197,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  3.0000]])\n",
      "actions: [[-0.08396065  0.0693883   0.22277628 -0.09824913  0.0870194  -0.2046484\n",
      "   0.4008594  -0.00286258 -0.06868493 -0.05193057  0.00385278 -0.21749464\n",
      "   0.10357498  0.2434736   0.205123   -0.06001742 -0.18513797  0.0109782\n",
      "   0.03230593  0.07234026]]\n",
      "values: tensor([[-62724.9766]])\n",
      "clipped_actions: [[0.         0.0693883  0.22277628 0.         0.0870194  0.\n",
      "  0.4008594  0.         0.         0.         0.00385278 0.\n",
      "  0.10357498 0.2434736  0.205123   0.         0.         0.0109782\n",
      "  0.03230593 0.07234026]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.\n",
      "  10. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   4.]]\n",
      "rewards: [-0.14827204]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.]])\n",
      "actions: [[-0.09115532  0.05652921  0.08247967 -0.0911532  -0.1830831  -0.34201762\n",
      "   0.00246861 -0.02291753  0.03386812 -0.10229916 -0.17465107 -0.15254745\n",
      "   0.14070559  0.12911083 -0.1922741  -0.00096312  0.12637703 -0.16003437\n",
      "   0.12582189 -0.08988679]]\n",
      "values: tensor([[-36036.8828]])\n",
      "clipped_actions: [[0.         0.05652921 0.08247967 0.         0.         0.\n",
      "  0.00246861 0.         0.03386812 0.         0.         0.\n",
      "  0.14070559 0.12911083 0.         0.         0.12637703 0.\n",
      "  0.12582189 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   5.]]\n",
      "rewards: [-0.14827721]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.]])\n",
      "actions: [[ 0.21371265 -0.03313944 -0.02130081 -0.04959382  0.2203652  -0.02813703\n",
      "  -0.00102578  0.14523822  0.14977673  0.090244   -0.07863846 -0.06226116\n",
      "   0.03541058 -0.03855944 -0.04120835  0.04399162 -0.03679878  0.07973377\n",
      "   0.11873849 -0.21531399]]\n",
      "values: tensor([[-13850.7852]])\n",
      "clipped_actions: [[0.21371265 0.         0.         0.         0.2203652  0.\n",
      "  0.         0.14523822 0.14977673 0.090244   0.         0.\n",
      "  0.03541058 0.         0.         0.04399162 0.         0.07973377\n",
      "  0.11873849 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.1482829]\n",
      "\n",
      "\n",
      "Log-Std at step 12294: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.043880748003721236 ; last reward: -0.14828290045261383\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[-0.00322507 -0.03965423 -0.07453045 -0.01063787  0.26867676 -0.3175186\n",
      "   0.0472538   0.16870806  0.10824682  0.11787882 -0.01873253 -0.17210963\n",
      "   0.02849994 -0.02451858 -0.09617877 -0.19615765 -0.06423251 -0.01033217\n",
      "  -0.19813609 -0.07008934]]\n",
      "values: tensor([[-184358.7969]])\n",
      "clipped_actions: [[0.         0.         0.         0.         0.26867676 0.\n",
      "  0.0472538  0.16870806 0.10824682 0.11787882 0.         0.\n",
      "  0.02849994 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0.  0.  0.\n",
      "   1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0.,  0.,  0.,  1.]])\n",
      "actions: [[ 0.21089461 -0.09564423  0.17075199 -0.15912582 -0.03191816 -0.09621771\n",
      "   0.21149902  0.20118614 -0.05646769 -0.01492707 -0.06498966  0.22433472\n",
      "   0.06646197 -0.03014984  0.02473667  0.02523964  0.10413717 -0.04742378\n",
      "   0.0030378  -0.3514271 ]]\n",
      "values: tensor([[-153696.8438]])\n",
      "clipped_actions: [[0.21089461 0.         0.17075199 0.         0.         0.\n",
      "  0.21149902 0.20118614 0.         0.         0.         0.22433472\n",
      "  0.06646197 0.         0.02473667 0.02523964 0.10413717 0.\n",
      "  0.0030378  0.        ]]\n",
      "new_obs: [[ 0.         0.         0.0575453  0.         0.         0.\n",
      "   0.         0.         0.06       0.         0.         0.\n",
      "  10.        10.         0.         0.         0.         0.\n",
      "  10.        10.         0.         0.        10.        10.\n",
      "   0.         0.        10.        10.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   2.       ]]\n",
      "rewards: [-0.12342159]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0575,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0600,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  2.0000]])\n",
      "actions: [[ 0.15922086  0.01295084  0.02144623 -0.08357688 -0.23565239 -0.06470376\n",
      "   0.00289791  0.02923539  0.03216596  0.01388194 -0.06785458  0.06441283\n",
      "  -0.03625364 -0.0572246  -0.29274464 -0.2554795  -0.05285192 -0.03355108\n",
      "   0.00408138 -0.06824137]]\n",
      "values: tensor([[-124976.1484]])\n",
      "clipped_actions: [[0.15922086 0.01295084 0.02144623 0.         0.         0.\n",
      "  0.00289791 0.02923539 0.03216596 0.01388194 0.         0.06441283\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00408138 0.        ]]\n",
      "new_obs: [[ 0.          0.          0.01149741  0.          0.          0.\n",
      "   0.03216596  0.01388194 -0.18030408  0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.        ]]\n",
      "rewards: [-0.12112341]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0115,  0.0000,  0.0000,  0.0000,  0.0322,  0.0139,\n",
      "         -0.1803,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  3.0000]])\n",
      "actions: [[ 0.06956439  0.16534184  0.03122521  0.16714634 -0.14036348 -0.1092396\n",
      "   0.13642141 -0.094275   -0.1378974   0.02136086 -0.24043585 -0.15441242\n",
      "  -0.0124761  -0.13021989 -0.13001266 -0.17333974 -0.04053609  0.03176791\n",
      "  -0.03231394 -0.05781786]]\n",
      "values: tensor([[-59364.1836]])\n",
      "clipped_actions: [[0.06956439 0.16534184 0.03122521 0.16714634 0.         0.\n",
      "  0.13642141 0.         0.         0.02136086 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.03176791\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.03216596  0.02537934  0.          0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   4.        ]]\n",
      "rewards: [-0.1205515]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0322,  0.0254,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  4.0000]])\n",
      "actions: [[-0.02234995  0.09220631 -0.20609371 -0.0128123  -0.01631712  0.22669941\n",
      "   0.31423336 -0.14099452  0.4669747   0.04118024  0.00300642 -0.16484413\n",
      "   0.11894423 -0.1597861   0.08648007 -0.03781145  0.05987702  0.02919423\n",
      "  -0.13034561 -0.08123964]]\n",
      "values: tensor([[-34175.3828]])\n",
      "clipped_actions: [[0.         0.09220631 0.         0.         0.         0.22669941\n",
      "  0.31423336 0.         0.4669747  0.04118024 0.00300642 0.\n",
      "  0.11894423 0.         0.08648007 0.         0.05987702 0.02919423\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.03216596  0.02537934  0.          0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   5.        ]]\n",
      "rewards: [-0.12055444]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0322,  0.0254,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  5.0000]])\n",
      "actions: [[-0.07011015 -0.11724747 -0.13650323 -0.15010531  0.28327203  0.07405903\n",
      "   0.07566333 -0.2224718  -0.18039356  0.10138087  0.05493977 -0.3318335\n",
      "  -0.13433999 -0.05453433 -0.10025546 -0.07742488 -0.01033309  0.13644317\n",
      "   0.17797786  0.06241962]]\n",
      "values: tensor([[-13128.5254]])\n",
      "clipped_actions: [[0.         0.         0.         0.         0.28327203 0.07405903\n",
      "  0.07566333 0.         0.         0.10138087 0.05493977 0.\n",
      "  0.         0.         0.         0.         0.         0.13644317\n",
      "  0.17797786 0.06241962]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.19241461]\n",
      "\n",
      "\n",
      "Log-Std at step 14346: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.045804894119501116 ; last reward: -0.19241461157798767\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.12218375  0.23083018 -0.12361228  0.16297743  0.09202032  0.07573794\n",
      "  -0.31076863 -0.20933577  0.06279694 -0.08073089 -0.12095095 -0.1091485\n",
      "  -0.05999765  0.12410547 -0.01482697 -0.3807488   0.36828008 -0.05350824\n",
      "   0.12435935 -0.14443177]]\n",
      "values: tensor([[-184358.7969]])\n",
      "clipped_actions: [[0.12218375 0.23083018 0.         0.16297743 0.09202032 0.07573794\n",
      "  0.         0.         0.06279694 0.         0.         0.\n",
      "  0.         0.12410547 0.         0.         0.36828008 0.\n",
      "  0.12435935 0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.16475113  0.          0.11632238\n",
      "   0.          0.          0.          0.06        0.          0.06\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.11736485]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.1648,  0.0000,  0.1163,  0.0000,  0.0000,\n",
      "          0.0000,  0.0600,  0.0000,  0.0600, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[ 0.15794876 -0.13009922  0.30919054 -0.14900725  0.06163938  0.04172098\n",
      "  -0.04109591  0.14180066 -0.1419943   0.1561181  -0.13359503 -0.0857303\n",
      "   0.08003151  0.12745678 -0.00244286  0.09183401  0.15252122 -0.24723391\n",
      "   0.24607535 -0.10322158]]\n",
      "values: tensor([[-153700.]])\n",
      "clipped_actions: [[0.15794876 0.         0.30919054 0.         0.06163938 0.04172098\n",
      "  0.         0.14180066 0.         0.1561181  0.         0.\n",
      "  0.08003151 0.12745678 0.         0.09183401 0.15252122 0.\n",
      "  0.24607535 0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.16475113  0.          0.02448837\n",
      "   0.          0.09183401  0.          0.06        0.         -0.16500644\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   2.        ]]\n",
      "rewards: [-0.3615154]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.1648,  0.0000,  0.0245,  0.0000,  0.0918,\n",
      "          0.0000,  0.0600,  0.0000, -0.1650, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  2.0000]])\n",
      "actions: [[ 0.19043157  0.10362543 -0.02114664  0.0900781  -0.09560247  0.17333898\n",
      "  -0.1032025  -0.02086931  0.14298394  0.13841783  0.02611805 -0.28039268\n",
      "  -0.031619    0.1695771   0.0927007   0.12788826  0.10067249  0.22725646\n",
      "   0.22171304 -0.10814989]]\n",
      "values: tensor([[-124936.5938]])\n",
      "clipped_actions: [[0.19043157 0.10362543 0.         0.0900781  0.         0.17333898\n",
      "  0.         0.         0.14298394 0.13841783 0.02611805 0.\n",
      "  0.         0.1695771  0.0927007  0.12788826 0.10067249 0.22725646\n",
      "  0.22171304 0.        ]]\n",
      "new_obs: [[ 0.          0.05391748  0.          0.16475113  0.          0.02448837\n",
      "   0.          0.09183401  0.          0.06        0.         -0.16500644\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.        ]]\n",
      "rewards: [-0.7339409]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0539,  0.0000,  0.1648,  0.0000,  0.0245,  0.0000,  0.0918,\n",
      "          0.0000,  0.0600,  0.0000, -0.1650,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  3.0000]])\n",
      "actions: [[-0.1450581  -0.05748527 -0.04425418  0.01624478  0.12561432 -0.20352006\n",
      "   0.00722279 -0.17290938  0.05791799 -0.14076047 -0.23123184 -0.16259064\n",
      "  -0.03516184  0.02109788 -0.03957583  0.07308193  0.03671934  0.20688\n",
      "   0.2660922  -0.0209182 ]]\n",
      "values: tensor([[-59527.8086]])\n",
      "clipped_actions: [[0.         0.         0.         0.01624478 0.12561432 0.\n",
      "  0.00722279 0.         0.05791799 0.         0.         0.\n",
      "  0.         0.02109788 0.         0.07308193 0.03671934 0.20688\n",
      "  0.2660922  0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6475113e-01 4.3368087e-19\n",
      "  0.0000000e+00 0.0000000e+00 1.1925405e-01 0.0000000e+00 5.9999999e-02\n",
      "  1.7575940e+15 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 4.0000000e+00]]\n",
      "rewards: [-0.8569376]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6475e-01, 4.3368e-19, 0.0000e+00,\n",
      "         0.0000e+00, 1.1925e-01, 0.0000e+00, 6.0000e-02, 1.7576e+15, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.0000e+00]])\n",
      "actions: [[-1.4571413e+12 -6.8854179e+11 -1.5482257e+12  6.0800356e+11\n",
      "  -2.2021386e+12  7.6124357e+11  5.7325847e+11 -2.1096192e+12\n",
      "   8.4117645e+11 -7.5010251e+11  7.1236739e+10 -3.5886672e+12\n",
      "   2.1797158e+12  2.2022054e+12 -6.6755291e+11 -2.0438728e+12\n",
      "   9.6367444e+11 -2.1420678e+11  9.3533661e+11 -9.0177051e+11]]\n",
      "values: tensor([[-2.0863e+18]])\n",
      "clipped_actions: [[ 0.  0.  0. 50.  0. 50. 50.  0. 50.  0. 50.  0. 50. 50.  0.  0. 50.  0.\n",
      "  50.  0.]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.11925405  0.          0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   5.        ]]\n",
      "rewards: [-1.0617195]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1193,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  5.0000]])\n",
      "actions: [[-1.20621912e-01 -1.09079376e-01 -7.51446411e-02  3.44630778e-01\n",
      "   1.68789551e-02  2.95581557e-02  1.36306584e-01 -6.63719028e-02\n",
      "   2.06533121e-04 -1.80202290e-01 -2.67733216e-01  5.42382803e-03\n",
      "  -3.83935124e-03  1.97057530e-01  5.08886762e-02  1.31872371e-01\n",
      "  -1.14308469e-01 -5.46713881e-02 -9.12488326e-02  1.14167809e-01]]\n",
      "values: tensor([[-13133.7705]])\n",
      "clipped_actions: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4463078e-01 1.6878955e-02\n",
      "  2.9558156e-02 1.3630658e-01 0.0000000e+00 2.0653312e-04 0.0000000e+00\n",
      "  0.0000000e+00 5.4238280e-03 0.0000000e+00 1.9705753e-01 5.0888676e-02\n",
      "  1.3187237e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1416781e-01]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-1.2033393]\n",
      "\n",
      "\n",
      "Log-Std at step 16392: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.05783828750252724 ; last reward: -1.2033393383026123\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[-0.25264305  0.17202574 -0.02314711  0.02836836 -0.05863104 -0.03356882\n",
      "  -0.1998153   0.11760096  0.00274059 -0.23619473  0.07473935  0.03700639\n",
      "   0.03776281  0.25110745 -0.0578563   0.05994103 -0.13843349 -0.01508861\n",
      "  -0.00121885 -0.14513391]]\n",
      "values: tensor([[-359025.5938]])\n",
      "clipped_actions: [[0.         0.17202574 0.         0.02836836 0.         0.\n",
      "  0.         0.11760096 0.00274059 0.         0.07473935 0.03700639\n",
      "  0.03776281 0.25110745 0.         0.05994103 0.         0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0.  0.  0.\n",
      "   1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0.,  0.,  0.,  1.]])\n",
      "actions: [[ 0.09502523  0.01098273 -0.02836    -0.01741652 -0.06260464  0.31973654\n",
      "   0.06528513 -0.0797985  -0.12998925 -0.00662344  0.02942324  0.14805615\n",
      "   0.32306886 -0.25819737  0.2020433  -0.15850568  0.05265106 -0.24622494\n",
      "   0.0302668   0.09605998]]\n",
      "values: tensor([[-425163.3750]])\n",
      "clipped_actions: [[0.09502523 0.01098273 0.         0.         0.         0.31973654\n",
      "  0.06528513 0.         0.         0.         0.02942324 0.14805615\n",
      "  0.32306886 0.         0.2020433  0.         0.05265106 0.\n",
      "  0.0302668  0.09605998]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 4.7196262e-02 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 9.0431864e-04 4.5504821e-03 5.9999999e-02 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 2.0000000e+00]]\n",
      "rewards: [0.00144716]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 4.7196e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         9.0432e-04, 4.5505e-03, 6.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0000e+00]])\n",
      "actions: [[-0.24654646  0.31288603 -0.06583112  0.17563942 -0.01141898  0.20424402\n",
      "  -0.0014071  -0.04672546  0.15964256  0.02238847  0.17919877 -0.10071582\n",
      "   0.17543754 -0.06762383  0.08630025 -0.3219545   0.14537536 -0.10238966\n",
      "  -0.09886332 -0.18212007]]\n",
      "values: tensor([[-558484.3125]])\n",
      "clipped_actions: [[0.         0.31288603 0.         0.17563942 0.         0.20424402\n",
      "  0.         0.         0.15964256 0.02238847 0.17919877 0.\n",
      "  0.17543754 0.         0.08630025 0.         0.14537536 0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.04229579  0.01035527  0.          0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.        ]]\n",
      "rewards: [-0.24526103]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0423,  0.0104,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  3.0000]])\n",
      "actions: [[-0.00246849  0.10766397 -0.26551178  0.11721138 -0.22860603 -0.14956123\n",
      "   0.08416678  0.29089198 -0.19400343  0.13572429  0.13506821  0.09395491\n",
      "   0.00282776  0.037974   -0.21003617  0.0408318  -0.00541741  0.18323019\n",
      "  -0.00608286 -0.04169862]]\n",
      "values: tensor([[-298076.5000]])\n",
      "clipped_actions: [[0.         0.10766397 0.         0.11721138 0.         0.\n",
      "  0.08416678 0.29089198 0.         0.13572429 0.13506821 0.09395491\n",
      "  0.00282776 0.037974   0.         0.0408318  0.         0.18323019\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.04229579  0.01035527  0.          0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   4.        ]]\n",
      "rewards: [-0.24526572]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0423,  0.0104,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  4.0000]])\n",
      "actions: [[ 0.00657564 -0.04921548  0.1432444  -0.03597181 -0.13347238  0.10926078\n",
      "   0.11669564 -0.12767512  0.0278417   0.32031056 -0.06415467  0.115843\n",
      "   0.08909451 -0.11872019  0.06386849  0.15122129  0.1021474  -0.16569945\n",
      "   0.10479584 -0.22560148]]\n",
      "values: tensor([[-174095.0312]])\n",
      "clipped_actions: [[0.00657564 0.         0.1432444  0.         0.         0.10926078\n",
      "  0.11669564 0.         0.0278417  0.32031056 0.         0.115843\n",
      "  0.08909451 0.         0.06386849 0.15122129 0.1021474  0.\n",
      "  0.10479584 0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.01035527  0.          0.          0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   5.        ]]\n",
      "rewards: [-0.29794508]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0104,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  5.0000]])\n",
      "actions: [[ 0.00501431  0.28038204  0.08872834  0.12086996 -0.10990945  0.11450101\n",
      "  -0.20024776 -0.19671988 -0.04944466  0.13900995  0.13750364 -0.05333976\n",
      "   0.10491952 -0.13996293  0.10564071 -0.03896362  0.03727769 -0.04923079\n",
      "   0.15848699  0.00865384]]\n",
      "values: tensor([[-45423.1094]])\n",
      "clipped_actions: [[0.00501431 0.28038204 0.08872834 0.12086996 0.         0.11450101\n",
      "  0.         0.         0.         0.13900995 0.13750364 0.\n",
      "  0.10491952 0.         0.10564071 0.         0.03727769 0.\n",
      "  0.15848699 0.00865384]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.30873048]\n",
      "\n",
      "\n",
      "Log-Std at step 18438: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.060925592333078385 ; last reward: -0.30873048305511475\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.11053511 -0.11049295 -0.09448171 -0.06763119 -0.08909747  0.20810513\n",
      "   0.0287634  -0.21989381  0.06581081  0.06548148  0.25958645  0.19690755\n",
      "   0.02439051  0.20703241  0.14536694 -0.07899968 -0.11105864  0.0422458\n",
      "  -0.02508842 -0.07925951]]\n",
      "values: tensor([[-40414.9609]])\n",
      "clipped_actions: [[0.11053511 0.         0.         0.         0.         0.20810513\n",
      "  0.0287634  0.         0.06581081 0.06548148 0.25958645 0.19690755\n",
      "  0.02439051 0.20703241 0.14536694 0.         0.         0.0422458\n",
      "  0.         0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 5.4066826e-04 4.5893202e-03 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+00]]\n",
      "rewards: [-0.12426113]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.4067e-04, 4.5893e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00]])\n",
      "actions: [[ 0.1974976   0.06723424 -0.17571624 -0.11668566 -0.03935463  0.00316267\n",
      "  -0.17757156  0.15016799 -0.00875392  0.15228125 -0.07237618 -0.01685312\n",
      "   0.1440113  -0.08870503  0.17197245  0.1156202   0.05010965 -0.20356949\n",
      "  -0.05571705 -0.45216283]]\n",
      "values: tensor([[-56646.2266]])\n",
      "clipped_actions: [[0.1974976  0.06723424 0.         0.         0.         0.00316267\n",
      "  0.         0.15016799 0.         0.15228125 0.         0.\n",
      "  0.1440113  0.         0.17197245 0.1156202  0.05010965 0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2726403e-02 0.0000000e+00\n",
      "  0.0000000e+00 5.4066826e-04 4.5893202e-03 0.0000000e+00 5.9999999e-02\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 2.0000000e+00]]\n",
      "rewards: [-0.24846427]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2726e-02, 0.0000e+00, 0.0000e+00,\n",
      "         5.4067e-04, 4.5893e-03, 0.0000e+00, 6.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0000e+00]])\n",
      "actions: [[ 0.01781254  0.15117691 -0.19610891  0.13128635 -0.23851176 -0.06932537\n",
      "   0.06371479  0.11352978 -0.11679444  0.23938575  0.04179667 -0.05273392\n",
      "  -0.15124705  0.07842435  0.17031041  0.0974234   0.10046062 -0.20479603\n",
      "   0.04812257  0.15805985]]\n",
      "values: tensor([[-104694.8047]])\n",
      "clipped_actions: [[0.01781254 0.15117691 0.         0.13128635 0.         0.\n",
      "  0.06371479 0.11352978 0.         0.23938575 0.04179667 0.\n",
      "  0.         0.07842435 0.17031041 0.0974234  0.10046062 0.\n",
      "  0.04812257 0.15805985]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2726403e-02 0.0000000e+00\n",
      "  0.0000000e+00 5.4066826e-04 1.0548703e-02 0.0000000e+00 5.9999999e-02\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [-0.49721602]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2726e-02, 0.0000e+00, 0.0000e+00,\n",
      "         5.4067e-04, 1.0549e-02, 0.0000e+00, 6.0000e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e+00]])\n",
      "actions: [[-0.23551185  0.04024446  0.06957608 -0.00461535  0.13983905 -0.00481767\n",
      "  -0.05643165  0.10264153 -0.20528562  0.2435378   0.12871431  0.08425772\n",
      "   0.11995339  0.03816289 -0.11195403 -0.1102911  -0.11852615 -0.19625184\n",
      "  -0.09410921  0.0456067 ]]\n",
      "values: tensor([[-44513.7969]])\n",
      "clipped_actions: [[0.         0.04024446 0.06957608 0.         0.13983905 0.\n",
      "  0.         0.10264153 0.         0.2435378  0.12871431 0.08425772\n",
      "  0.11995339 0.03816289 0.         0.         0.         0.\n",
      "  0.         0.0456067 ]]\n",
      "new_obs: [[0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 8.232148e-03 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+01\n",
      "  1.000000e+01 0.000000e+00 0.000000e+00 1.000000e+01 1.000000e+01\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 4.000000e+00]]\n",
      "rewards: [-0.51624894]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.2321e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.0000e+00]])\n",
      "actions: [[-0.00578514 -0.04224896 -0.04955149  0.26205918 -0.0919949  -0.05447292\n",
      "   0.09488419 -0.03429991 -0.16709273  0.0619894   0.05379669  0.10538296\n",
      "   0.02539846  0.0960786   0.08194578  0.06238835  0.14525758 -0.2390668\n",
      "  -0.05604972  0.09626872]]\n",
      "values: tensor([[-23019.0645]])\n",
      "clipped_actions: [[0.         0.         0.         0.26205918 0.         0.\n",
      "  0.09488419 0.         0.         0.0619894  0.05379669 0.10538296\n",
      "  0.02539846 0.0960786  0.08194578 0.06238835 0.14525758 0.\n",
      "  0.         0.09626872]]\n",
      "new_obs: [[0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 8.232148e-03 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+01\n",
      "  1.000000e+01 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      "  0.000000e+00 5.000000e+00]]\n",
      "rewards: [-0.5162525]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         8.2321e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.0000e+00]])\n",
      "actions: [[-0.0867281   0.01798857 -0.16797036 -0.11418716  0.17015706 -0.14120415\n",
      "   0.16029929  0.05656365  0.2431866  -0.22769575  0.06843901  0.14552298\n",
      "   0.16026275 -0.04878974  0.00953875 -0.149039    0.00947919  0.15366085\n",
      "   0.04480308 -0.11071873]]\n",
      "values: tensor([[-2821.9492]])\n",
      "clipped_actions: [[0.         0.01798857 0.         0.         0.17015706 0.\n",
      "  0.16029929 0.05656365 0.2431866  0.         0.06843901 0.14552298\n",
      "  0.16026275 0.         0.00953875 0.         0.00947919 0.15366085\n",
      "  0.04480308 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.5264929]\n",
      "\n",
      "\n",
      "Log-Std at step 20490: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.0661905212700367 ; last reward: -0.5264928936958313\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[-0.11007793 -0.06641625 -0.04390489 -0.18362632 -0.04754511 -0.0524385\n",
      "  -0.16100688 -0.26639766  0.27421626 -0.06650353  0.19012085 -0.22300944\n",
      "  -0.0255421   0.02897983  0.00870915 -0.06221703 -0.16126482  0.21644886\n",
      "   0.09156713 -0.10652477]]\n",
      "values: tensor([[-435946.0625]])\n",
      "clipped_actions: [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.27421626 0.         0.19012085 0.\n",
      "  0.         0.02897983 0.00870915 0.         0.         0.21644886\n",
      "  0.09156713 0.        ]]\n",
      "new_obs: [[ 0.          0.21644886  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [0.00269765]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.2164,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[-0.05336478 -0.0292216   0.05858557 -0.08393607 -0.05802613  0.1345328\n",
      "  -0.11962231 -0.04243161 -0.07653096 -0.09594198 -0.11422306  0.11389753\n",
      "  -0.0700465  -0.03701572  0.0244375   0.10271583  0.12476963 -0.00121897\n",
      "  -0.00121382 -0.08164889]]\n",
      "values: tensor([[-392823.7812]])\n",
      "clipped_actions: [[0.         0.         0.05858557 0.         0.         0.1345328\n",
      "  0.         0.         0.         0.         0.         0.11389753\n",
      "  0.         0.         0.0244375  0.10271583 0.12476963 0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.06618406  0.08191606  0.          0.          0.05858557  0.\n",
      "   0.          0.          0.          0.          0.06        0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   2.        ]]\n",
      "rewards: [-0.11964975]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0662,  0.0819,  0.0000,  0.0000,  0.0586,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0600,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  2.0000]])\n",
      "actions: [[-0.01702515 -0.09994408  0.1636407  -0.0058023   0.18131052 -0.10040517\n",
      "   0.19672045 -0.05155278  0.23613875 -0.02977614 -0.09764583 -0.12732662\n",
      "   0.05311497 -0.12784341 -0.08532374 -0.06558028 -0.1288007   0.09643835\n",
      "   0.07318655  0.07556054]]\n",
      "values: tensor([[-509162.2500]])\n",
      "clipped_actions: [[0.         0.         0.1636407  0.         0.18131052 0.\n",
      "  0.19672045 0.         0.23613875 0.         0.         0.\n",
      "  0.05311497 0.         0.         0.         0.         0.09643835\n",
      "  0.07318655 0.07556054]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.05858557  0.\n",
      "   0.          0.          0.          0.          0.06        0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.        ]]\n",
      "rewards: [-0.36892077]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0586,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  3.0000]])\n",
      "actions: [[ 0.26573715 -0.28155884  0.28253952  0.4740862  -0.13136026 -0.07971352\n",
      "   0.20601515 -0.03854338 -0.22668001  0.22810815 -0.13085887 -0.03509441\n",
      "  -0.09388909 -0.01395876 -0.26055554 -0.14016642 -0.18431324  0.04770115\n",
      "   0.34039813  0.00542672]]\n",
      "values: tensor([[-254575.5625]])\n",
      "clipped_actions: [[0.26573715 0.         0.28253952 0.4740862  0.         0.\n",
      "  0.20601515 0.         0.         0.22810815 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04770115\n",
      "  0.34039813 0.00542672]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.05858557  0.\n",
      "   0.          0.          0.          0.          0.06        0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   4.        ]]\n",
      "rewards: [-0.36892864]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0586,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  4.0000]])\n",
      "actions: [[ 0.1614273  -0.02233395 -0.03011473 -0.09853901  0.1263193   0.05095814\n",
      "   0.07208568 -0.08679791  0.2636749   0.1906273   0.1631875   0.06508737\n",
      "  -0.20558898 -0.3722059  -0.03162476  0.20298569  0.11854692  0.11738854\n",
      "   0.17249386  0.06481664]]\n",
      "values: tensor([[-164105.6094]])\n",
      "clipped_actions: [[0.1614273  0.         0.         0.         0.1263193  0.05095814\n",
      "  0.07208568 0.         0.2636749  0.1906273  0.1631875  0.06508737\n",
      "  0.         0.         0.         0.20298569 0.11854692 0.11738854\n",
      "  0.17249386 0.06481664]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.05858557  0.\n",
      "   0.          0.          0.          0.          0.06        0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   5.        ]]\n",
      "rewards: [-0.36893657]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0586,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0600,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  5.0000]])\n",
      "actions: [[-0.14018428  0.12696657 -0.05780715 -0.07164092 -0.1799363   0.07267156\n",
      "  -0.22847433  0.03782472  0.08206163 -0.04842875 -0.12704681 -0.13395639\n",
      "  -0.00707953  0.00484323  0.05694227 -0.01971506 -0.02148044 -0.08263881\n",
      "  -0.08701679 -0.10219862]]\n",
      "values: tensor([[-44238.4453]])\n",
      "clipped_actions: [[0.         0.12696657 0.         0.         0.         0.07267156\n",
      "  0.         0.03782472 0.08206163 0.         0.         0.\n",
      "  0.         0.00484323 0.05694227 0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.36870092]\n",
      "\n",
      "\n",
      "Log-Std at step 22536: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.06987753048539162 ; last reward: -0.36870092153549194\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 1.5462379e-01 -1.8845987e-01 -2.4120086e-01 -2.5307721e-01\n",
      "   2.2550672e-04 -7.7894941e-02 -1.3875213e-01 -1.2610979e-01\n",
      "  -5.4826371e-02  1.3933931e-01 -1.1676159e-01  8.9873873e-02\n",
      "  -1.1288575e-01  3.2754639e-01 -8.3801582e-02  1.6929843e-02\n",
      "   2.2817212e-01 -2.8152026e-02  1.4018911e-01  1.7994978e-02]]\n",
      "values: tensor([[-67667.4844]])\n",
      "clipped_actions: [[1.5462379e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2550672e-04\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3933931e-01\n",
      "  0.0000000e+00 8.9873873e-02 0.0000000e+00 3.2754639e-01 0.0000000e+00\n",
      "  1.6929843e-02 2.2817212e-01 0.0000000e+00 1.4018911e-01 1.7994978e-02]]\n",
      "new_obs: [[ 0.07354833  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.12396173]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0735,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[ 0.06518804 -0.02883107  0.04707695 -0.18668392 -0.33380508 -0.01398179\n",
      "   0.24686012 -0.21142781 -0.07378306 -0.00627322  0.19642946 -0.03180178\n",
      "  -0.07074556  0.08823985 -0.14417015 -0.07473143  0.00886962  0.05082726\n",
      "  -0.27075094  0.07493721]]\n",
      "values: tensor([[-60587.6406]])\n",
      "clipped_actions: [[0.06518804 0.         0.04707695 0.         0.         0.\n",
      "  0.24686012 0.         0.         0.         0.19642946 0.\n",
      "  0.         0.08823985 0.         0.         0.00886962 0.05082726\n",
      "  0.         0.07493721]]\n",
      "new_obs: [[ 0.          0.          0.04785699  0.          0.          0.\n",
      "   0.          0.          0.06        0.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   2.        ]]\n",
      "rewards: [-0.2476495]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0479,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0600,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  2.0000]])\n",
      "actions: [[-0.09639934  0.03484694 -0.11369991 -0.00432811  0.0004608  -0.13781676\n",
      "   0.2989667  -0.1867982   0.02582338 -0.12854773  0.11907262 -0.25219622\n",
      "   0.00994272  0.05621704 -0.23460306 -0.16729444 -0.19107096  0.07014399\n",
      "  -0.12294465  0.03885385]]\n",
      "values: tensor([[-101394.0234]])\n",
      "clipped_actions: [[0.         0.03484694 0.         0.         0.0004608  0.\n",
      "  0.2989667  0.         0.02582338 0.         0.11907262 0.\n",
      "  0.00994272 0.05621704 0.         0.         0.         0.07014399\n",
      "  0.         0.03885385]]\n",
      "new_obs: [[ 0.0000000e+00  0.0000000e+00  2.2141557e-02  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  2.5823381e-02  0.0000000e+00\n",
      "  -8.7095601e-03  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  1.0000000e+01  1.0000000e+01\n",
      "   0.0000000e+00  0.0000000e+00  1.0000000e+01  1.0000000e+01\n",
      "   0.0000000e+00  0.0000000e+00  1.0000000e+01  1.0000000e+01\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   3.0000000e+00]]\n",
      "rewards: [-0.37124813]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000e+00,  0.0000e+00,  2.2142e-02,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  2.5823e-02,  0.0000e+00, -8.7096e-03,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+01,\n",
      "          1.0000e+01,  0.0000e+00,  0.0000e+00,  1.0000e+01,  1.0000e+01,\n",
      "          0.0000e+00,  0.0000e+00,  1.0000e+01,  1.0000e+01,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  3.0000e+00]])\n",
      "actions: [[ 0.02891417 -0.01720811 -0.20984541 -0.22978438 -0.07010424 -0.14393415\n",
      "  -0.0815502   0.1799295   0.04102387 -0.06778426  0.03325041 -0.12520176\n",
      "   0.11058182 -0.12630618  0.1204602   0.06045542  0.07516391  0.1295106\n",
      "  -0.0612715   0.3494966 ]]\n",
      "values: tensor([[-43613.3867]])\n",
      "clipped_actions: [[0.02891417 0.         0.         0.         0.         0.\n",
      "  0.         0.1799295  0.04102387 0.         0.03325041 0.\n",
      "  0.11058182 0.         0.1204602  0.06045542 0.07516391 0.1295106\n",
      "  0.         0.3494966 ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 3.4694470e-18 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 4.7964938e-02 0.0000000e+00 5.5583273e+13 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 4.0000000e+00]]\n",
      "rewards: [-0.37014955]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 3.4694e-18, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.7965e-02, 0.0000e+00, 5.5583e+13, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.0000e+00]])\n",
      "actions: [[ 1.7877967e+10  1.2509676e+09 -6.2424297e+10 -2.8893495e+10\n",
      "  -4.3899367e+10  3.9598244e+10  2.6471195e+10 -5.2384670e+10\n",
      "   4.4739752e+10 -1.4429424e+10 -6.6434253e+10 -6.9591671e+10\n",
      "   3.1910236e+10  1.0246164e+11 -3.5303154e+10 -1.2877191e+10\n",
      "   7.0141764e+10 -6.1659187e+09  1.7730146e+10 -4.3567931e+10]]\n",
      "values: tensor([[-2.7153e+16]])\n",
      "clipped_actions: [[50. 50.  0.  0.  0. 50. 50.  0. 50.  0.  0.  0. 50. 50.  0.  0. 50.  0.\n",
      "  50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   5.]]\n",
      "rewards: [-0.4300579]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.]])\n",
      "actions: [[ 0.1546916   0.01592269  0.05605714  0.22774991 -0.2787372   0.02000858\n",
      "   0.1393038   0.04110204 -0.16917829  0.03115096  0.11858707  0.14486535\n",
      "   0.01573822 -0.03728068 -0.04752104 -0.0289544  -0.07215823 -0.07033131\n",
      "   0.14215657 -0.17457777]]\n",
      "values: tensor([[-4622.4604]])\n",
      "clipped_actions: [[0.1546916  0.01592269 0.05605714 0.22774991 0.         0.02000858\n",
      "  0.1393038  0.04110204 0.         0.03115096 0.11858707 0.14486535\n",
      "  0.01573822 0.         0.         0.         0.         0.\n",
      "  0.14215657 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.4300595]\n",
      "\n",
      "\n",
      "Log-Std at step 24582: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.07417812541127206 ; last reward: -0.4300594925880432\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[-0.08032945 -0.24967663  0.13017224 -0.08281834  0.15495613 -0.17800367\n",
      "   0.06042344 -0.13367349 -0.1300606  -0.05645445 -0.069846    0.20582788\n",
      "   0.07064483  0.30480427 -0.06321906  0.03039641  0.00155943  0.19912279\n",
      "   0.07716686 -0.01461352]]\n",
      "values: tensor([[-315520.3125]])\n",
      "clipped_actions: [[0.         0.         0.13017224 0.         0.15495613 0.\n",
      "  0.06042344 0.         0.         0.         0.         0.20582788\n",
      "  0.07064483 0.30480427 0.         0.03039641 0.00155943 0.19912279\n",
      "  0.07716686 0.        ]]\n",
      "new_obs: [[2.1684043e-19 0.0000000e+00 1.4326009e-01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5999999e-01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+00]]\n",
      "rewards: [-0.12108777]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[2.1684e-19, 0.0000e+00, 1.4326e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 2.6000e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00]])\n",
      "actions: [[ 0.00689465  0.07044556 -0.18798536  0.36139137  0.19546665 -0.15458883\n",
      "  -0.21622989  0.23030776  0.25172868 -0.10929158 -0.03071727 -0.07640725\n",
      "  -0.07894686 -0.01612221 -0.14279608  0.00646231  0.13083872 -0.09967915\n",
      "  -0.07464317 -0.3347063 ]]\n",
      "values: tensor([[-335426.3125]])\n",
      "clipped_actions: [[0.00689465 0.07044556 0.         0.36139137 0.19546665 0.\n",
      "  0.         0.23030776 0.25172868 0.         0.         0.\n",
      "  0.         0.         0.         0.00646231 0.13083872 0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1008305e-02 0.0000000e+00\n",
      "  1.0131199e-01 1.4531621e-01 6.4623058e-03 0.0000000e+00 5.9999999e-02\n",
      "  0.0000000e+00 6.3827172e-02 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 2.0000000e+00]]\n",
      "rewards: [-0.11047058]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1008e-02, 0.0000e+00, 1.0131e-01,\n",
      "         1.4532e-01, 6.4623e-03, 0.0000e+00, 6.0000e-02, 0.0000e+00, 6.3827e-02,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0000e+00]])\n",
      "actions: [[ 0.00734745  0.02731103 -0.30588847 -0.09739005  0.00680566  0.04812835\n",
      "  -0.2159706  -0.10286681 -0.09554937 -0.11921388 -0.18421465 -0.13647264\n",
      "   0.14334871 -0.11504922  0.02369608  0.08723226  0.07750333 -0.09456713\n",
      "  -0.10003267 -0.0363534 ]]\n",
      "values: tensor([[-491616.5000]])\n",
      "clipped_actions: [[0.00734745 0.02731103 0.         0.         0.00680566 0.04812835\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.14334871 0.         0.02369608 0.08723226 0.07750333 0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[4.2844858e-02 0.0000000e+00 7.3474473e-03 4.8319332e-02 0.0000000e+00\n",
      "  0.0000000e+00 1.6695808e-01 8.6132422e-02 5.9999999e-02 9.3913168e-02\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [-0.10402153]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[4.2845e-02, 0.0000e+00, 7.3474e-03, 4.8319e-02, 0.0000e+00, 0.0000e+00,\n",
      "         1.6696e-01, 8.6132e-02, 6.0000e-02, 9.3913e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e+00]])\n",
      "actions: [[ 0.10752007  0.14047924 -0.12193536  0.03081731  0.02067737 -0.2180485\n",
      "   0.14627847  0.00730931 -0.01633111 -0.04626609 -0.01764719 -0.00106837\n",
      "  -0.16108513  0.1125886  -0.1041749  -0.08121678  0.10193869  0.021554\n",
      "  -0.33296147 -0.20400925]]\n",
      "values: tensor([[-215251.3281]])\n",
      "clipped_actions: [[0.10752007 0.14047924 0.         0.03081731 0.02067737 0.\n",
      "  0.14627847 0.00730931 0.         0.         0.         0.\n",
      "  0.         0.1125886  0.         0.         0.10193869 0.021554\n",
      "  0.         0.        ]]\n",
      "new_obs: [[6.9388939e-18 0.0000000e+00 2.3869712e-02 6.9906324e-02 0.0000000e+00\n",
      "  4.7355969e-03 1.6695808e-01 8.6132422e-02 1.0153112e-01 1.1244110e-01\n",
      "  0.0000000e+00 5.9999999e-02 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 4.0000000e+00]]\n",
      "rewards: [-0.10295456]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[6.9389e-18, 0.0000e+00, 2.3870e-02, 6.9906e-02, 0.0000e+00, 4.7356e-03,\n",
      "         1.6696e-01, 8.6132e-02, 1.0153e-01, 1.1244e-01, 0.0000e+00, 6.0000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.0000e+00]])\n",
      "actions: [[-0.02990937  0.25590175 -0.0194085   0.03968015  0.05310676 -0.17408805\n",
      "  -0.20028295 -0.09156696  0.23023503 -0.07465008  0.01616159 -0.15944594\n",
      "   0.19040485 -0.04888754 -0.05250207 -0.15742618 -0.01150148 -0.11122366\n",
      "  -0.14341462 -0.32775474]]\n",
      "values: tensor([[-125337.2031]])\n",
      "clipped_actions: [[0.         0.25590175 0.         0.03968015 0.05310676 0.\n",
      "  0.         0.         0.23023503 0.         0.01616159 0.\n",
      "  0.19040485 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 5.3744733e-02 0.0000000e+00\n",
      "  4.7355969e-03 2.0698939e-01 8.6132422e-02 0.0000000e+00 7.8628905e-02\n",
      "  0.0000000e+00 5.9999999e-02 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 5.0000000e+00]]\n",
      "rewards: [-0.10095979]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 5.3745e-02, 0.0000e+00, 4.7356e-03,\n",
      "         2.0699e-01, 8.6132e-02, 0.0000e+00, 7.8629e-02, 0.0000e+00, 6.0000e-02,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         5.0000e+00]])\n",
      "actions: [[-0.02630727  0.13338435 -0.08301394 -0.212043   -0.19801126 -0.02347165\n",
      "  -0.08401674  0.14048658 -0.02217319  0.16005293 -0.03078094 -0.20061144\n",
      "   0.17465852  0.13889307  0.18468082 -0.11153173  0.12050204  0.04534989\n",
      "   0.1490931   0.0726346 ]]\n",
      "values: tensor([[-17981.4004]])\n",
      "clipped_actions: [[0.         0.13338435 0.         0.         0.         0.\n",
      "  0.         0.14048658 0.         0.16005293 0.         0.\n",
      "  0.17465852 0.13889307 0.18468082 0.         0.12050204 0.04534989\n",
      "  0.1490931  0.0726346 ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.37738317]\n",
      "\n",
      "\n",
      "Log-Std at step 26634: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.0779519571363926 ; last reward: -0.37738317251205444\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.05664723 -0.20570879 -0.1367486   0.14277743 -0.17031944  0.23535654\n",
      "  -0.13430807 -0.29068825 -0.00754702  0.00943402 -0.1787373  -0.17812508\n",
      "   0.0909165   0.1064597   0.19740447 -0.1276948  -0.17855838  0.08077679\n",
      "  -0.04233049  0.05905482]]\n",
      "values: tensor([[-315520.3125]])\n",
      "clipped_actions: [[0.05664723 0.         0.         0.14277743 0.         0.23535654\n",
      "  0.         0.         0.         0.00943402 0.         0.\n",
      "  0.0909165  0.1064597  0.19740447 0.         0.         0.08077679\n",
      "  0.         0.05905482]]\n",
      "new_obs: [[ 0.          0.          0.          0.08077679  0.          0.\n",
      "   0.          0.          0.          0.26        0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [0.00200771]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0808,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.2600,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[-0.09686469 -0.08330078 -0.11500343 -0.25141796 -0.15429525 -0.24047591\n",
      "   0.05027892 -0.176772   -0.02229803 -0.12253886  0.03613167  0.01879536\n",
      "   0.16467184  0.08598206  0.38858753  0.09448537 -0.1786924   0.09749118\n",
      "  -0.04887699 -0.08998558]]\n",
      "values: tensor([[-335336.3125]])\n",
      "clipped_actions: [[0.         0.         0.         0.         0.         0.\n",
      "  0.05027892 0.         0.         0.         0.03613167 0.01879536\n",
      "  0.16467184 0.08598206 0.38858753 0.09448537 0.         0.09749118\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.04721227  0.          0.02584976  0.          0.\n",
      "   0.03613167  0.01879536  0.         -0.29246268  0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   2.        ]]\n",
      "rewards: [-0.11895075]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0472,  0.0000,  0.0258,  0.0000,  0.0000,  0.0361,  0.0188,\n",
      "          0.0000, -0.2925,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  2.0000]])\n",
      "actions: [[-0.0416848  -0.28018388 -0.15312846 -0.27317825 -0.18843     0.22868271\n",
      "   0.09323452 -0.11945517 -0.13457546  0.02896968  0.10677272 -0.01366537\n",
      "   0.07812302  0.10191111 -0.05522573 -0.03592177  0.1852626   0.01018088\n",
      "  -0.01424167  0.15579553]]\n",
      "values: tensor([[-491528.9062]])\n",
      "clipped_actions: [[0.         0.         0.         0.         0.         0.22868271\n",
      "  0.09323452 0.         0.         0.02896968 0.10677272 0.\n",
      "  0.07812302 0.10191111 0.         0.         0.1852626  0.01018088\n",
      "  0.         0.15579553]]\n",
      "new_obs: [[ 0.1852626   0.          0.          0.02584976  0.          0.\n",
      "   0.03613167  0.01879536  0.         -0.29246268  0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.        ]]\n",
      "rewards: [-0.36520413]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.1853,  0.0000,  0.0000,  0.0258,  0.0000,  0.0000,  0.0361,  0.0188,\n",
      "          0.0000, -0.2925,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  3.0000]])\n",
      "actions: [[ 0.04290918 -0.32100645 -0.01378523  0.06096201  0.09092695  0.26083043\n",
      "  -0.06738998 -0.3321237   0.08365504 -0.22489637  0.05143273  0.2762141\n",
      "  -0.09652963 -0.06560921 -0.29076815  0.0006395  -0.0301211  -0.00686277\n",
      "  -0.1680823  -0.16665658]]\n",
      "values: tensor([[-214850.]])\n",
      "clipped_actions: [[0.04290918 0.         0.         0.06096201 0.09092695 0.26083043\n",
      "  0.         0.         0.08365504 0.         0.05143273 0.2762141\n",
      "  0.         0.         0.         0.0006395  0.         0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.08139142  0.          0.          0.          0.          0.06032252\n",
      "   0.04018947  0.04122682  0.          0.          0.          0.06063608\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   4.        ]]\n",
      "rewards: [-0.48667356]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0814,  0.0000,  0.0000,  0.0000,  0.0000,  0.0603,  0.0402,  0.0412,\n",
      "          0.0000,  0.0000,  0.0000,  0.0606,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  4.0000]])\n",
      "actions: [[ 0.03819105  0.10108009 -0.07776027  0.05757915 -0.19293109 -0.00136205\n",
      "  -0.00023781 -0.13314089  0.03812144  0.06912394  0.18718877  0.13440351\n",
      "  -0.03563913 -0.14466967  0.01111529 -0.03932615 -0.01540191  0.12356337\n",
      "   0.10085039 -0.19193739]]\n",
      "values: tensor([[-124974.8828]])\n",
      "clipped_actions: [[0.03819105 0.10108009 0.         0.05757915 0.         0.\n",
      "  0.         0.         0.03812144 0.06912394 0.18718877 0.13440351\n",
      "  0.         0.         0.01111529 0.         0.         0.12356337\n",
      "  0.10085039 0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.06032252\n",
      "   0.          0.04122682  0.          0.          0.          0.06063608\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   5.        ]]\n",
      "rewards: [-0.9094637]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0603,  0.0000,  0.0412,\n",
      "          0.0000,  0.0000,  0.0000,  0.0606,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  5.0000]])\n",
      "actions: [[ 0.05301049 -0.11002161 -0.00961292 -0.01636033  0.10413322 -0.06542425\n",
      "  -0.22515649  0.00744538  0.1830422   0.07989456  0.14079848  0.03022747\n",
      "   0.20396361  0.05139527  0.26109535  0.05775594 -0.18404831  0.12011572\n",
      "  -0.30817878 -0.04426236]]\n",
      "values: tensor([[-17793.9473]])\n",
      "clipped_actions: [[0.05301049 0.         0.         0.         0.10413322 0.\n",
      "  0.         0.00744538 0.1830422  0.07989456 0.14079848 0.03022747\n",
      "  0.20396361 0.05139527 0.26109535 0.05775594 0.         0.12011572\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.90643394]\n",
      "\n",
      "\n",
      "Log-Std at step 28680: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.08701629653573036 ; last reward: -0.9064339399337769\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.03674832  0.12467243 -0.02927801  0.11694363  0.12333564 -0.01110529\n",
      "  -0.02252111  0.05765279 -0.13302377 -0.11989695 -0.02885892  0.03487881\n",
      "   0.26881713  0.14252019  0.14410764  0.0683093  -0.18886465  0.19490355\n",
      "  -0.13555256 -0.16494456]]\n",
      "values: tensor([[-315520.3125]])\n",
      "clipped_actions: [[0.03674832 0.12467243 0.         0.11694363 0.12333564 0.\n",
      "  0.         0.05765279 0.         0.         0.         0.03487881\n",
      "  0.26881713 0.14252019 0.14410764 0.0683093  0.         0.19490355\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.01391513  0.12333564  0.          0.          0.\n",
      "   0.          0.          0.26        0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.12115133]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0139,  0.1233,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.2600,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[ 0.13600323  0.12996973 -0.08291137  0.04973978 -0.00246323  0.04085322\n",
      "  -0.06045602  0.16096574  0.03094584 -0.07110997 -0.03285218  0.03266361\n",
      "  -0.10805359 -0.01936304  0.22464378 -0.161679   -0.0254802  -0.25472942\n",
      "   0.06342694 -0.02957974]]\n",
      "values: tensor([[-335434.9375]])\n",
      "clipped_actions: [[0.13600323 0.12996973 0.         0.04973978 0.         0.04085322\n",
      "  0.         0.16096574 0.03094584 0.         0.         0.03266361\n",
      "  0.         0.         0.22464378 0.         0.         0.\n",
      "  0.06342694 0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 9.2389792e-02 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0945845e-02 2.8167707e-03 1.7291333e-01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 2.0000000e+00]]\n",
      "rewards: [-0.24386643]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 9.2390e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0946e-02, 2.8168e-03, 1.7291e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0000e+00]])\n",
      "actions: [[ 0.03788549 -0.14505084 -0.07178749  0.2422502   0.10262241  0.02172427\n",
      "  -0.19133323  0.08473554  0.17370571  0.05920808 -0.3512932  -0.09544475\n",
      "   0.02555295 -0.49785492 -0.015442   -0.08414471 -0.03403798  0.12570351\n",
      "  -0.10642701  0.17419857]]\n",
      "values: tensor([[-491885.4062]])\n",
      "clipped_actions: [[0.03788549 0.         0.         0.2422502  0.10262241 0.02172427\n",
      "  0.         0.08473554 0.17370571 0.05920808 0.         0.\n",
      "  0.02555295 0.         0.         0.         0.         0.12570351\n",
      "  0.         0.17419857]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 9.2389792e-02 1.3060972e-02 0.0000000e+00\n",
      "  5.0944336e-02 3.0945845e-02 2.8167707e-03 1.7291333e-01 2.5999999e-01\n",
      "  0.0000000e+00 2.5999999e-01 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [-0.3666734]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 9.2390e-02, 1.3061e-02, 0.0000e+00, 5.0944e-02,\n",
      "         3.0946e-02, 2.8168e-03, 1.7291e-01, 2.6000e-01, 0.0000e+00, 2.6000e-01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e+00]])\n",
      "actions: [[ 0.12094177  0.01958727  0.10684507  0.18152384  0.15428789 -0.08516296\n",
      "   0.01088559 -0.29188558  0.15625885 -0.00992541 -0.18895656 -0.18308584\n",
      "   0.15816942 -0.05403974  0.03897657  0.0751006  -0.00635283 -0.03101484\n",
      "  -0.06244595 -0.04811238]]\n",
      "values: tensor([[-215260.7344]])\n",
      "clipped_actions: [[0.12094177 0.01958727 0.10684507 0.18152384 0.15428789 0.\n",
      "  0.01088559 0.         0.15625885 0.         0.         0.\n",
      "  0.15816942 0.         0.03897657 0.0751006  0.         0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.01306097  0.          0.\n",
      "   0.1407417   0.03635503  0.          0.26        0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   4.        ]]\n",
      "rewards: [-0.35954717]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0131,  0.0000,  0.0000,  0.1407,  0.0364,\n",
      "          0.0000,  0.2600,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  4.0000]])\n",
      "actions: [[ 3.2821450e-02 -2.1373853e-04 -3.4405668e-03 -1.7673199e-01\n",
      "   2.0305064e-02  5.9311450e-02 -2.2550985e-01 -7.8052357e-02\n",
      "  -8.0815420e-02 -6.8027914e-02 -3.1971204e-01 -1.4106204e-01\n",
      "   4.8500121e-02  1.8819217e-01  5.4392423e-02 -1.3641806e-01\n",
      "   8.9545049e-02  8.8928230e-02 -3.0958679e-02 -2.8876162e-01]]\n",
      "values: tensor([[-125289.6094]])\n",
      "clipped_actions: [[0.03282145 0.         0.         0.         0.02030506 0.05931145\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.04850012 0.18819217 0.05439242 0.         0.08954505 0.08892823\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.01306097  0.          0.\n",
      "   0.1407417   0.03635503  0.          0.26        0.          0.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   5.        ]]\n",
      "rewards: [-0.35955212]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0131,  0.0000,  0.0000,  0.1407,  0.0364,\n",
      "          0.0000,  0.2600,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  5.0000]])\n",
      "actions: [[-0.10956044  0.203634   -0.04211417  0.20143487 -0.00645167 -0.15599073\n",
      "  -0.18432018 -0.23447564  0.04902279 -0.02463274  0.19421767  0.00164402\n",
      "  -0.01090489 -0.18310209  0.0433028  -0.04567859 -0.07009748 -0.17103688\n",
      "   0.21862859  0.08996473]]\n",
      "values: tensor([[-17988.6387]])\n",
      "clipped_actions: [[0.         0.203634   0.         0.20143487 0.         0.\n",
      "  0.         0.         0.04902279 0.         0.19421767 0.00164402\n",
      "  0.         0.         0.0433028  0.         0.         0.\n",
      "  0.21862859 0.08996473]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.5961079]\n",
      "\n",
      "\n",
      "Log-Std at step 30726: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.09297737553715706 ; last reward: -0.5961079001426697\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.0580715  -0.18230103  0.20185931  0.1219099  -0.07551271 -0.3159053\n",
      "   0.09685546  0.12566419  0.03043499  0.03518683  0.03578734 -0.05748101\n",
      "   0.15277353 -0.05526739 -0.15666205 -0.09947591 -0.29020104  0.13237499\n",
      "   0.11920179  0.09111626]]\n",
      "values: tensor([[-231363.0938]])\n",
      "clipped_actions: [[0.0580715  0.         0.20185931 0.1219099  0.         0.\n",
      "  0.09685546 0.12566419 0.03043499 0.03518683 0.03578734 0.\n",
      "  0.15277353 0.         0.         0.         0.         0.13237499\n",
      "  0.11920179 0.09111626]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.07475653\n",
      "   0.          0.          0.          0.          0.          0.26\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.12205644]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0748,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.2600, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[ 0.0180372   0.10222655  0.04967596  0.01150106 -0.11453483 -0.04687491\n",
      "  -0.14153507 -0.1831029  -0.17906632 -0.21874252 -0.07354698 -0.19575775\n",
      "   0.2779563   0.279955   -0.14513005 -0.04936795 -0.07753752 -0.11014411\n",
      "  -0.2045464   0.00816005]]\n",
      "values: tensor([[-278517.2812]])\n",
      "clipped_actions: [[0.0180372  0.10222655 0.04967596 0.01150106 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2779563  0.279955   0.         0.         0.         0.\n",
      "  0.         0.00816005]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.07475653\n",
      "   0.          0.          0.          0.          0.          0.26\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   2.        ]]\n",
      "rewards: [-0.12205715]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0748,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.2600, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  2.0000]])\n",
      "actions: [[ 0.19895622  0.14208083  0.27634037  0.0249065  -0.175414    0.1064052\n",
      "  -0.04875834 -0.19249693 -0.14477253 -0.22383443 -0.15305848  0.23677331\n",
      "  -0.02597952  0.03791872 -0.02782672 -0.03124387  0.25863326  0.17279798\n",
      "   0.24316216 -0.08185362]]\n",
      "values: tensor([[-309794.9062]])\n",
      "clipped_actions: [[0.19895622 0.14208083 0.27634037 0.0249065  0.         0.1064052\n",
      "  0.         0.         0.         0.         0.         0.23677331\n",
      "  0.         0.03791872 0.         0.         0.25863326 0.17279798\n",
      "  0.24316216 0.        ]]\n",
      "new_obs: [[ 0.          0.06639278  0.08011518  0.          0.          0.08478581\n",
      "   0.          0.          0.06        0.          0.          0.26709738\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.        ]]\n",
      "rewards: [-0.3668247]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0664,  0.0801,  0.0000,  0.0000,  0.0848,  0.0000,  0.0000,\n",
      "          0.0600,  0.0000,  0.0000,  0.2671,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  3.0000]])\n",
      "actions: [[-0.01622074 -0.15285611 -0.07641654  0.05619203  0.04359585 -0.04859657\n",
      "   0.11103268  0.14232135  0.17210785 -0.07295364  0.01402344 -0.00783655\n",
      "  -0.0895817  -0.13056862  0.05022945 -0.02399768 -0.07086447 -0.12765023\n",
      "  -0.05532232 -0.10747588]]\n",
      "values: tensor([[-90706.4219]])\n",
      "clipped_actions: [[0.         0.         0.         0.05619203 0.04359585 0.\n",
      "  0.11103268 0.14232135 0.17210785 0.         0.01402344 0.\n",
      "  0.         0.         0.05022945 0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.02482496  0.08478581\n",
      "   0.08986245  0.          0.          0.          0.26        0.26709738\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   4.        ]]\n",
      "rewards: [-0.4856742]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0248,  0.0848,  0.0899,  0.0000,\n",
      "          0.0000,  0.0000,  0.2600,  0.2671,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  4.0000]])\n",
      "actions: [[ 0.04295986 -0.05394027 -0.08390262  0.01767837 -0.10362325  0.0936684\n",
      "   0.07888386 -0.10492048  0.07287236 -0.06861453 -0.00173828  0.137646\n",
      "   0.10542366  0.13976012 -0.16337095  0.05417552  0.08233554  0.12161071\n",
      "   0.00930595 -0.01102037]]\n",
      "values: tensor([[-34468.2461]])\n",
      "clipped_actions: [[0.04295986 0.         0.         0.01767837 0.         0.0936684\n",
      "  0.07888386 0.         0.07287236 0.         0.         0.137646\n",
      "  0.10542366 0.13976012 0.         0.05417552 0.08233554 0.12161071\n",
      "  0.00930595 0.        ]]\n",
      "new_obs: [[ 0.          0.          0.          0.          0.          0.03061029\n",
      "   0.09123069  0.06832629  0.          0.          0.         -0.20562409\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   5.        ]]\n",
      "rewards: [-0.49375638]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0306,  0.0912,  0.0683,\n",
      "          0.0000,  0.0000,  0.0000, -0.2056,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  5.0000]])\n",
      "actions: [[ 0.1778563   0.02553878 -0.13432312  0.11571544 -0.02127881  0.05051586\n",
      "  -0.00849078 -0.21653986 -0.00683668 -0.07260318 -0.03681507 -0.14623733\n",
      "   0.09799239 -0.07607906  0.12868041  0.11532928  0.16023058 -0.11443526\n",
      "   0.00207125 -0.35329175]]\n",
      "values: tensor([[-2273.4341]])\n",
      "clipped_actions: [[0.1778563  0.02553878 0.         0.11571544 0.         0.05051586\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09799239 0.         0.12868041 0.11532928 0.16023058 0.\n",
      "  0.00207125 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.49490806]\n",
      "\n",
      "\n",
      "Log-Std at step 32778: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.09792645618319512 ; last reward: -0.49490806460380554\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.,  0.,  0.,  0., 10., 10.,\n",
      "          0.,  0., 10., 10.,  0.,  0., 10., 10.,  0.]])\n",
      "actions: [[ 0.11630943 -0.0285663   0.18316358 -0.15142128 -0.0015861  -0.03863801\n",
      "  -0.02141712 -0.22443883 -0.12057827 -0.16652273  0.11787891 -0.24091738\n",
      "   0.1723389   0.23004091 -0.3406044  -0.08449321  0.07560477  0.03048979\n",
      "   0.01705173 -0.10087712]]\n",
      "values: tensor([[-234583.5938]])\n",
      "clipped_actions: [[0.11630943 0.         0.18316358 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.11787891 0.\n",
      "  0.1723389  0.23004091 0.         0.         0.07560477 0.03048979\n",
      "  0.01705173 0.        ]]\n",
      "new_obs: [[ 0.          0.03048979  0.02936341  0.          0.          0.\n",
      "   0.          0.          0.06        0.          0.          0.\n",
      "  10.         10.          0.          0.         10.         10.\n",
      "   0.          0.          0.          0.         10.         10.\n",
      "   0.          0.         10.         10.          0.          0.\n",
      "  10.         10.          0.          0.          0.          0.\n",
      "   1.        ]]\n",
      "rewards: [-0.12287294]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.0305,  0.0294,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0600,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000,\n",
      "         10.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000, 10.0000, 10.0000,  0.0000,  0.0000, 10.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  1.0000]])\n",
      "actions: [[ 0.03969751  0.0335556  -0.00262949 -0.05959905 -0.13722146  0.28292435\n",
      "  -0.06702936  0.25842634  0.21092871  0.01666898 -0.16299063  0.20477028\n",
      "   0.01870477  0.1436199  -0.16651121  0.10907798  0.00871826  0.02345623\n",
      "   0.13363433  0.0205795 ]]\n",
      "values: tensor([[-282551.3125]])\n",
      "clipped_actions: [[0.03969751 0.0335556  0.         0.         0.         0.28292435\n",
      "  0.         0.25842634 0.21092871 0.01666898 0.         0.20477028\n",
      "  0.01870477 0.1436199  0.         0.10907798 0.00871826 0.02345623\n",
      "  0.13363433 0.0205795 ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.1591464e-02 2.4965662e-03 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 2.0000000e+00]]\n",
      "rewards: [-0.3691443]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.1591e-02, 2.4966e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         2.0000e+00]])\n",
      "actions: [[ 9.4224051e-02 -1.2367041e-01  1.2160761e-02 -1.2355491e-01\n",
      "  -2.8758081e-02  3.1148031e-02  1.6146062e-01 -1.6178931e-01\n",
      "  -2.2565350e-01 -1.3520689e-01 -3.6397707e-01 -2.2192164e-01\n",
      "   1.7320308e-01 -3.3836734e-01  7.9945907e-02 -1.3593237e-01\n",
      "   2.5414009e-02  8.0582902e-02  2.1731481e-05 -1.2673056e-01]]\n",
      "values: tensor([[-313511.5625]])\n",
      "clipped_actions: [[9.4224051e-02 0.0000000e+00 1.2160761e-02 0.0000000e+00 0.0000000e+00\n",
      "  3.1148031e-02 1.6146062e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.7320308e-01 0.0000000e+00 7.9945907e-02\n",
      "  0.0000000e+00 2.5414009e-02 8.0582902e-02 2.1731481e-05 0.0000000e+00]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 2.2508955e-02 1.3031599e-02 0.0000000e+00\n",
      "  0.0000000e+00 3.1591464e-02 2.4965662e-03 5.9999999e-02 2.5999999e-01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [-0.49225035]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 2.2509e-02, 1.3032e-02, 0.0000e+00, 0.0000e+00,\n",
      "         3.1591e-02, 2.4966e-03, 6.0000e-02, 2.6000e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         3.0000e+00]])\n",
      "actions: [[ 0.01272698  0.12225804 -0.16860679  0.09595215 -0.07302003  0.04047937\n",
      "  -0.10341122 -0.24631807 -0.06723094 -0.04901645 -0.18224043 -0.09830488\n",
      "  -0.14260972  0.08016663  0.07112451 -0.01306197  0.03589068 -0.0184337\n",
      "  -0.03156773 -0.08152002]]\n",
      "values: tensor([[-91894.6641]])\n",
      "clipped_actions: [[0.01272698 0.12225804 0.         0.09595215 0.         0.04047937\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.08016663 0.07112451 0.         0.03589068 0.\n",
      "  0.         0.        ]]\n",
      "new_obs: [[0.0000000e+00 0.0000000e+00 2.2508955e-02 1.3031599e-02 0.0000000e+00\n",
      "  0.0000000e+00 3.1591464e-02 2.4965662e-03 5.9999999e-02 2.5999999e-01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+01\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 1.0000000e+01 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 4.0000000e+00]]\n",
      "rewards: [-0.4922561]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[0.0000e+00, 0.0000e+00, 2.2509e-02, 1.3032e-02, 0.0000e+00, 0.0000e+00,\n",
      "         3.1591e-02, 2.4966e-03, 6.0000e-02, 2.6000e-01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+01, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         4.0000e+00]])\n",
      "actions: [[-0.10191833 -0.07630233  0.00895233  0.23152567 -0.00942753 -0.04486422\n",
      "  -0.02129806  0.13705787  0.16302185  0.14068694 -0.09975909  0.1426678\n",
      "   0.1091101  -0.01740669  0.08120477 -0.03890405  0.07833774  0.09900949\n",
      "   0.11551356  0.24996884]]\n",
      "values: tensor([[-34727.2266]])\n",
      "clipped_actions: [[0.         0.         0.00895233 0.23152567 0.         0.\n",
      "  0.         0.13705787 0.16302185 0.14068694 0.         0.1426678\n",
      "  0.1091101  0.         0.08120477 0.         0.07833774 0.09900949\n",
      "  0.11551356 0.24996884]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   5.]]\n",
      "rewards: [-0.5785842]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.]])\n",
      "actions: [[-0.02578909  0.22881562  0.16167186 -0.0371664  -0.01121426 -0.21401855\n",
      "  -0.05922398 -0.01095986 -0.08390021  0.03830268  0.05070182 -0.02326986\n",
      "   0.05830231  0.14872806  0.22055043 -0.05899027  0.15871102 -0.04481859\n",
      "   0.19187546 -0.06545584]]\n",
      "values: tensor([[-2288.5784]])\n",
      "clipped_actions: [[0.         0.22881562 0.16167186 0.         0.         0.\n",
      "  0.         0.         0.         0.03830268 0.05070182 0.\n",
      "  0.05830231 0.14872806 0.22055043 0.         0.15871102 0.\n",
      "  0.19187546 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.  0. 10. 10.  0.  0.  0.  0. 10. 10.  0.  0. 10. 10.  0.  0. 10. 10.\n",
      "   0.]]\n",
      "rewards: [-0.5785724]\n",
      "\n",
      "\n",
      "Log-Std at step 34824: [-2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2. -2.\n",
      " -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.10371218010783195 ; last reward: -0.5785723924636841\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (64, 20)) of distribution Normal(loc: torch.Size([64, 20]), scale: torch.Size([64, 20])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m logpath \u001b[38;5;241m=\u001b[39m model_name[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m):]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogging at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:217\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m--> 217\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\policies.py:737\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m    735\u001b[0m     latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(pi_features)\n\u001b[0;32m    736\u001b[0m     latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(vf_features)\n\u001b[1;32m--> 737\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[0;32m    739\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\policies.py:694\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[1;34m(self, latent_pi)\u001b[0m\n\u001b[0;32m    691\u001b[0m mean_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_net(latent_pi)\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\distributions.py:164\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.proba_distribution\u001b[1;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    163\u001b[0m action_std \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mones_like(mean_actions) \u001b[38;5;241m*\u001b[39m log_std\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\distributions\\normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (64, 20)) of distribution Normal(loc: torch.Size([64, 20]), scale: torch.Size([64, 20])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "logpath = model_name[len(\"models/\"):]\n",
    "print(f\"logging at {logpath}\")\n",
    "model.learn(total_timesteps = 100000, \n",
    "            progress_bar = False, \n",
    "            tb_log_name = logpath, \n",
    "            callback = callback,\n",
    "            reset_num_timesteps = False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt.plot([k*2048 for k in range(len(log_std_callback.log_stds))], [k for k in log_std_callback.log_stds])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average action log std\")\n",
    "plt.title(model_name.split(\"/\")[-1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M,Q,P,B,Z,D = 10, 0, 5, 5, 1, 0\n",
    "M, Q, P, B, Z, D  = cfg[\"env\"][\"M\"], cfg[\"env\"][\"Q\"], cfg[\"env\"][\"P\"], cfg[\"env\"][\"B\"], cfg[\"env\"][\"Z\"], 0\n",
    "# M,Q,P,B,Z,D = 0, 0, 0, 0, 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = True, \n",
    "               M = M, Q = Q, P = P, B = B, Z = Z, D = D, \n",
    "               action_sample = action_sample, \n",
    "               connections = connections, \n",
    "               tau0 = tau0,\n",
    "               delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub,\n",
    "               sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb,\n",
    "               s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb,\n",
    "               d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d,\n",
    "               betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ['source_blend', 's1', 'j1']),\n",
       " (1, ['source_blend', 's1', 'j2']),\n",
       " (2, ['source_blend', 's1', 'j3']),\n",
       " (3, ['source_blend', 's1', 'j4']),\n",
       " (4, ['source_blend', 's2', 'j1']),\n",
       " (5, ['source_blend', 's2', 'j2']),\n",
       " (6, ['source_blend', 's2', 'j3']),\n",
       " (7, ['source_blend', 's2', 'j4']),\n",
       " (8, ['blend_demand', 'j1', 'p1']),\n",
       " (9, ['blend_demand', 'j1', 'p2']),\n",
       " (10, ['blend_demand', 'j2', 'p1']),\n",
       " (11, ['blend_demand', 'j2', 'p2']),\n",
       " (12, ['blend_demand', 'j3', 'p1']),\n",
       " (13, ['blend_demand', 'j3', 'p2']),\n",
       " (14, ['blend_demand', 'j4', 'p1']),\n",
       " (15, ['blend_demand', 'j4', 'p2']),\n",
       " (16, ['tau', 's1']),\n",
       " (17, ['tau', 's2']),\n",
       " (18, ['delta', 'p1']),\n",
       " (19, ['delta', 'p2'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.mapping_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t1; s1:\t\t\tbought too much (more than supply)\n",
      "s1: b: 0.2\n",
      "[PEN] t1; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0, in_flow_sources: 10.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0.0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 10.0} {'p1': 0.0}\n",
      "    -2.1\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 50.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t2; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0.0 through tank population in s1\n",
      "j1: inv: 10.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 50.0\n",
      "j1: b: 0.2\n",
      "[PEN] t2; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0.0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 10.0} {'j1': 0.0} {'p1': 10.0}\n",
      "    -4.199999999999999\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t3; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0.0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 20.0} {'j1': 0.0} {'p1': 10.0}\n",
      "    -5.199999999999999\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 50.0}}\n",
      "[PEN] t4; s1:\t\t\tbought too much (more than supply)\n",
      "[PEN] t4; p1:\t\t\tsold too much (more than demand)\n",
      "s1: b: 0.4\n",
      "[PEN] t4; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 20.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0.0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 20.0} {'p1': 0.0}\n",
      "    991.6999999999999\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 50.0}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 50.0}}\n",
      "[PEN] t5; p1:\t\t\tsold too much (more than demand)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 20.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 50.0\n",
      "j1: b: 0.4\n",
      "[PEN] t5; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0.0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 0.0} {'p1': 10.0}\n",
      "    1989.6\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 50.0}}\n",
      "[PEN] t6; p1:\t\t\tsold too much (more than demand)\n",
      "s1: b: 0.0\n",
      "[PEN] t6; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 0.0} {'p1': 0.0}\n",
      "    2987.6\n"
     ]
    }
   ],
   "source": [
    "with th.autograd.set_detect_anomaly(True):\n",
    "    obs = env.reset()\n",
    "    obs, obs_dict = obs\n",
    "    for k in range(env.T):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        print(\"\\n\\n   \",reconstruct_dict(action, env.mapping_act))\n",
    "        obs, reward, done, term, _ = env.step(action)\n",
    "        dobs = reconstruct_dict(obs, env.mapping_obs)\n",
    "        print(\"\\n    >>     \",dobs[\"sources\"], dobs[\"blenders\"], dobs[\"demands\"])\n",
    "        print(\"   \" ,reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 (only once per episode)\n",
    "episode_rewards = []\n",
    "obs = env.reset()\n",
    "obs, obs_dict = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 1 Get first action\n",
    "print(env.t)\n",
    "action, _ = model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'s1': 17.46205}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_blend': {'s1': {'j1': 0.0}},\n",
       " 'blend_demand': {'j1': {'p1': 30.307917}},\n",
       " 'tau': {'s1': 8.731916},\n",
       " 'delta': {'p1': 17.08481}}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Visualize action\n",
    "print(env.t)\n",
    "reconstruct_dict(action, env.mapping_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Step once: get 2nd action\n",
    "print(env.t)\n",
    "obs, reward, done, term, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'s1': 26.193966}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# 4 Visualize new state\n",
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blendv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
