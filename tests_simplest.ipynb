{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN policy ?\n",
    "- grid search for HP tuning (OK)\n",
    "- Increasingly difficult Environment\n",
    "- Positive reward for populating increasingly \"deep\" blending tanks ?\n",
    "- RL for chem sched paper (https://arxiv.org/pdf/2203.00636)\n",
    "- Masking (https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html, https://arxiv.org/pdf/2006.14171)\n",
    "    - Adding binary decision variables ?g  \n",
    "    - Requires discrete action space (only integer flows -> treated as categories ?)\n",
    "    - masking: disable incoming flows (resp. outgoing flows) for tanks at UB inv limit (resp. LB inv. limit), disable selling/buying when available = 0\n",
    "    - multiple envs with multiple agents ? (MARL, https://arxiv.org/pdf/2103.01955)\n",
    "        - Predict successive pipelines (\"source > blend\" then \"blend > blend\" (as many as required) then \"blend > demand\")\n",
    "        - Each agent has access to the whole state\n",
    "        - Action mask is derived from the previous agent's actions (0 if inventory at bounds or incoming flow already reserved, else 1)\n",
    "        - https://github.com/Rohan138/marl-baselines3/blob/main/marl_baselines3/independent_ppo.py\n",
    "- Safe RL: (https://proceedings.mlr.press/v119/wachi20a/wachi20a.pdf)\n",
    "    - \"Unsafe state\" ? > Do not enforce constraints strictly, instead opt for early episode termination to show which states are unsafe ? \n",
    "    - Implementations:\n",
    "        - https://pypi.org/project/fast-safe-rl/#description (Policy optimizers)\n",
    "        - https://github.com/PKU-Alignment/safety-gymnasium/tree/main/safety_gymnasium (environments; \"cost\" ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try other learning rates/CNN policies\n",
    "2. Implement Masking with single agent\n",
    "3. Try other ways to tell the model what are illegal/unsafe states (safe RL)\n",
    "4. Try multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masking: Discretization of action space is too slow/might not work -> Need to implement masking for continuous action space\n",
    "- Recurrent policy makes the most sense ? (window of demand forecasts)\n",
    "- https://www.reddit.com/r/reinforcementlearning/comments/17l5b47/invalid_action_masking_when_action_space_is/\n",
    "    - Suggestion of autoregressive model for having constraints respected: one predicted action is input to a second model\n",
    "    - Suggestion of editing the distribution in such a way that the constraint is respected\n",
    "- https://www.sciencedirect.com/science/article/pii/S0098135420301599\n",
    "    - Choice of ELU activation ?\n",
    "    - Choice of NN size ?\n",
    "    - \"The feature engineering in the net inventory means the network does not have to learn these relationships itself, which did help speed training.\" ?\n",
    "- Simplify the problem (remove tanks 5 to 8), find the optimal solution with Gurobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove all constraints except in/out\n",
    "- https://arxiv.org/pdf/1711.11157\n",
    "- https://arxiv.org/pdf/2111.01564\n",
    "- Softmax with large coef to produce action mask\n",
    "- Graph convolution NN instead of RNN ?\n",
    "    - https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "    - Graph rep. learning - William L Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPG\n",
    "- Softmax\n",
    "- ~~Remove non-selling rewards~~\n",
    "- MultiplexNet\n",
    "- Why softmax doesn't work ? -> gradient doesn't compute properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finalize adjustment of flows\n",
    "- Add more difficulty (bigger env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "import json\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import *\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
    "\n",
    "from envs import BlendEnv, flatten_and_track_mappings, reconstruct_dict\n",
    "from models import *\n",
    "from math import exp, log\n",
    "import yaml\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( Regexp for Tensorboard coloring )\n",
    "\n",
    "(1\\\\|2\\\\|3\\\\|4\\\\|5\\\\|6\\\\|7\\\\|8\\\\|9\\\\|10\\\\|11\\\\|12\\\\|13\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/12.yaml\", \"r\") as f:\n",
    "    s = \"\".join(f.readlines())\n",
    "    cfg = yaml.load(s, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](simplest.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"clipped_std\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_std\n",
    "elif cfg[\"custom_softmax\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_softmax\n",
    "elif cfg[\"policytype\"] == \"MLP\":\n",
    "    policytype = \"MlpPolicy\"\n",
    "elif cfg[\"policytype\"] == \"MLPtanh\":\n",
    "    policytype = CustomMLP_ACP_simplest_tanh\n",
    "    \n",
    "if cfg[\"optimizer\"] == \"PPO\":\n",
    "    optimizer_cls = PPO\n",
    "elif cfg[\"optimizer\"] == \"DDPG\":\n",
    "    optimizer_cls = DDPG\n",
    "\n",
    "if cfg[\"model\"][\"act_fn\"] == \"ReLU\":\n",
    "    act_cls = th.nn.ReLU\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"tanh\":\n",
    "    act_cls = th.nn.Tanh\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"sigmoid\":\n",
    "    act_cls = th.nn.Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = {\n",
    "    \"source_blend\": {\"s1\": [\"j1\"]},\n",
    "    \"blend_blend\": {\"j1\": []},\n",
    "    \"blend_demand\": {\"j1\": [\"p1\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sample = {\n",
    "    'source_blend':{'s1': {'j1':1}},\n",
    "    'blend_blend':{},\n",
    "    'blend_demand':{'j1': {'p1':1}},\n",
    "    \"tau\": {\"s1\": 10},\n",
    "    \"delta\": {\"p1\": 0}\n",
    "}\n",
    "action_sample_flat, mapp = flatten_and_track_mappings(action_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau0   = {'s1': [10, 10, 10, 0, 0, 0]}\n",
    "delta0 = {'p1': [0, 0, 0, 10, 10, 10]}\n",
    "sigma = {\"s1\":{\"q1\": 0.06}} # Source concentrations\n",
    "sigma_ub = {\"p1\":{\"q1\": 0.16}} # Demand concentrations UBs/LBs\n",
    "sigma_lb = {\"p1\":{\"q1\": 0}}\n",
    "s_inv_lb = {'s1': 0}\n",
    "s_inv_ub = {'s1': 999}\n",
    "d_inv_lb = {'p1': 0}\n",
    "d_inv_ub = {'p1': 999}\n",
    "betaT_d = {'p1': 1} # Price of sold products\n",
    "betaT_s = {'s1': cfg[\"env\"][\"product_cost\"]} # Cost of bought products\n",
    "b_inv_ub = {\"j1\": 30} \n",
    "b_inv_lb = {j:0 for j in b_inv_ub.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = False, \n",
    "               D = cfg[\"env\"][\"D\"], \n",
    "               Q = cfg[\"env\"][\"Q\"], \n",
    "               P = cfg[\"env\"][\"P\"], \n",
    "               B = cfg[\"env\"][\"B\"], \n",
    "               Z = cfg[\"env\"][\"Z\"], \n",
    "               M = cfg[\"env\"][\"M\"],\n",
    "               reg = cfg[\"env\"][\"reg\"],\n",
    "               reg_lambda = cfg[\"env\"][\"reg_lambda\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"],\n",
    "               alpha = cfg[\"env\"][\"alpha\"],\n",
    "               beta = cfg[\"env\"][\"beta\"],\n",
    "               connections = connections,\n",
    "               action_sample = action_sample,\n",
    "               tau0 = tau0,delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "\n",
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, \n",
    "                   norm_obs=cfg[\"obs_normalizer\"], \n",
    "                   norm_reward=cfg[\"reward_normalizer\"])\n",
    "env = VecCheckNan(env, raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"], \n",
    "                   vf = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"])],\n",
    "    activation_fn = act_cls,\n",
    "    log_std_init = cfg[\"model\"][\"log_std_init\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MlpPolicy'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policytype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpPolicy\n"
     ]
    }
   ],
   "source": [
    "print(policytype)\n",
    "\n",
    "if optimizer_cls == DDPG:\n",
    "    kwa = dict(policy = policytype, \n",
    "                env = env,\n",
    "                tensorboard_log = \"./logs\",\n",
    "                learning_rate = cfg[\"model\"][\"lr\"])\n",
    "\n",
    "else:\n",
    "    kwa = dict(policy = policytype, \n",
    "                env = env,\n",
    "                tensorboard_log = \"./logs\",\n",
    "                clip_range = cfg[\"model\"][\"clip_range\"],\n",
    "                learning_rate = cfg[\"model\"][\"lr\"],\n",
    "                ent_coef = cfg[\"model\"][\"ent_coef\"],\n",
    "                use_sde = cfg[\"model\"][\"use_sde\"],\n",
    "                policy_kwargs = policy_kwargs)\n",
    "\n",
    "model = optimizer_cls(**kwa)\n",
    "\n",
    "if cfg[\"starting_point\"]:\n",
    "    model.set_parameters(cfg[\"starting_point\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "if type(model.policy) == CustomRNN_ACP:\n",
    "    policytype = \"CRNN\"\n",
    "elif type(model.policy) == CustomMLP_ACP_simplest_std:\n",
    "    policytype = \"CMLP\"\n",
    "else:\n",
    "    policytype = \"MLP\"\n",
    "    \n",
    "entcoef = str(model.ent_coef) if type(model) == PPO else \"\"\n",
    "cliprange = str(model.clip_range(0)) if type(model) == PPO else \"\"\n",
    "model_name = f\"models/simplest/{cfg['id']}/{cfg['id']}_{datetime.datetime.now().strftime('%m%d-%H%M')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallbackPPO(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_stds = []\n",
    "        self.total_rewards = []\n",
    "        self.update1 = True\n",
    "        self.print_flag = False\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []\n",
    "        \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        self.logger.record('train/learning_rate', self.model.learning_rate)\n",
    "        self.logger.record('train/clip_range', self.model.clip_range(0))\n",
    "        \n",
    "        self.stds = th.exp(self.model.policy.log_std).mean().item()\n",
    "        \n",
    "        if self.stds > 50:\n",
    "            print(\"clipping log-stds\")\n",
    "            self.model.policy.log_std = nn.Parameter( 2*th.ones(self.model.policy.log_std.shape, requires_grad=True) )\n",
    "        \n",
    "        self.logger.record(\"train/std\", th.exp(self.model.policy.log_std).mean().item())\n",
    "        self.logger.record(\"penalties/in_out\", sum(self.pen_M)/len(self.pen_M))\n",
    "        self.logger.record(\"penalties/buysell_bounds\", sum(self.pen_B)/len(self.pen_B))\n",
    "        self.logger.record(\"penalties/tank_bounds\", sum(self.pen_P)/len(self.pen_P))\n",
    "        self.logger.record(\"penalties/regterm\", sum(self.pen_reg)/len(self.pen_reg))\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        log_std: th.Tensor = self.model.policy.log_std\n",
    "        # print(self.locals)\n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        \n",
    "        if self.locals[\"dones\"][0]: # record info at each episode end\n",
    "            self.pen_M.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"M\"])\n",
    "            self.pen_B.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"B\"])\n",
    "            self.pen_P.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"P\"])\n",
    "            self.pen_reg.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"reg\"])\n",
    "            \n",
    "            self.log_stds.append(log_std.mean().item())\n",
    "            self.total_rewards.append(self.locals['rewards'][0])\n",
    "            \n",
    "            if self.locals['rewards'][0] > 200 and self.update1:\n",
    "                self.model.learning_rate = 1e2\n",
    "                self.model.clip_range = 5e-2\n",
    "                self.update1 = False\n",
    "        \n",
    "        if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "            self.print_flag = True\n",
    "            \n",
    "        if self.print_flag:\n",
    "            print(\"\\nt:\", t)\n",
    "            if np.isnan(self.locals['rewards'][0]) or np.isinf(self.locals['rewards'][0]):\n",
    "                print(f\"is invalid reward {self.locals['rewards'][0]}\")\n",
    "            for i in ['obs_tensor', 'actions', 'values', 'clipped_actions', 'new_obs', 'rewards']:\n",
    "                if i in self.locals:\n",
    "                    print(f\"{i}: \" + str(self.locals[i]))\n",
    "            if t == 6:\n",
    "                self.print_flag = False\n",
    "                print(f\"\\n\\nLog-Std at step {self.num_timesteps}: {log_std.detach().numpy()}\")\n",
    "                # print(f\"\\nAvg rewards over the last 100 episodes:{sum(self.total_rewards[-100:])/100} ; last reward: {self.total_rewards[-1]}\")\n",
    "                print(\"\\n\\n\\n\\n\\n\")\n",
    "                \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallbackDDPG(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.total_rewards = []\n",
    "        self.update1 = True\n",
    "        self.print_flag = False\n",
    "        \n",
    "        self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []\n",
    "        \n",
    "    def _on_rollout_end(self) -> None: ...\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # print(self.locals)\n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        # print(self.locals[\"infos\"][0][\"pen_tracker\"])\n",
    "        \n",
    "        if self.locals[\"dones\"][0]: # record info at each episode end\n",
    "            self.pen_M.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"M\"])\n",
    "            self.pen_B.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"B\"])\n",
    "            self.pen_P.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"P\"])\n",
    "            self.pen_reg.append(self.locals[\"infos\"][0][\"pen_tracker\"][\"reg\"])\n",
    "            \n",
    "            self.total_rewards.append(self.locals['rewards'][0])\n",
    "            \n",
    "            # if self.locals['rewards'][0] > 200 and self.update1:\n",
    "            #     self.model.learning_rate = 1e2\n",
    "            #     self.update1 = False\n",
    "        \n",
    "        if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "            self.print_flag = True\n",
    "            \n",
    "        if self.print_flag:\n",
    "            print(\"\\nt:\", t)\n",
    "            if np.isnan(self.locals['rewards'][0]) or np.isinf(self.locals['rewards'][0]):\n",
    "                print(f\"is invalid reward {self.locals['rewards'][0]}\")\n",
    "            for i in ['obs_tensor', 'actions', 'values', 'new_obs', 'rewards']:\n",
    "                if i in self.locals:\n",
    "                    print(f\"{i}: \" + str(self.locals[i]))\n",
    "            if t == 6:\n",
    "                self.print_flag = False\n",
    "                # print(f\"\\nAvg rewards over the last 100 episodes:{sum(self.total_rewards[-100:])/100} ; last reward: {self.total_rewards[-1]}\")\n",
    "                \n",
    "                self.logger.record('train/learning_rate', self.model.learning_rate)\n",
    "                self.logger.record(\"penalties/in_out\", sum(self.pen_M)/len(self.pen_M))\n",
    "                self.logger.record(\"penalties/buysell_bounds\", sum(self.pen_B)/len(self.pen_B))\n",
    "                self.logger.record(\"penalties/tank_bounds\", sum(self.pen_P)/len(self.pen_P))\n",
    "                self.logger.record(\"penalties/regterm\", sum(self.pen_reg)/len(self.pen_reg))\n",
    "        \n",
    "                self.pen_M, self.pen_B, self.pen_P, self.pen_reg = [], [], [], []   \n",
    "                \n",
    "                print(\"\\n\\n\\n\\n\\n\")\n",
    "                \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/simplest/12/12_0716-1703'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_callback = CustomLoggingCallbackPPO() if optimizer_cls == PPO else CustomLoggingCallbackDDPG()\n",
    "callback = CallbackList([log_callback])\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging at simplest/12/12_0716-1703\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 0.1070335  -0.22346415  0.10184398 -0.16763408]]\n",
      "values: tensor([[-4.2488]])\n",
      "clipped_actions: [[0.1070335  0.         0.10184398 0.        ]]\n",
      "new_obs: [[ 0.          0.10184398  0.          0.06       10.          0.\n",
      "  10.          0.          0.         10.          0.         10.\n",
      "   0.         10.          0.          0.          1.        ]]\n",
      "rewards: [-10.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000,  0.1018,  0.0000,  0.0600, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 0.15069139 -0.07826404  0.03532644 -0.01408055]]\n",
      "values: tensor([[-2.8299]])\n",
      "clipped_actions: [[0.15069139 0.         0.03532644 0.        ]]\n",
      "new_obs: [[ 0.          0.13717043  0.          0.07545222 10.          0.\n",
      "   0.         10.          0.         10.          0.         10.\n",
      "   0.          0.          0.          0.          2.        ]]\n",
      "rewards: [-2.009761]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000,  0.1372,  0.0000,  0.0755, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-0.1327326   0.09499184 -0.008776    0.09206419]]\n",
      "values: tensor([[-3.8868]])\n",
      "clipped_actions: [[0.         0.09499184 0.         0.09206419]]\n",
      "new_obs: [[ 0.          0.04217859  0.09499184 -0.0944763   0.         10.\n",
      "   0.         10.          0.         10.          0.          0.\n",
      "   0.          0.          0.          0.          3.        ]]\n",
      "rewards: [-1.4628286]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000,  0.0422,  0.0950, -0.0945,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 0.06523844 -0.1394774   0.12024481  0.05996776]]\n",
      "values: tensor([[-5.2055]])\n",
      "clipped_actions: [[0.06523844 0.         0.12024481 0.05996776]]\n",
      "new_obs: [[ 0.          0.04217859  0.03502408 -0.0944763   0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [-1.4898405]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  0.0422,  0.0350, -0.0945,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[-0.10769638 -0.11579882 -0.10470841  0.09324495]]\n",
      "values: tensor([[-3.6127]])\n",
      "clipped_actions: [[0.         0.         0.         0.09324495]]\n",
      "new_obs: [[ 0.          0.04217859  0.         -0.0944763   0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [-1.1591612]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  0.0422,  0.0000, -0.0945,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[-0.08316633  0.17669684  0.14060293 -0.03856498]]\n",
      "values: tensor([[-2.4065]])\n",
      "clipped_actions: [[0.         0.17669684 0.14060293 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.98603547]\n",
      "\n",
      "\n",
      "Log-Std at step 6: [-2. -2. -2. -2.]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  7.75442  -14.096987  13.079884 -14.029251]]\n",
      "values: tensor([[-2.6528]])\n",
      "clipped_actions: [[ 7.75442   0.       13.079884  0.      ]]\n",
      "new_obs: [[ 2.2455802  7.75442    0.         0.06      10.         0.\n",
      "  10.         0.         0.        10.         0.        10.\n",
      "   0.        10.         0.         0.         1.       ]]\n",
      "rewards: [0.21654348]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 2.2456,  7.7544,  0.0000,  0.0600, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[  5.8302526 -12.3188505  11.091751  -12.5535555]]\n",
      "values: tensor([[-0.8021]])\n",
      "clipped_actions: [[ 5.8302526  0.        11.091751   0.       ]]\n",
      "new_obs: [[ 6.4153275  13.584673    0.          0.08575072 10.          0.\n",
      "   0.         10.          0.         10.          0.         10.\n",
      "   0.          0.          0.          0.          2.        ]]\n",
      "rewards: [0.40022662]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 6.4153, 13.5847,  0.0000,  0.0858, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[  5.255251 -10.601439   9.474067 -10.861264]]\n",
      "values: tensor([[-0.8815]])\n",
      "clipped_actions: [[5.255251 0.       9.474067 0.      ]]\n",
      "new_obs: [[10.634144   18.839924    0.          0.10248726  0.         10.\n",
      "   0.         10.          0.         10.          0.          0.\n",
      "   0.          0.          0.          0.          3.        ]]\n",
      "rewards: [0.6494538]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.6341, 18.8399,  0.0000,  0.1025,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 6.202622  -9.469992   8.9121065 -9.220478 ]]\n",
      "values: tensor([[-1.1126]])\n",
      "clipped_actions: [[6.202622  0.        8.9121065 0.       ]]\n",
      "new_obs: [[ 4.4315214  25.042545    0.          0.11734826  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.77333283]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 4.4315, 25.0425,  0.0000,  0.1173,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  5.791086  -9.700588   8.928468 -10.0393  ]]\n",
      "values: tensor([[-1.2309]])\n",
      "clipped_actions: [[5.791086 0.       8.928468 0.      ]]\n",
      "new_obs: [[ 0.         29.474068    0.          0.12636946  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.7515805]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 29.4741,  0.0000,  0.1264,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 5.976748  -9.539026   9.19109   -9.6726885]]\n",
      "values: tensor([[-1.1897]])\n",
      "clipped_actions: [[5.976748 0.       9.19109  0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.5804258]\n",
      "\n",
      "\n",
      "Log-Std at step 2058: [-1.5661771 -1.7448113 -1.4572123 -1.2536227]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-52.35478   18.226217  63.946976 -56.291718]]\n",
      "values: tensor([[1.5654]])\n",
      "clipped_actions: [[ 0.       18.226217 63.946976  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-59.410725  19.803917  78.44547  -61.542843]]\n",
      "values: tensor([[1.6267]])\n",
      "clipped_actions: [[ 0.       19.803917 78.44547   0.      ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-87.74983   25.186035 130.66757  -97.15067 ]]\n",
      "values: tensor([[1.2630]])\n",
      "clipped_actions: [[  0.        25.186035 130.66757    0.      ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-172.63005   49.24202  271.39078 -194.61841]]\n",
      "values: tensor([[0.0607]])\n",
      "clipped_actions: [[  0.       49.24202 271.39078   0.     ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.09623846]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-170.542      48.431072  266.745    -192.05977 ]]\n",
      "values: tensor([[-0.1589]])\n",
      "clipped_actions: [[  0.        48.431072 266.745      0.      ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.19249548]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-145.0427     41.444454  221.74297  -160.73582 ]]\n",
      "values: tensor([[0.7858]])\n",
      "clipped_actions: [[  0.        41.444454 221.74297    0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.28875902]\n",
      "\n",
      "\n",
      "Log-Std at step 4104: [-0.65390396 -0.56636775 -0.00508276  0.6033018 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-1901.8319  3544.711   4276.7065 -1562.1194]]\n",
      "values: tensor([[-0.3168]])\n",
      "clipped_actions: [[  0. 500. 500.   0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-1905.8221  3559.1406  4298.4854 -1565.0048]]\n",
      "values: tensor([[-0.4194]])\n",
      "clipped_actions: [[  0. 500. 500.   0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-1419.6721  2657.4084  3223.2021 -1168.0265]]\n",
      "values: tensor([[-0.5167]])\n",
      "clipped_actions: [[  0. 500. 500.   0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-1079.7931  2049.5845  2482.9585  -891.3306]]\n",
      "values: tensor([[-0.5534]])\n",
      "clipped_actions: [[  0. 500. 500.   0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.11453874]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-1049.3368  2003.7598  2415.963   -905.1375]]\n",
      "values: tensor([[-0.4655]])\n",
      "clipped_actions: [[  0. 500. 500.   0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.22909294]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-1128.5858  2156.9614  2608.705   -958.9241]]\n",
      "values: tensor([[-0.3197]])\n",
      "clipped_actions: [[  0. 500. 500.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.34365153]\n",
      "\n",
      "\n",
      "Log-Std at step 6150: [0.6386171 1.0080895 1.7259779 2.3610182]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[176.59471 367.13043  33.21217 130.45038]]\n",
      "values: tensor([[-0.6672]])\n",
      "clipped_actions: [[176.59471 367.13043  33.21217 130.45038]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.3244654]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 45.205193 109.08837   13.540206 103.45375 ]]\n",
      "values: tensor([[-0.5470]])\n",
      "clipped_actions: [[ 45.205193 109.08837   13.540206 103.45375 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.64893264]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 36.347664  29.210892 -33.55018    8.212799]]\n",
      "values: tensor([[-0.3063]])\n",
      "clipped_actions: [[36.347664 29.210892  0.        8.212799]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.8434954]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  9.890876  10.778221  57.194363 -14.866359]]\n",
      "values: tensor([[-0.1609]])\n",
      "clipped_actions: [[ 9.890876 10.778221 57.194363  0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-1.0377051]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 11.881004   -4.5875497   2.7888913 -22.23891  ]]\n",
      "values: tensor([[-0.1809]])\n",
      "clipped_actions: [[11.881004   0.         2.7888913  0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-1.1663709]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-0.72127724 20.230846   41.040752    1.4306438 ]]\n",
      "values: tensor([[-0.0295]])\n",
      "clipped_actions: [[ 0.        20.230846  41.040752   1.4306438]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-1.3585384]\n",
      "\n",
      "\n",
      "Log-Std at step 8202: [2.0280118 2.520362  3.3606405 4.01224  ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -68.35649  -137.38368    18.484108  -54.12342 ]]\n",
      "values: tensor([[-3.2874]])\n",
      "clipped_actions: [[ 0.        0.       18.484108  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.04073315]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -32.867718 -108.31815    55.93394  -109.434944]]\n",
      "values: tensor([[-3.1171]])\n",
      "clipped_actions: [[ 0.       0.      55.93394  0.     ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.08146954]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-19.784698 -66.945145  -0.703084 -52.069485]]\n",
      "values: tensor([[-1.6238]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[20.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.08147249]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -0.07151687 -21.841787     2.2145505  -25.454737  ]]\n",
      "values: tensor([[-1.4065]])\n",
      "clipped_actions: [[0.        0.        2.2145505 0.       ]]\n",
      "new_obs: [[20.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.04073764]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-5.589947  6.272924 41.882137 28.64376 ]]\n",
      "values: tensor([[-0.8893]])\n",
      "clipped_actions: [[ 0.        6.272924 41.882137 28.64376 ]]\n",
      "new_obs: [[20.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.12221783]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[  1.7130643  17.632095   38.780724  -46.35979  ]]\n",
      "values: tensor([[-0.4673]])\n",
      "clipped_actions: [[ 1.7130643 17.632095  38.780724   0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.24444722]\n",
      "\n",
      "\n",
      "Log-Std at step 10248: [1.9928051 2.4813046 3.343083  4.0035176]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -902.1851 -3141.7246 -1156.2059  -787.2712]]\n",
      "values: tensor([[-0.4624]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -521.9623 -1794.0693  -638.1861  -483.6057]]\n",
      "values: tensor([[-0.5074]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-153.95824 -584.4597  -234.89543 -125.35724]]\n",
      "values: tensor([[-0.6007]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-9.85672   -1.8509309 35.649414  43.7892   ]]\n",
      "values: tensor([[-0.6345]])\n",
      "clipped_actions: [[ 0.        0.       35.649414 43.7892  ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.12794466]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -7.0527887  10.2624855  30.283083  112.80308  ]]\n",
      "values: tensor([[-0.4439]])\n",
      "clipped_actions: [[  0.         10.2624855  30.283083  112.80308  ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.29854935]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-2.1821423 -3.652916  22.316832  68.45197  ]]\n",
      "values: tensor([[-0.4120]])\n",
      "clipped_actions: [[ 0.        0.       22.316832 68.45197 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.42651036]\n",
      "\n",
      "\n",
      "Log-Std at step 12294: [1.9896903 2.523072  3.3762033 4.010169 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -4549.025 -26687.14  -13139.289   1982.14 ]]\n",
      "values: tensor([[-0.6317]])\n",
      "clipped_actions: [[  0.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.04580427]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -3057.8792 -17874.29    -8815.944    1321.1577]]\n",
      "values: tensor([[-0.6487]])\n",
      "clipped_actions: [[  0.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.09161168]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-1006.6523  -5862.5635  -2912.8933    388.27676]]\n",
      "values: tensor([[-0.6689]])\n",
      "clipped_actions: [[  0.        0.        0.      388.27676]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.13742231]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -2.5247183   5.86219   -13.908133   19.89202  ]]\n",
      "values: tensor([[-0.6773]])\n",
      "clipped_actions: [[ 0.       5.86219  0.      19.89202]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.27485344]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[  3.8934479  13.018567  -58.040047   43.787167 ]]\n",
      "values: tensor([[-0.5631]])\n",
      "clipped_actions: [[ 3.8934479 13.018567   0.        43.787167 ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.4580964]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[  5.178774     4.15524      0.26914412 -53.940125  ]]\n",
      "values: tensor([[-0.3492]])\n",
      "clipped_actions: [[5.178774   4.15524    0.26914412 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.59550995]\n",
      "\n",
      "\n",
      "Log-Std at step 14346: [1.9781029 2.521052  3.389814  3.9549325]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  563.55774 -8685.456   -3669.1008   1089.4441 ]]\n",
      "values: tensor([[-1.2670]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.09610903]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  195.80772 -3061.7263  -1340.1023    275.90125]]\n",
      "values: tensor([[-1.2655]])\n",
      "clipped_actions: [[195.80772   0.        0.      275.90125]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.19222394]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  40.02784 -579.96576 -227.24342  115.57495]]\n",
      "values: tensor([[-1.1983]])\n",
      "clipped_actions: [[ 40.02784   0.        0.      115.57495]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.2883441]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  2.4522796    0.03561985 -14.709066     3.6123962 ]]\n",
      "values: tensor([[-1.0612]])\n",
      "clipped_actions: [[2.4522796  0.03561985 0.         3.6123962 ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.43252307]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -4.0415454   4.658766   33.026394  -33.21308  ]]\n",
      "values: tensor([[-0.8352]])\n",
      "clipped_actions: [[ 0.        4.658766 33.026394  0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.5286319]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[  7.177179    2.7311602 -35.906322  -12.957687 ]]\n",
      "values: tensor([[-0.4821]])\n",
      "clipped_actions: [[7.177179  2.7311602 0.        0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.624703]\n",
      "\n",
      "\n",
      "Log-Std at step 16392: [1.972239  2.5216336 3.3807144 3.8911252]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  8131.873  -38605.66   -10504.89     5991.4414]]\n",
      "values: tensor([[-1.8433]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.09746597]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  2978.8267 -14197.08    -3821.9307   2248.4727]]\n",
      "values: tensor([[-1.7993]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.19493718]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  703.0924  -3329.9722   -906.5351    545.34186]]\n",
      "values: tensor([[-1.6925]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.29241338]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -9.579694  10.873627  55.27967  -66.82271 ]]\n",
      "values: tensor([[-1.4388]])\n",
      "clipped_actions: [[ 0.       10.873627 55.27967   0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.38989148]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -6.6990933  -5.6078253  -5.3154383 -71.23633  ]]\n",
      "values: tensor([[-1.0799]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.3898922]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-14.7733345  26.676445    9.584051  -29.903202 ]]\n",
      "values: tensor([[-0.5955]])\n",
      "clipped_actions: [[ 0.       26.676445  9.584051  0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.48735112]\n",
      "\n",
      "\n",
      "Log-Std at step 18438: [1.965191  2.5175948 3.370551  3.8387244]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 37823.773  -99403.836   -6378.5186   4525.2095]]\n",
      "values: tensor([[-1.9970]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.09902136]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 17711.537  -46567.574   -2987.3086   2079.3362]]\n",
      "values: tensor([[-1.9124]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.19804744]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  6210.366   -16332.684    -1001.07184    747.0297 ]]\n",
      "values: tensor([[-1.7554]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.29707825]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -0.27397162   5.098835    49.699623   -51.79454   ]]\n",
      "values: tensor([[-1.4162]])\n",
      "clipped_actions: [[ 0.        5.098835 49.699623  0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.3961112]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[  3.8004239 -15.393314  -17.433746   67.08267  ]]\n",
      "values: tensor([[-1.0638]])\n",
      "clipped_actions: [[ 3.8004239  0.         0.        67.08267  ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.5446508]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[  0.8845786  14.614836   24.488258  -13.401142 ]]\n",
      "values: tensor([[-0.5944]])\n",
      "clipped_actions: [[ 0.8845786 14.614836  24.488258   0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.6931567]\n",
      "\n",
      "\n",
      "Log-Std at step 20490: [1.9752177 2.5051079 3.3667579 3.7567472]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  66249.04  -108715.18   -33969.297   34550.285]]\n",
      "values: tensor([[-2.0346]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.10067648]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 21805.998 -35804.81  -11165.164  11363.004]]\n",
      "values: tensor([[-1.9238]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.20135726]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 2870.3218 -4711.8887 -1463.4856  1573.0929]]\n",
      "values: tensor([[-1.7633]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.3020425]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  2.685162  10.368692 -11.153481 -40.370667]]\n",
      "values: tensor([[-1.5106]])\n",
      "clipped_actions: [[ 2.685162 10.368692  0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.40273]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 12.167133    7.2076807 -22.15808   -35.29687  ]]\n",
      "values: tensor([[-1.1064]])\n",
      "clipped_actions: [[12.167133   7.2076807  0.         0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.5034124]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -8.323769   3.482863 -14.634835 -16.58346 ]]\n",
      "values: tensor([[-0.5869]])\n",
      "clipped_actions: [[0.       3.482863 0.       0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.5537368]\n",
      "\n",
      "\n",
      "Log-Std at step 22536: [1.9873629 2.5130794 3.3564606 3.7125077]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 18578.97  -32806.023 -14581.259  16156.537]]\n",
      "values: tensor([[-2.0093]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.10240965]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 2882.5627 -5092.214  -2216.5554  2507.1733]]\n",
      "values: tensor([[-1.9118]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.20482326]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  6.0379033   -0.41060424  -2.480434   -16.874176  ]]\n",
      "values: tensor([[-1.7028]])\n",
      "clipped_actions: [[6.0379033 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.25603426]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  0.7670549  -8.234339  -34.835217  -48.803402 ]]\n",
      "values: tensor([[-1.4547]])\n",
      "clipped_actions: [[0.7670549 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.30724669]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -7.4668674  -9.365717  -25.872616  -22.241455 ]]\n",
      "values: tensor([[-1.0483]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.30725047]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[  2.3557527 -25.501982   41.795685   -1.3383102]]\n",
      "values: tensor([[-0.5749]])\n",
      "clipped_actions: [[ 2.3557527  0.        41.795685   0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.4096669]\n",
      "\n",
      "\n",
      "Log-Std at step 24582: [1.97906   2.4952774 3.3419583 3.6735067]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  4351.735 -12163.19   -9784.973   5423.318]]\n",
      "values: tensor([[-1.5290]])\n",
      "clipped_actions: [[500.   0.   0. 500.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.10306702]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 155.67183 -437.51712 -323.67374  240.19856]]\n",
      "values: tensor([[-1.4680]])\n",
      "clipped_actions: [[155.67183   0.        0.      240.19856]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.20613767]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -7.908967   -2.2895389  -1.1850367 -11.930128 ]]\n",
      "values: tensor([[-1.2809]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.20614153]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -5.2626657  -5.008509    7.208871  -38.45814  ]]\n",
      "values: tensor([[-1.1083]])\n",
      "clipped_actions: [[0.       0.       7.208871 0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.25768155]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -7.3670077  10.512826  -43.34044   -18.85074  ]]\n",
      "values: tensor([[-0.8804]])\n",
      "clipped_actions: [[ 0.       10.512826  0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.30922216]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -8.126319   -1.6359979 -14.037899  -29.580502 ]]\n",
      "values: tensor([[-0.6038]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.30922422]\n",
      "\n",
      "\n",
      "Log-Std at step 26634: [1.9692934 2.4937942 3.3272429 3.6617708]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  332.79425 -3095.5554  -2975.345     147.41408]]\n",
      "values: tensor([[-1.6953]])\n",
      "clipped_actions: [[332.79425   0.        0.      147.41408]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.10478465]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-15.628857 -17.429659  -8.661416 -30.132404]]\n",
      "values: tensor([[-1.6258]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.10478622]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  0.5309696 -24.858486  -16.843227  -27.617931 ]]\n",
      "values: tensor([[-1.4349]])\n",
      "clipped_actions: [[0.5309696 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.15718195]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -9.330499    2.0409632   4.7577877 -83.120926 ]]\n",
      "values: tensor([[-1.2543]])\n",
      "clipped_actions: [[0.        2.0409632 4.7577877 0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.26197448]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[  2.7467299 -14.285471  -45.009476   -0.9601288]]\n",
      "values: tensor([[-0.8856]])\n",
      "clipped_actions: [[2.7467299 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.31437418]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -2.2936819  -2.1098928  45.186523  -29.426985 ]]\n",
      "values: tensor([[-0.6049]])\n",
      "clipped_actions: [[ 0.        0.       45.186523  0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.36677313]\n",
      "\n",
      "\n",
      "Log-Std at step 28680: [1.9806095 2.4605372 3.3004084 3.6352227]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  911.3501 -4352.406  -4070.5686 -1188.4445]]\n",
      "values: tensor([[-1.2770]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05211506]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -12.939264   17.070482   22.432074 -127.254944]]\n",
      "values: tensor([[-1.1396]])\n",
      "clipped_actions: [[ 0.       17.070482 22.432074  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.05211572]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 2.3216841 -5.9806437 14.752426   9.667683 ]]\n",
      "values: tensor([[-1.1783]])\n",
      "clipped_actions: [[ 2.3216841  0.        14.752426   9.667683 ]]\n",
      "new_obs: [[17.678316   2.3216841  0.         0.06       0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [-0.02791683]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[17.6783,  2.3217,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[-10.210096   2.811008 -28.58738  -62.88528 ]]\n",
      "values: tensor([[-0.7451]])\n",
      "clipped_actions: [[0.       2.811008 0.       0.      ]]\n",
      "new_obs: [[17.678316   0.         2.3216841  0.         0.        10.\n",
      "   0.        10.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         4.       ]]\n",
      "rewards: [0.01676523]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[17.6783,  0.0000,  2.3217,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  55.900017 -333.18918  -278.71173   -83.91751 ]]\n",
      "values: tensor([[-2.0044]])\n",
      "clipped_actions: [[55.900017  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.        17.678316   2.3216841  0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.3331868]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 17.6783,  2.3217,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ -2.7540543 -31.464952   -2.307982  -12.673843 ]]\n",
      "values: tensor([[-0.7451]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.33318526]\n",
      "\n",
      "\n",
      "Log-Std at step 30726: [1.9556377 2.4618223 3.2858586 3.584376 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  3251.5586 -11099.052   -5768.7886  -8102.1826]]\n",
      "values: tensor([[-0.7753]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05278434]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -4.697107   -7.0294795  -9.270694  -75.9825   ]]\n",
      "values: tensor([[-0.7753]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.05278497]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -2.594761  12.292868  10.427096 -17.766165]]\n",
      "values: tensor([[-0.7753]])\n",
      "clipped_actions: [[ 0.       12.292868 10.427096  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.05278564]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  11.832963   -20.218033    -5.6291513 -102.49956  ]]\n",
      "values: tensor([[-0.3924]])\n",
      "clipped_actions: [[11.832963  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.   10.    0.    0.06  0.   10.    0.   10.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    4.  ]]\n",
      "rewards: [0.10557245]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 10.0000,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 18.922907    -0.36145377 -17.273323   -29.638985  ]]\n",
      "values: tensor([[-0.1512]])\n",
      "clipped_actions: [[18.922907  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.   10.    0.    0.06  0.   10.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    5.  ]]\n",
      "rewards: [0.05278677]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 10.0000,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[  2.9039106 -11.425274    2.0704951 -79.545296 ]]\n",
      "values: tensor([[0.0239]])\n",
      "clipped_actions: [[2.9039106 0.        2.0704951 0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.05278735]\n",
      "\n",
      "\n",
      "Log-Std at step 32778: [1.9431503 2.4490073 3.2795427 3.5596123]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 17077.84  -45610.89  -17416.4    -9368.538]]\n",
      "values: tensor([[-0.7326]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05336931]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  867.9654  -2318.9758   -906.82623  -509.48154]]\n",
      "values: tensor([[-0.7326]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.1067399]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-11.884863  -8.659753 -29.378813 -61.953968]]\n",
      "values: tensor([[-0.7326]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.1067413]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  0.07459569 -19.232658    -4.0219593  -61.033894  ]]\n",
      "values: tensor([[-0.8876]])\n",
      "clipped_actions: [[0.07459569 0.         0.         0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.16011421]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -5.131798   6.215059 -30.73417  -53.84129 ]]\n",
      "values: tensor([[-0.5213]])\n",
      "clipped_actions: [[0.       6.215059 0.       0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.21348865]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -1.9820766   2.1418738 -10.378904  -35.463135 ]]\n",
      "values: tensor([[-0.3384]])\n",
      "clipped_actions: [[0.        2.1418738 0.        0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.26686424]\n",
      "\n",
      "\n",
      "Log-Std at step 34824: [1.9482677 2.4390879 3.274706  3.556795 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  23914.975 -114418.875  -38807.465  -53327.152]]\n",
      "values: tensor([[-0.8858]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05377928]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 1591.2494 -7595.258  -2569.8665 -3512.7493]]\n",
      "values: tensor([[-0.8858]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.10755979]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -1.4529467 -13.127116   23.331118  -66.52784  ]]\n",
      "values: tensor([[-0.8858]])\n",
      "clipped_actions: [[ 0.        0.       23.331118  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.05378054]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-1.8734455e-02 -1.1112118e-01 -9.2416000e+00 -6.4603653e+01]]\n",
      "values: tensor([[-0.2859]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.0537812]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -5.5073395 -23.020378  -24.604763  -35.10817  ]]\n",
      "values: tensor([[-0.0108]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.05378189]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[  721.0813 -3500.3894 -1115.8247 -1661.3413]]\n",
      "values: tensor([[0.1043]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.10756505]\n",
      "\n",
      "\n",
      "Log-Std at step 36870: [1.9566723 2.4139624 3.2714257 3.5399902]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  3233.7065 -39276.535  -17616.287  -33424.668 ]]\n",
      "values: tensor([[-0.9065]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05471297]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  -7.481638   -8.028783   29.522501 -127.374985]]\n",
      "values: tensor([[-0.9065]])\n",
      "clipped_actions: [[ 0.        0.       29.522501  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -0.20168829  -4.5796227  -11.212552    -2.4347649 ]]\n",
      "values: tensor([[-0.3601]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -1.904401   -6.5248013  -7.594578  -79.22623  ]]\n",
      "values: tensor([[-0.2089]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -8.18689    -2.1965814  30.011326  -80.076294 ]]\n",
      "values: tensor([[-0.0800]])\n",
      "clipped_actions: [[ 0.        0.       30.011326  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.05471511]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[  74.40222 -954.5781  -402.55606 -795.20215]]\n",
      "values: tensor([[0.0681]])\n",
      "clipped_actions: [[74.40222  0.       0.       0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.10943118]\n",
      "\n",
      "\n",
      "Log-Std at step 38922: [1.9438859 2.4047139 3.2560956 3.5153086]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -1548.1031 -32777.227   -5416.875  -44380.16  ]]\n",
      "values: tensor([[-0.1205]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  3.987177 -10.214193  14.805298 -74.93187 ]]\n",
      "values: tensor([[0.0216]])\n",
      "clipped_actions: [[ 3.987177  0.       14.805298  0.      ]]\n",
      "new_obs: [[ 6.012823  3.987177  0.        0.06     10.        0.        0.\n",
      "  10.        0.       10.        0.       10.        0.        0.\n",
      "   0.        0.        2.      ]]\n",
      "rewards: [0.09957283]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 6.0128,  3.9872,  0.0000,  0.0600, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[  4.9480233  13.35842   -20.595032   -7.442295 ]]\n",
      "values: tensor([[0.2412]])\n",
      "clipped_actions: [[ 4.9480233 13.35842    0.         0.       ]]\n",
      "new_obs: [[ 1.0647998  3.987177   0.         0.06       0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [-0.01122159]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 1.0648,  3.9872,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[  1.0817413 -24.740501   34.819885  -79.444534 ]]\n",
      "values: tensor([[-0.2870]])\n",
      "clipped_actions: [[ 1.0817413  0.        34.819885   0.       ]]\n",
      "new_obs: [[ 0.          5.0519767   0.          0.07264613  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [-0.09842267]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000,  5.0520,  0.0000,  0.0726,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ -4.2613187  -4.152449  -61.79431   -32.850792 ]]\n",
      "values: tensor([[-0.3551]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.          5.0519767   0.          0.07264613  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [-0.09842365]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  5.0520,  0.0000,  0.0726,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ -9.272683  -8.310051 -42.484657 -40.357803]]\n",
      "values: tensor([[-0.2244]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.09842473]\n",
      "\n",
      "\n",
      "Log-Std at step 40968: [1.9299873 2.3975558 3.2049422 3.5135126]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-12626.959 -83826.62  -10409.423 -80226.91 ]]\n",
      "values: tensor([[0.2492]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -10.557631  -14.742587   34.582123 -111.37752 ]]\n",
      "values: tensor([[0.2672]])\n",
      "clipped_actions: [[ 0.        0.       34.582123  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.05595528]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  3.951452    6.4024353 -15.111873  -35.70917  ]]\n",
      "values: tensor([[0.3415]])\n",
      "clipped_actions: [[3.951452  6.4024353 0.        0.       ]]\n",
      "new_obs: [[ 6.0485477  0.         0.         0.         0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [-0.05595575]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 6.0485,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ -0.2440362  -9.308357  -28.98634   -99.98107  ]]\n",
      "values: tensor([[-0.1383]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 6.0485477  0.         0.         0.         0.        10.\n",
      "   0.        10.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         4.       ]]\n",
      "rewards: [-0.05595627]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 6.0485,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  3.2168517 -24.624504  -26.084381  -62.245262 ]]\n",
      "values: tensor([[-0.1914]])\n",
      "clipped_actions: [[3.2168517 0.        0.        0.       ]]\n",
      "new_obs: [[ 2.8316963  3.2168517  0.         0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.01604508]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 2.8317,  3.2169,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ -8.224771 -10.657123 -29.598873 -64.52931 ]]\n",
      "values: tensor([[-0.1510]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.01604523]\n",
      "\n",
      "\n",
      "Log-Std at step 43014: [1.9190491 2.3932858 3.1593578 3.4985337]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  -2899.3179 -168867.19    -41559.805  -136003.47  ]]\n",
      "values: tensor([[0.2191]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 12.784836 -15.587332  50.856762 -78.92608 ]]\n",
      "values: tensor([[0.2383]])\n",
      "clipped_actions: [[12.784836  0.       50.856762  0.      ]]\n",
      "new_obs: [[ 0.   10.    0.    0.06 10.    0.    0.   10.    0.   10.    0.   10.\n",
      "   0.    0.    0.    0.    2.  ]]\n",
      "rewards: [0.11306715]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000, 10.0000,  0.0000,  0.0600, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[  4.029471 -19.13208   25.149122 -47.73621 ]]\n",
      "values: tensor([[0.9562]])\n",
      "clipped_actions: [[ 4.029471  0.       25.149122  0.      ]]\n",
      "new_obs: [[ 5.970529   14.02947     0.          0.07723288  0.         10.\n",
      "   0.         10.          0.         10.          0.          0.\n",
      "   0.          0.          0.          0.          3.        ]]\n",
      "rewards: [0.2151616]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 5.9705, 14.0295,  0.0000,  0.0772,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[  -3.8598247  -13.985431   -47.83271   -102.35411  ]]\n",
      "values: tensor([[1.1938]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 5.970529   14.02947     0.          0.07723288  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.21516147]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 5.9705, 14.0295,  0.0000,  0.0772,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  4.924215 -31.29963   19.372498 -65.3255  ]]\n",
      "values: tensor([[0.9965]])\n",
      "clipped_actions: [[ 4.924215  0.       19.372498  0.      ]]\n",
      "new_obs: [[ 1.0463142  18.953686    0.          0.09282103  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.26997948]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 1.0463, 18.9537,  0.0000,  0.0928,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ -18.016409   33.8085   -148.91513  -245.34096 ]]\n",
      "values: tensor([[1.0224]])\n",
      "clipped_actions: [[ 0.     33.8085  0.      0.    ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.0706043]\n",
      "\n",
      "\n",
      "Log-Std at step 45066: [1.9521722 2.3465343 3.1152866 3.4918742]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-13216.041 -90199.945 -38232.246 -86901.22 ]]\n",
      "values: tensor([[0.3910]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-11.319511 -32.134693  50.685394 -80.00709 ]]\n",
      "values: tensor([[0.4096]])\n",
      "clipped_actions: [[ 0.        0.       50.685394  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.05694721]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  1.9992464 -17.857756   46.749725  -98.28111  ]]\n",
      "values: tensor([[0.4544]])\n",
      "clipped_actions: [[ 1.9992464  0.        46.749725   0.       ]]\n",
      "new_obs: [[18.000753   1.9992464  0.         0.06       0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [0.13666554]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[18.0008,  1.9992,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[  3.491711 -23.125624 -24.431843 -36.072998]]\n",
      "values: tensor([[0.4372]])\n",
      "clipped_actions: [[3.491711 0.       0.       0.      ]]\n",
      "new_obs: [[14.509043    5.4909573   0.          0.09815412  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.21620387]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[14.5090,  5.4910,  0.0000,  0.0982,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ -9.891372  -33.528225   -7.5247316  -3.3065987]]\n",
      "values: tensor([[0.4438]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[14.509043    5.4909573   0.          0.09815412  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.21620348]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[14.5090,  5.4910,  0.0000,  0.0982,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[  1.0384037  -7.884975  -19.3251    -90.37169  ]]\n",
      "values: tensor([[0.1435]])\n",
      "clipped_actions: [[1.0384037 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.23985547]\n",
      "\n",
      "\n",
      "Log-Std at step 47112: [1.9793553 2.308067  3.1072965 3.4753492]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -50008.59  -236659.23  -102303.125 -181572.2  ]]\n",
      "values: tensor([[0.6838]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -14.939573  -18.909431   51.576347 -105.834335]]\n",
      "values: tensor([[0.7239]])\n",
      "clipped_actions: [[ 0.        0.       51.576347  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.05733142]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-22.242014   6.165023  25.969025 -10.107208]]\n",
      "values: tensor([[0.6348]])\n",
      "clipped_actions: [[ 0.        6.165023 25.969025  0.      ]]\n",
      "new_obs: [[20.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.05733184]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  0.18013987 -13.174723    -9.743963   -76.1669    ]]\n",
      "values: tensor([[0.4307]])\n",
      "clipped_actions: [[0.18013987 0.         0.         0.        ]]\n",
      "new_obs: [[19.81986     0.18013987  0.          0.06        0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.06146333]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[19.8199,  0.1801,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  9.8723755  -4.690551    8.66695   -51.948345 ]]\n",
      "values: tensor([[0.1308]])\n",
      "clipped_actions: [[9.8723755 0.        8.66695   0.       ]]\n",
      "new_obs: [[ 9.947485  10.052515   0.         0.1189248  0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.23053382]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 9.9475, 10.0525,  0.0000,  0.1189,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[  -0.312294   20.726463    4.705459 -116.94278 ]]\n",
      "values: tensor([[0.2715]])\n",
      "clipped_actions: [[ 0.       20.726463  4.705459  0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.57693094]\n",
      "\n",
      "\n",
      "Log-Std at step 49158: [2.0238633 2.2956839 3.098348  3.4559755]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-107910.17 -466405.88 -287139.2  -325652.38]]\n",
      "values: tensor([[0.7521]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -8181.888 -35346.34  -21773.488 -24675.174]]\n",
      "values: tensor([[0.7540]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-12.725623   -5.4150543  34.31503   -85.71542  ]]\n",
      "values: tensor([[0.3386]])\n",
      "clipped_actions: [[ 0.       0.      34.31503  0.     ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.05765207]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -5.474958 -12.195821 -56.755005 -46.687775]]\n",
      "values: tensor([[0.3481]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.05765248]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[   7.635853   -21.480976     2.5942478 -145.00331  ]]\n",
      "values: tensor([[0.0847]])\n",
      "clipped_actions: [[7.635853  0.        2.5942478 0.       ]]\n",
      "new_obs: [[ 2.3641472  7.635853   0.         0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.17609122]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 2.3641,  7.6359,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ -14.940771   11.368751  -46.31991  -107.001495]]\n",
      "values: tensor([[0.3406]])\n",
      "clipped_actions: [[ 0.       11.368751  0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.47061923]\n",
      "\n",
      "\n",
      "Log-Std at step 51210: [2.01495   2.260568  3.0846531 3.4607668]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -32061.715 -206225.39  -147736.84  -175454.88 ]]\n",
      "values: tensor([[0.2742]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -1606.6897 -10329.6     -7388.294   -8763.591 ]]\n",
      "values: tensor([[0.2752]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  5.316504 -13.498852  25.123138 -25.373962]]\n",
      "values: tensor([[0.2852]])\n",
      "clipped_actions: [[ 5.316504  0.       25.123138  0.      ]]\n",
      "new_obs: [[ 4.683496  5.316504  0.        0.06      0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.12036575]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 4.6835,  5.3165,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[  1.1329037 -13.214644  -16.797493  -65.705444 ]]\n",
      "values: tensor([[0.6209]])\n",
      "clipped_actions: [[1.1329037 0.        0.        0.       ]]\n",
      "new_obs: [[ 3.5505924   6.4494076   0.          0.07053961  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.1468024]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 3.5506,  6.4494,  0.0000,  0.0705,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  8.667713  -20.582281    1.2437706 -45.855072 ]]\n",
      "values: tensor([[0.4673]])\n",
      "clipped_actions: [[8.667713  0.        1.2437706 0.       ]]\n",
      "new_obs: [[ 0.         10.          0.          0.09184316  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.1129813]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 10.0000,  0.0000,  0.0918,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[  -1.35322    31.082561  -31.57295  -104.13759 ]]\n",
      "values: tensor([[0.5559]])\n",
      "clipped_actions: [[ 0.       31.082561  0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.5213374]\n",
      "\n",
      "\n",
      "Log-Std at step 53256: [2.008454  2.2168572 3.039788  3.453892 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  -1860.9636 -301882.25   -169623.6    -265685.7   ]]\n",
      "values: tensor([[0.4902]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[   -88.70026 -12898.456    -7249.817   -11381.745  ]]\n",
      "values: tensor([[0.5208]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -8.45328    8.683485  10.603252 -66.04408 ]]\n",
      "values: tensor([[0.5568]])\n",
      "clipped_actions: [[ 0.        8.683485 10.603252  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  5.9589   -27.101269 -16.897678 -64.26348 ]]\n",
      "values: tensor([[0.4637]])\n",
      "clipped_actions: [[5.9589 0.     0.     0.    ]]\n",
      "new_obs: [[ 4.0411  5.9589  0.      0.06    0.     10.      0.     10.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      4.    ]]\n",
      "rewards: [0.14047053]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 4.0411,  5.9589,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  1.6208285  -7.6664777 -21.375002  -57.432003 ]]\n",
      "values: tensor([[0.4451]])\n",
      "clipped_actions: [[1.6208285 0.        0.        0.       ]]\n",
      "new_obs: [[ 2.4202714   7.5797286   0.          0.07283024  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.17867956]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 2.4203,  7.5797,  0.0000,  0.0728,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ -56.474575  117.435425 -102.08276  -154.24277 ]]\n",
      "values: tensor([[0.2873]])\n",
      "clipped_actions: [[  0.       117.435425   0.         0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.47710368]\n",
      "\n",
      "\n",
      "Log-Std at step 55302: [2.0271032 2.2034779 2.9852104 3.434636 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  58088.285 -567376.5   -393563.06  -514902.7  ]]\n",
      "values: tensor([[0.5339]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05956065]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  3944.5273 -38556.836  -26765.55   -34966.51  ]]\n",
      "values: tensor([[0.5344]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.11912228]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -9.081547 -14.238541  32.853046 -83.96588 ]]\n",
      "values: tensor([[0.5371]])\n",
      "clipped_actions: [[ 0.        0.       32.853046  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.05956165]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  9.277778 -26.796963 -37.122753 -41.74506 ]]\n",
      "values: tensor([[0.6295]])\n",
      "clipped_actions: [[9.277778 0.       0.       0.      ]]\n",
      "new_obs: [[ 0.7222223  9.277778   0.         0.06       0.        10.\n",
      "   0.        10.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         4.       ]]\n",
      "rewards: [0.16147956]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.7222,  9.2778,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[-18.0514     -3.4994378 -26.250744  -78.881256 ]]\n",
      "values: tensor([[0.7942]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.7222223  9.277778   0.         0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.16148067]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.7222,  9.2778,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ -80.83584   126.01691  -108.790146 -183.82396 ]]\n",
      "values: tensor([[0.5501]])\n",
      "clipped_actions: [[  0.      126.01691   0.        0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.54400456]\n",
      "\n",
      "\n",
      "Log-Std at step 57354: [2.024983  2.1625295 2.975306  3.4111624]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  99050.71 -488219.38 -469098.97 -395922.56]]\n",
      "values: tensor([[0.1313]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.06045794]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  5705.49  -28110.559 -27039.166 -22819.514]]\n",
      "values: tensor([[0.1857]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.12091686]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -13.060085  -11.842071   46.761463 -112.38869 ]]\n",
      "values: tensor([[0.2902]])\n",
      "clipped_actions: [[ 0.        0.       46.761463  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.06045893]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -0.643826 -22.177406 -25.736502 -69.2315  ]]\n",
      "values: tensor([[0.5356]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.06045944]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-22.40279  -15.979688  -3.843254  15.530262]]\n",
      "values: tensor([[0.3106]])\n",
      "clipped_actions: [[ 0.        0.        0.       15.530262]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.18137982]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ 12.459555 -14.737595 -14.349183 -80.99212 ]]\n",
      "values: tensor([[-0.0130]])\n",
      "clipped_actions: [[12.459555  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 59400: [1.9860052 2.1596572 2.953492  3.3914032]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  35757.79 -201609.48 -238190.52 -191111.33]]\n",
      "values: tensor([[0.1146]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.0613461]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 1319.6095 -7407.3447 -8721.094  -7028.733 ]]\n",
      "values: tensor([[0.1695]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.12269317]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -1.2586198  -3.036004   18.59123   -59.68958  ]]\n",
      "values: tensor([[0.2985]])\n",
      "clipped_actions: [[ 0.       0.      18.59123  0.     ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.06134707]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -1.7776961 -18.33001   -17.020687  -43.46654  ]]\n",
      "values: tensor([[0.5050]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.06134757]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[  0.49249268  -9.911997   -34.4945     -77.8728    ]]\n",
      "values: tensor([[0.2757]])\n",
      "clipped_actions: [[0.49249268 0.         0.         0.        ]]\n",
      "new_obs: [[ 9.507507    0.49249268  0.          0.06        0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [-0.04926268]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 9.5075,  0.4925,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 18.861753 -14.970158 -46.165237 -50.508633]]\n",
      "values: tensor([[0.0327]])\n",
      "clipped_actions: [[18.861753  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.12269712]\n",
      "\n",
      "\n",
      "Log-Std at step 61446: [1.9684479 2.1380112 2.9461029 3.3737237]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  10118.315  -85435.03  -121036.76  -120294.   ]]\n",
      "values: tensor([[0.2068]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.0621815]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  372.54865 -3083.075   -4396.001   -4396.163  ]]\n",
      "values: tensor([[0.2326]])\n",
      "clipped_actions: [[372.54865   0.        0.        0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.12436394]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-14.12195    -6.3878217  42.545723  -86.89961  ]]\n",
      "values: tensor([[0.3792]])\n",
      "clipped_actions: [[ 0.        0.       42.545723  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.06218246]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  7.3522997 -25.890003  -35.690247  -70.16418  ]]\n",
      "values: tensor([[0.5695]])\n",
      "clipped_actions: [[7.3522997 0.        0.        0.       ]]\n",
      "new_obs: [[ 2.6477003  7.3522997  0.         0.06       0.        10.\n",
      "   0.        10.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         4.       ]]\n",
      "rewards: [0.12069207]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 2.6477,  7.3523,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ -0.57054853 -19.276817    -6.2335644    2.535736  ]]\n",
      "values: tensor([[0.6028]])\n",
      "clipped_actions: [[0.       0.       0.       2.535736]]\n",
      "new_obs: [[ 2.6477003  7.3522997  0.         0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.05850957]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 2.6477,  7.3523,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[-47.845337  59.924194 -46.648823 -92.720215]]\n",
      "values: tensor([[0.3589]])\n",
      "clipped_actions: [[ 0.       59.924194  0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.36208048]\n",
      "\n",
      "\n",
      "Log-Std at step 63498: [1.9763314 2.1167002 2.9265842 3.3726215]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  7996.887 -27502.754 -52005.586 -53449.105]]\n",
      "values: tensor([[0.2054]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.06244145]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 24.607174 -17.477365 -55.23362  -66.05612 ]]\n",
      "values: tensor([[0.2524]])\n",
      "clipped_actions: [[24.607174  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.12488382]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -9.807787   -5.9520874  87.86994   -71.880264 ]]\n",
      "values: tensor([[0.3829]])\n",
      "clipped_actions: [[ 0.       0.      87.86994  0.     ]]\n",
      "new_obs: [[10.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.06244238]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 12.915387 -23.760885 -12.314791 -35.024513]]\n",
      "values: tensor([[0.5491]])\n",
      "clipped_actions: [[12.915387  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.   10.    0.    0.06  0.   10.    0.   10.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    4.  ]]\n",
      "rewards: [0.12488566]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 10.0000,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[-14.256913  17.921352 -40.834084 -48.232033]]\n",
      "values: tensor([[0.7688]])\n",
      "clipped_actions: [[ 0.       17.921352  0.        0.      ]]\n",
      "new_obs: [[ 0.  0. 10.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.5619871]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0., 10.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-173.21152  229.6987  -190.51253 -320.93192]]\n",
      "values: tensor([[0.4094]])\n",
      "clipped_actions: [[  0.     229.6987   0.       0.    ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.4995416]\n",
      "\n",
      "\n",
      "Log-Std at step 65544: [1.9550334 2.067332  2.8837714 3.3636103]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  17683.918 -100780.26  -176988.7   -172295.3  ]]\n",
      "values: tensor([[0.4467]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.06278066]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  671.5897 -3746.9    -6590.6255 -6495.239 ]]\n",
      "values: tensor([[0.6056]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.12556224]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  2.0047927 -15.114306   47.03281   -45.044777 ]]\n",
      "values: tensor([[0.6902]])\n",
      "clipped_actions: [[ 2.0047927  0.        47.03281    0.       ]]\n",
      "new_obs: [[ 7.9952073  2.0047927  0.         0.06       0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [-0.03760877]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 7.9952,  2.0048,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 16.796131   -5.4772635 -39.011208  -16.63507  ]]\n",
      "values: tensor([[0.8930]])\n",
      "clipped_actions: [[16.796131  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.         10.          0.          0.10797124  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.10039106]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 10.0000,  0.0000,  0.1080,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[-17.062407     0.53092957 -39.663773   -76.37338   ]]\n",
      "values: tensor([[0.9135]])\n",
      "clipped_actions: [[0.         0.53092957 0.         0.        ]]\n",
      "new_obs: [[ 0.          9.46907     0.53092957  0.10191731  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.12705815]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  9.4691,  0.5309,  0.1019,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[-103.73943  137.37791 -142.78084 -210.50642]]\n",
      "values: tensor([[0.6321]])\n",
      "clipped_actions: [[  0.      137.37791   0.        0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.5398696]\n",
      "\n",
      "\n",
      "Log-Std at step 67590: [1.9506501 2.046935  2.861399  3.3577642]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  14990.913  -77503.18  -136454.88  -137068.89 ]]\n",
      "values: tensor([[0.6081]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.0634493]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -5.6616564 -28.170334   39.132378  -89.741066 ]]\n",
      "values: tensor([[0.7102]])\n",
      "clipped_actions: [[ 0.        0.       39.132378  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-14.946976   -9.895769    9.46521    -3.3816872]]\n",
      "values: tensor([[1.5372]])\n",
      "clipped_actions: [[0.      0.      9.46521 0.     ]]\n",
      "new_obs: [[19.46521  0.       0.       0.       0.      10.       0.      10.\n",
      "   0.      10.       0.       0.       0.       0.       0.       0.\n",
      "   3.     ]]\n",
      "rewards: [0.12011371]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[19.4652,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 10.17588  -25.83355   -8.319864 -24.45013 ]]\n",
      "values: tensor([[1.7269]])\n",
      "clipped_actions: [[10.17588  0.       0.       0.     ]]\n",
      "new_obs: [[ 9.28933 10.17588  0.       0.06     0.      10.       0.      10.\n",
      "   0.       0.       0.       0.       0.       0.       0.       0.\n",
      "   4.     ]]\n",
      "rewards: [0.3783791]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 9.2893, 10.1759,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 20.22408  -23.033506  -9.552145 -99.6405  ]]\n",
      "values: tensor([[1.5506]])\n",
      "clipped_actions: [[20.22408  0.       0.       0.     ]]\n",
      "new_obs: [[ 0.         19.46521     0.          0.08863363  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.55069023]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 19.4652,  0.0000,  0.0886,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[-144.9847   169.41823 -190.82712 -219.75171]]\n",
      "values: tensor([[1.9723]])\n",
      "clipped_actions: [[  0.      169.41823   0.        0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.4752226]\n",
      "\n",
      "\n",
      "Log-Std at step 69642: [1.9320279 2.0253375 2.825164  3.33838  ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  74631.72 -184480.34 -342057.47 -281645.9 ]]\n",
      "values: tensor([[1.8706]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.06206217]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -4.3613033 -16.228643   60.14408   -97.146126 ]]\n",
      "values: tensor([[1.9021]])\n",
      "clipped_actions: [[ 0.       0.      60.14408  0.     ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-10.082592 -25.010029  31.01789  -92.10555 ]]\n",
      "values: tensor([[2.1653]])\n",
      "clipped_actions: [[ 0.       0.      31.01789  0.     ]]\n",
      "new_obs: [[20.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.06206298]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 11.894313  -9.281462 -10.216295 -48.437355]]\n",
      "values: tensor([[1.7939]])\n",
      "clipped_actions: [[11.894313  0.        0.        0.      ]]\n",
      "new_obs: [[ 8.105687 11.894313  0.        0.06      0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.35734296]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 8.1057, 11.8943,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  7.0022798  -7.994485  -13.41899   -75.634834 ]]\n",
      "values: tensor([[1.3533]])\n",
      "clipped_actions: [[7.0022798 0.        0.        0.       ]]\n",
      "new_obs: [[ 1.1034074  18.896593    0.          0.08223347  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.5311751]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 1.1034, 18.8966,  0.0000,  0.0822,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ -98.15265  138.58368 -128.86441 -120.59367]]\n",
      "values: tensor([[1.0443]])\n",
      "clipped_actions: [[  0.      138.58368   0.        0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.4072806]\n",
      "\n",
      "\n",
      "Log-Std at step 71688: [1.9793372 2.0092125 2.804851  3.3383856]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  50748.027  -74102.484 -141594.56  -120204.39 ]]\n",
      "values: tensor([[2.3558]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.06118417]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -12.219761     -0.25700378   49.333355   -114.600136  ]]\n",
      "values: tensor([[2.4622]])\n",
      "clipped_actions: [[ 0.        0.       49.333355  0.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  6.3465266 -26.250877   48.02866   -70.379875 ]]\n",
      "values: tensor([[2.4834]])\n",
      "clipped_actions: [[ 6.3465266  0.        48.02866    0.       ]]\n",
      "new_obs: [[13.653473   6.3465266  0.         0.06       0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [0.13884729]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[13.6535,  6.3465,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[  6.376547 -22.4698   -46.12963  -57.019783]]\n",
      "values: tensor([[2.5852]])\n",
      "clipped_actions: [[6.376547 0.       0.       0.      ]]\n",
      "new_obs: [[ 7.2769265  12.723074    0.          0.09007078  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.29490793]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 7.2769, 12.7231,  0.0000,  0.0901,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  6.4034524 -22.118933   -2.94977   -67.51107  ]]\n",
      "values: tensor([[1.7237]])\n",
      "clipped_actions: [[6.4034524 0.        0.        0.       ]]\n",
      "new_obs: [[ 0.8734741  19.126526    0.          0.11015844  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.45162585]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.8735, 19.1265,  0.0000,  0.1102,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[-120.56443  164.62433 -145.51288 -175.54425]]\n",
      "values: tensor([[1.3307]])\n",
      "clipped_actions: [[  0.      164.62433   0.        0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.3266045]\n",
      "\n",
      "\n",
      "Log-Std at step 73734: [1.9853402 2.0109422 2.8055248 3.325339 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  51211.688  -84408.88  -218412.4   -119593.98 ]]\n",
      "values: tensor([[2.3443]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.06035763]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  5.4929037 -17.397972   72.957664  -23.400555 ]]\n",
      "values: tensor([[2.4045]])\n",
      "clipped_actions: [[ 5.4929037  0.        72.957664   0.       ]]\n",
      "new_obs: [[ 4.5070963  5.4929037  0.         0.06      10.         0.\n",
      "   0.        10.         0.        10.         0.        10.\n",
      "   0.         0.         0.         0.         2.       ]]\n",
      "rewards: [0.06630815]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 4.5071,  5.4929,  0.0000,  0.0600, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ -16.787153  -19.042028   39.009438 -107.07415 ]]\n",
      "values: tensor([[2.4985]])\n",
      "clipped_actions: [[ 0.        0.       39.009438  0.      ]]\n",
      "new_obs: [[14.507096   5.4929037  0.         0.06       0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [0.12666689]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[14.5071,  5.4929,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 15.256018  -8.968622 -39.46116  -52.802498]]\n",
      "values: tensor([[2.3330]])\n",
      "clipped_actions: [[15.256018  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.         20.          0.          0.10352129  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.41655937]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 20.0000,  0.0000,  0.1035,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ -51.92869    59.839592  -85.951004 -124.00388 ]]\n",
      "values: tensor([[2.4499]])\n",
      "clipped_actions: [[ 0.       59.839592  0.        0.      ]]\n",
      "new_obs: [[ 0.  0. 20.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [1.3219073]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0., 20.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-187.5463   223.20526 -362.0468  -431.95047]]\n",
      "values: tensor([[1.2299]])\n",
      "clipped_actions: [[  0.      223.20526   0.        0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.2614653]\n",
      "\n",
      "\n",
      "Log-Std at step 75786: [2.004122  2.0074975 2.790471  3.2996082]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  21757.809  -58134.312 -202723.88   -83097.58 ]]\n",
      "values: tensor([[2.5142]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05943505]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-19.592087 -26.380478  50.10419  -82.73188 ]]\n",
      "values: tensor([[2.6136]])\n",
      "clipped_actions: [[ 0.       0.      50.10419  0.     ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -5.3535805 -27.026482   54.63513   -63.048603 ]]\n",
      "values: tensor([[2.5883]])\n",
      "clipped_actions: [[ 0.       0.      54.63513  0.     ]]\n",
      "new_obs: [[20.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.05943578]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 15.156492 -17.090515 -26.255798 -86.71355 ]]\n",
      "values: tensor([[2.4195]])\n",
      "clipped_actions: [[15.156492  0.        0.        0.      ]]\n",
      "new_obs: [[ 4.843508 15.156492  0.        0.06      0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.41977262]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 4.8435, 15.1565,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  8.26976     2.4395027  -3.5878925 -66.082794 ]]\n",
      "values: tensor([[1.8128]])\n",
      "clipped_actions: [[8.26976   2.4395027 0.        0.       ]]\n",
      "new_obs: [[ 0.       15.156492  0.        0.06      0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.24146497]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 15.1565,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ -15.792276   21.237972 -104.65241   -87.45297 ]]\n",
      "values: tensor([[0.8730]])\n",
      "clipped_actions: [[ 0.       21.237972  0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.902691]\n",
      "\n",
      "\n",
      "Log-Std at step 77832: [1.9827161 1.993314  2.755293  3.2933877]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  15328.9795  -16592.268  -125931.87    -49339.484 ]]\n",
      "values: tensor([[2.4762]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05876349]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  2.910799 -24.174671  27.429565 -42.21265 ]]\n",
      "values: tensor([[2.5417]])\n",
      "clipped_actions: [[ 2.910799  0.       27.429565  0.      ]]\n",
      "new_obs: [[ 7.089201  2.910799  0.        0.06     10.        0.        0.\n",
      "  10.        0.       10.        0.       10.        0.        0.\n",
      "   0.        0.        2.      ]]\n",
      "rewards: [0.03420995]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 7.0892,  2.9108,  0.0000,  0.0600, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[-10.237098 -13.731052  39.146465 -56.166317]]\n",
      "values: tensor([[2.6052]])\n",
      "clipped_actions: [[ 0.        0.       39.146465  0.      ]]\n",
      "new_obs: [[17.089201  2.910799  0.        0.06      0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.09297435]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[17.0892,  2.9108,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 31.493477 -32.31213  -10.942661 -51.993317]]\n",
      "values: tensor([[2.4794]])\n",
      "clipped_actions: [[31.493477  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.        20.         0.         0.1112676  0.        10.\n",
      "   0.        10.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         4.       ]]\n",
      "rewards: [0.4359049]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 20.0000,  0.0000,  0.1113,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ -15.819022   26.361546  -84.18602  -119.344635]]\n",
      "values: tensor([[2.3853]])\n",
      "clipped_actions: [[ 0.       26.361546  0.        0.      ]]\n",
      "new_obs: [[ 0.  0. 20.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [1.3173461]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0., 20.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -42.9626     52.596844 -253.18134  -320.38004 ]]\n",
      "values: tensor([[1.1658]])\n",
      "clipped_actions: [[ 0.       52.596844  0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.2585081]\n",
      "\n",
      "\n",
      "Log-Std at step 79878: [1.9771965 1.9557514 2.7454104 3.2982304]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[   9001.471  -62425.26  -265396.16  -105072.51 ]]\n",
      "values: tensor([[2.5349]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05803733]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  7.240794 -23.180286  65.50926  -71.469025]]\n",
      "values: tensor([[2.6167]])\n",
      "clipped_actions: [[ 7.240794  0.       65.50926   0.      ]]\n",
      "new_obs: [[ 2.7592058  7.240794   0.         0.06      10.         0.\n",
      "   0.        10.         0.        10.         0.        10.\n",
      "   0.         0.         0.         0.         2.       ]]\n",
      "rewards: [0.08404777]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 2.7592,  7.2408,  0.0000,  0.0600, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ -2.1847422  -8.304035   19.968487  -37.53533  ]]\n",
      "values: tensor([[2.6819]])\n",
      "clipped_actions: [[ 0.        0.       19.968487  0.      ]]\n",
      "new_obs: [[12.759206  7.240794  0.        0.06      0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.14208624]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[12.7592,  7.2408,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 27.144451 -18.099514 -14.063213 -64.70159 ]]\n",
      "values: tensor([[2.6258]])\n",
      "clipped_actions: [[27.144451  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.         20.          0.          0.09827762  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.3802569]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 20.0000,  0.0000,  0.0983,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ -13.878398   36.139473  -70.89628  -118.12589 ]]\n",
      "values: tensor([[2.4771]])\n",
      "clipped_actions: [[ 0.       36.139473  0.        0.      ]]\n",
      "new_obs: [[ 0.  0. 20.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [1.2508082]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0., 20.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -62.949806   88.99442  -404.57858  -468.63327 ]]\n",
      "values: tensor([[1.2478]])\n",
      "clipped_actions: [[ 0.      88.99442  0.       0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.1927073]\n",
      "\n",
      "\n",
      "Log-Std at step 81930: [1.9696869 1.9173313 2.727322  3.2878616]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  20836.07   -24496.248 -126497.984  -44148.727]]\n",
      "values: tensor([[2.5844]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05723503]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  5.187942 -21.242172  49.45536  -63.73374 ]]\n",
      "values: tensor([[2.6494]])\n",
      "clipped_actions: [[ 5.187942  0.       49.45536   0.      ]]\n",
      "new_obs: [[ 4.812058  5.187942  0.        0.06     10.        0.        0.\n",
      "  10.        0.       10.        0.       10.        0.        0.\n",
      "   0.        0.        2.      ]]\n",
      "rewards: [0.05938675]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 4.8121,  5.1879,  0.0000,  0.0600, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ -4.852312  -8.778807  27.378586 -70.42262 ]]\n",
      "values: tensor([[2.6476]])\n",
      "clipped_actions: [[ 0.        0.       27.378586  0.      ]]\n",
      "new_obs: [[14.8120575  5.187942   0.         0.06       0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [0.11662278]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[14.8121,  5.1879,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 18.443817 -27.51099  -16.782087 -49.393463]]\n",
      "values: tensor([[2.6238]])\n",
      "clipped_actions: [[18.443817  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.         20.          0.          0.10443617  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.3984998]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 20.0000,  0.0000,  0.1044,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[-32.612312  54.267456 -72.756165 -90.27856 ]]\n",
      "values: tensor([[2.4129]])\n",
      "clipped_actions: [[ 0.       54.267456  0.        0.      ]]\n",
      "new_obs: [[ 0.  0. 20.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [1.2570186]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0., 20.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-104.12753  161.74774 -334.93494 -376.42108]]\n",
      "values: tensor([[1.1369]])\n",
      "clipped_actions: [[  0.      161.74774   0.        0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.199723]\n",
      "\n",
      "\n",
      "Log-Std at step 83976: [1.9262613 1.903996  2.7136147 3.278497 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  4525.6055  -5101.8184 -25925.19   -10373.609 ]]\n",
      "values: tensor([[2.6860]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.0565435]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -2.0614657 -20.070198   44.34371   -98.53311  ]]\n",
      "values: tensor([[2.7828]])\n",
      "clipped_actions: [[ 0.       0.      44.34371  0.     ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -8.91827  -16.998682  44.150078 -48.774895]]\n",
      "values: tensor([[2.7855]])\n",
      "clipped_actions: [[ 0.        0.       44.150078  0.      ]]\n",
      "new_obs: [[20.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.05654415]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 19.09947  -20.751556 -18.544651 -54.122086]]\n",
      "values: tensor([[2.7903]])\n",
      "clipped_actions: [[19.09947  0.       0.       0.     ]]\n",
      "new_obs: [[ 0.90052986 19.09947     0.          0.06        0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.48853135]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.9005, 19.0995,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ -4.752439    9.8409195 -27.954914  -79.84439  ]]\n",
      "values: tensor([[2.3974]])\n",
      "clipped_actions: [[0.        9.8409195 0.        0.       ]]\n",
      "new_obs: [[ 9.0052986e-01  9.2585506e+00  9.8409195e+00 -3.7740390e-03\n",
      "   0.0000000e+00  1.0000000e+01  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   5.0000000e+00]]\n",
      "rewards: [0.93368304]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 9.0053e-01,  9.2586e+00,  9.8409e+00, -3.7740e-03,  0.0000e+00,\n",
      "          1.0000e+01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  5.0000e+00]])\n",
      "actions: [[  10.838109    4.450814 -125.35995  -197.7209  ]]\n",
      "values: tensor([[1.7460]])\n",
      "clipped_actions: [[10.838109  4.450814  0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.7640327]\n",
      "\n",
      "\n",
      "Log-Std at step 86022: [1.8849039 1.8828694 2.713049  3.244309 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  670.5441   -303.74033 -3074.2676  -1714.0491 ]]\n",
      "values: tensor([[2.4086]])\n",
      "clipped_actions: [[500.   0.   0.   0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05617915]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -6.6679945 -20.950733   40.34989   -91.87999  ]]\n",
      "values: tensor([[2.4722]])\n",
      "clipped_actions: [[ 0.       0.      40.34989  0.     ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -7.7358546 -16.898602   24.731215   -5.6144295]]\n",
      "values: tensor([[2.4755]])\n",
      "clipped_actions: [[ 0.        0.       24.731215  0.      ]]\n",
      "new_obs: [[20.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.05617979]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 21.49162  -20.887577 -24.373478 -59.36046 ]]\n",
      "values: tensor([[2.5403]])\n",
      "clipped_actions: [[21.49162  0.       0.       0.     ]]\n",
      "new_obs: [[ 0.   20.    0.    0.06  0.   10.    0.   10.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    4.  ]]\n",
      "rewards: [0.44944012]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 20.0000,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[   6.143713   14.42346   -54.753834 -105.93268 ]]\n",
      "values: tensor([[2.0444]])\n",
      "clipped_actions: [[ 6.143713 14.42346   0.        0.      ]]\n",
      "new_obs: [[ 0.          5.57654    14.42346    -0.09518719  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [1.0414995]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000,  5.5765, 14.4235, -0.0952,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[   5.172248   54.339233 -235.76015  -271.97906 ]]\n",
      "values: tensor([[1.0966]])\n",
      "clipped_actions: [[ 5.172248 54.339233  0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.1797268]\n",
      "\n",
      "\n",
      "Log-Std at step 88074: [1.8406122 1.8597351 2.6805935 3.254169 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 12.956012  -21.051107   -1.8401184 -65.58259  ]]\n",
      "values: tensor([[2.4987]])\n",
      "clipped_actions: [[12.956012  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.05567545]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -7.7504816 -25.288263   65.28666   -68.083275 ]]\n",
      "values: tensor([[2.5774]])\n",
      "clipped_actions: [[ 0.       0.      65.28666  0.     ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -9.1622305 -21.853014   40.87436   -72.82293  ]]\n",
      "values: tensor([[2.6268]])\n",
      "clipped_actions: [[ 0.       0.      40.87436  0.     ]]\n",
      "new_obs: [[20.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.05567607]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  20.729912  -23.881386  -40.98397  -128.64189 ]]\n",
      "values: tensor([[2.6627]])\n",
      "clipped_actions: [[20.729912  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.   20.    0.    0.06  0.   10.    0.   10.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    4.  ]]\n",
      "rewards: [0.4454104]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 20.0000,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ -7.691425  21.786911 -60.294937 -91.09483 ]]\n",
      "values: tensor([[2.2968]])\n",
      "clipped_actions: [[ 0.       21.786911  0.        0.      ]]\n",
      "new_obs: [[ 0.  0. 20.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [1.2805393]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0., 20.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[   4.637537   18.19132   -67.622955 -106.59737 ]]\n",
      "values: tensor([[1.1197]])\n",
      "clipped_actions: [[ 4.637537 18.19132   0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.1691384]\n",
      "\n",
      "\n",
      "Log-Std at step 90120: [1.7972037 1.8366466 2.6600404 3.2514696]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  7.480197 -17.457865  26.264408 -94.73228 ]]\n",
      "values: tensor([[4.1356]])\n",
      "clipped_actions: [[ 7.480197  0.       26.264408  0.      ]]\n",
      "new_obs: [[ 2.519803  7.480197  0.        0.06     10.        0.       10.\n",
      "   0.        0.       10.        0.       10.        0.       10.\n",
      "   0.        0.        1.      ]]\n",
      "rewards: [0.13262431]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 2.5198,  7.4802,  0.0000,  0.0600, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[  2.269625 -23.689922  54.932312 -76.946   ]]\n",
      "values: tensor([[4.2301]])\n",
      "clipped_actions: [[ 2.269625  0.       54.932312  0.      ]]\n",
      "new_obs: [[10.250178    9.749822    0.          0.07396718 10.          0.\n",
      "   0.         10.          0.         10.          0.         10.\n",
      "   0.          0.          0.          0.          2.        ]]\n",
      "rewards: [0.20987803]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.2502,  9.7498,  0.0000,  0.0740, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ -5.1905932 -13.681893   42.294697  -70.84654  ]]\n",
      "values: tensor([[4.3770]])\n",
      "clipped_actions: [[ 0.        0.       42.294697  0.      ]]\n",
      "new_obs: [[20.250177    9.749822    0.          0.07396718  0.         10.\n",
      "   0.         10.          0.         10.          0.          0.\n",
      "   0.          0.          0.          0.          3.        ]]\n",
      "rewards: [0.26301304]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[20.2502,  9.7498,  0.0000,  0.0740,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 18.831814 -33.17106  -11.468023 -34.714687]]\n",
      "values: tensor([[4.1143]])\n",
      "clipped_actions: [[18.831814  0.        0.        0.      ]]\n",
      "new_obs: [[ 1.4183643  28.581636    0.          0.11349986  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.66325694]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 1.4184, 28.5816,  0.0000,  0.1135,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  13.9031925     0.49564934  -71.36327    -137.36095   ]]\n",
      "values: tensor([[3.6818]])\n",
      "clipped_actions: [[13.9031925   0.49564934  0.          0.        ]]\n",
      "new_obs: [[ 0.         28.581636    0.          0.11349986  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.5038487]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 28.5816,  0.0000,  0.1135,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[  45.728237  -19.27548  -133.78624  -208.2444  ]]\n",
      "values: tensor([[1.8992]])\n",
      "clipped_actions: [[45.728237  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.4507056]\n",
      "\n",
      "\n",
      "Log-Std at step 92166: [1.8056391 1.8384256 2.6142414 3.2514534]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 10.522671 -24.16735   41.925213 -64.07986 ]]\n",
      "values: tensor([[2.7196]])\n",
      "clipped_actions: [[10.522671  0.       41.925213  0.      ]]\n",
      "new_obs: [[ 0.   10.    0.    0.06 10.    0.   10.    0.    0.   10.    0.   10.\n",
      "   0.   10.    0.    0.    1.  ]]\n",
      "rewards: [0.10545362]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000, 10.0000,  0.0000,  0.0600, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ -5.819525 -10.807617  48.90904  -82.44732 ]]\n",
      "values: tensor([[2.5870]])\n",
      "clipped_actions: [[ 0.       0.      48.90904  0.     ]]\n",
      "new_obs: [[10.   10.    0.    0.06 10.    0.    0.   10.    0.   10.    0.   10.\n",
      "   0.    0.    0.    0.    2.  ]]\n",
      "rewards: [0.1581812]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[10.0000, 10.0000,  0.0000,  0.0600, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 10.026175 -11.310493  27.34674  -69.551125]]\n",
      "values: tensor([[2.3350]])\n",
      "clipped_actions: [[10.026175  0.       27.34674   0.      ]]\n",
      "new_obs: [[ 9.973825   20.026175    0.          0.09003921  0.         10.\n",
      "   0.         10.          0.         10.          0.          0.\n",
      "   0.          0.          0.          0.          3.        ]]\n",
      "rewards: [0.31691554]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 9.9738, 20.0262,  0.0000,  0.0900,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[  6.0525255   -0.29718757 -42.823723   -39.342644  ]]\n",
      "values: tensor([[2.0102]])\n",
      "clipped_actions: [[6.0525255 0.        0.        0.       ]]\n",
      "new_obs: [[ 3.9213     26.078701    0.          0.10396443  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.44456866]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 3.9213, 26.0787,  0.0000,  0.1040,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  20.634735    0.318403  -98.55896  -201.06401 ]]\n",
      "values: tensor([[1.2054]])\n",
      "clipped_actions: [[20.634735  0.318403  0.        0.      ]]\n",
      "new_obs: [[ 0.         26.078701    0.          0.10396443  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.2863859]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 26.0787,  0.0000,  0.1040,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[  68.24952   -22.396103 -187.12465  -340.8239  ]]\n",
      "values: tensor([[0.5083]])\n",
      "clipped_actions: [[68.24952  0.       0.       0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.23365723]\n",
      "\n",
      "\n",
      "Log-Std at step 94218: [1.816583  1.8234802 2.579499  3.256374 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  1.7628884 -17.409721   22.0163    -62.932404 ]]\n",
      "values: tensor([[2.3069]])\n",
      "clipped_actions: [[ 1.7628884  0.        22.0163     0.       ]]\n",
      "new_obs: [[ 8.237112   1.7628884  0.         0.06      10.         0.\n",
      "  10.         0.         0.        10.         0.        10.\n",
      "   0.        10.         0.         0.         1.       ]]\n",
      "rewards: [0.0708625]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 8.2371,  1.7629,  0.0000,  0.0600, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ -0.7955165 -24.281548   54.38594   -82.024086 ]]\n",
      "values: tensor([[2.2406]])\n",
      "clipped_actions: [[ 0.       0.      54.38594  0.     ]]\n",
      "new_obs: [[18.237112   1.7628884  0.         0.06      10.         0.\n",
      "   0.        10.         0.        10.         0.        10.\n",
      "   0.         0.         0.         0.         2.       ]]\n",
      "rewards: [0.12325382]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[18.2371,  1.7629,  0.0000,  0.0600, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ -0.5522547 -15.588841   53.696377  -58.550472 ]]\n",
      "values: tensor([[2.1461]])\n",
      "clipped_actions: [[ 0.        0.       53.696377  0.      ]]\n",
      "new_obs: [[28.237112   1.7628884  0.         0.06       0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [0.17564559]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[28.2371,  1.7629,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 35.771828 -15.595953  -8.799664 -31.588463]]\n",
      "values: tensor([[1.9699]])\n",
      "clipped_actions: [[35.771828  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.         30.          0.          0.11647423  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.71500444]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 30.0000,  0.0000,  0.1165,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  37.608086   -1.541791 -164.04108  -251.39583 ]]\n",
      "values: tensor([[1.2135]])\n",
      "clipped_actions: [[37.608086  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.         30.          0.          0.11647423  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.66260654]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 30.0000,  0.0000,  0.1165,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[  72.59917   -25.546335 -262.55115  -402.21872 ]]\n",
      "values: tensor([[0.5650]])\n",
      "clipped_actions: [[72.59917  0.       0.       0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.6102022]\n",
      "\n",
      "\n",
      "Log-Std at step 96264: [1.8306947 1.8337172 2.5638282 3.2672317]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  6.87994  -10.754503  24.343866 -69.20498 ]]\n",
      "values: tensor([[2.1693]])\n",
      "clipped_actions: [[ 6.87994   0.       24.343866  0.      ]]\n",
      "new_obs: [[ 3.12006  6.87994  0.       0.06    10.       0.      10.       0.\n",
      "   0.      10.       0.      10.       0.      10.       0.       0.\n",
      "   1.     ]]\n",
      "rewards: [0.12377924]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 3.1201,  6.8799,  0.0000,  0.0600, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[  -1.8022012  -19.122314    19.453703  -109.96796  ]]\n",
      "values: tensor([[2.0463]])\n",
      "clipped_actions: [[ 0.        0.       19.453703  0.      ]]\n",
      "new_obs: [[13.12006  6.87994  0.       0.06    10.       0.       0.      10.\n",
      "   0.      10.       0.      10.       0.       0.       0.       0.\n",
      "   2.     ]]\n",
      "rewards: [0.17587596]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[13.1201,  6.8799,  0.0000,  0.0600, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[  1.3550835 -22.514988   16.594286  -46.830997 ]]\n",
      "values: tensor([[2.0063]])\n",
      "clipped_actions: [[ 1.3550835  0.        16.594286   0.       ]]\n",
      "new_obs: [[21.764977    8.2350235   0.          0.06987307  0.         10.\n",
      "   0.         10.          0.         10.          0.          0.\n",
      "   0.          0.          0.          0.          3.        ]]\n",
      "rewards: [0.24209197]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[21.7650,  8.2350,  0.0000,  0.0699,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[  25.260805  -26.286718  -22.530218 -135.37979 ]]\n",
      "values: tensor([[1.8085]])\n",
      "clipped_actions: [[25.260805  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.         30.          0.          0.11340303  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.6435449]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 30.0000,  0.0000,  0.1134,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[  28.218458    -7.8445034 -162.15233   -297.78754  ]]\n",
      "values: tensor([[1.1991]])\n",
      "clipped_actions: [[28.218458  0.        0.        0.      ]]\n",
      "new_obs: [[ 0.         30.          0.          0.11340303  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.59144276]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 30.0000,  0.0000,  0.1134,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[  74.02259   -32.009537 -210.80527  -353.0931  ]]\n",
      "values: tensor([[0.5680]])\n",
      "clipped_actions: [[74.02259  0.       0.       0.     ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.5393359]\n",
      "\n",
      "\n",
      "Log-Std at step 98310: [1.828157  1.8260708 2.544492  3.2653198]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x19574011520>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logpath = model_name[len(\"models/\"):]\n",
    "print(f\"logging at {logpath}\")\n",
    "model.learn(total_timesteps = 100000, \n",
    "            progress_bar = False, \n",
    "            tb_log_name = logpath, \n",
    "            callback = callback,\n",
    "            reset_num_timesteps = False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M,Q,P,B,Z,D = 10, 0, 5, 5, 1, 0\n",
    "M, Q, P, B, Z, D  = cfg[\"env\"][\"M\"], cfg[\"env\"][\"Q\"], cfg[\"env\"][\"P\"], cfg[\"env\"][\"B\"], cfg[\"env\"][\"Z\"], 0\n",
    "# M,Q,P,B,Z,D = 0, 0, 0, 0, 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = True, \n",
    "               D = cfg[\"env\"][\"D\"], \n",
    "               Q = cfg[\"env\"][\"Q\"], \n",
    "               P = cfg[\"env\"][\"P\"], \n",
    "               B = cfg[\"env\"][\"B\"], \n",
    "               Z = cfg[\"env\"][\"Z\"], \n",
    "               M = cfg[\"env\"][\"M\"],\n",
    "               reg = cfg[\"env\"][\"reg\"],\n",
    "               reg_lambda = cfg[\"env\"][\"reg_lambda\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"],\n",
    "               alpha = cfg[\"env\"][\"alpha\"],\n",
    "               beta = cfg[\"env\"][\"beta\"],\n",
    "               connections = connections, \n",
    "               action_sample = action_sample,\n",
    "               tau0 = tau0,delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 0, 'B': 0, 'P': 0, 'reg': 0}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.13335086}}, 'tau': {'s1': 0.106610715}, 'delta': {'p1': 0.022796683}}\n",
      "[PEN] t1; p1:\t\t\tsold too much (more than demand)\n",
      "Increased reward by 0.10661071538925171 through tank population in s1\n",
      "j1: inv: 0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.13335086405277252\n",
      "j1: b: 0.0\n",
      "[PEN] t1; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.106610715} {'j1': 0.0} {'p1': 0.0}\n",
      "    -10.04953682422638\n",
      "{'M': 0, 'B': -5, 'P': -5, 'reg': -0.2627582550048828}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.10351893}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 0.0}}\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.10351893305778503\n",
      "j1: b: 0.0\n",
      "[PEN] t2; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.106610715} {'j1': 0.0} {'p1': 0.0}\n",
      "    -15.153055757284164\n",
      "{'M': 0, 'B': -5, 'P': -10, 'reg': -0.36627718806266785}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.11744881}, 'delta': {'p1': 0.021353317}}\n",
      "[PEN] t3; p1:\t\t\tsold too much (more than demand)\n",
      "Increased reward by 0.11744880676269531 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.22405952} {'j1': 0.0} {'p1': 0.0}\n",
      "    -20.05696026980877\n",
      "{'M': 0, 'B': -10, 'P': -10, 'reg': -0.5050793141126633}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 0.06863385}}\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "p1: newdelta: 0.0\n",
      "[PEN] t4; p1:\t\t\tsold too much (resulting amount less than demand tank LB)\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.22405952} {'j1': 0.0} {'p1': 0.0}\n",
      "    -25.12559411674738\n",
      "{'M': 0, 'B': -15, 'P': -10, 'reg': -0.5737131610512733}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.07955089}, 'delta': {'p1': 0.047517113}}\n",
      "[PEN] t5; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "p1: newdelta: 0.0\n",
      "[PEN] t5; p1:\t\t\tsold too much (resulting amount less than demand tank LB)\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.22405952} {'j1': 0.0} {'p1': 0.0}\n",
      "    -35.252662129700184\n",
      "{'M': 0, 'B': -25, 'P': -10, 'reg': -0.7007811740040779}\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.15008144}}, 'blend_demand': {'j1': {'p1': 0.1658755}}, 'tau': {'s1': 0.16930379}, 'delta': {'p1': 0.031857736}}\n",
      "[PEN] t6; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.15008144080638885, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.16587549448013306\n",
      "[PEN] t6; j1:\t\t\tIn and out flow both non-zero (in: 0.15, out: 0.17)\n",
      "p1: newdelta: 0.0\n",
      "[PEN] t6; p1:\t\t\tsold too much (resulting amount less than demand tank LB)\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.07397808} {'j1': 0.0} {'p1': 0.0}\n",
      "    -55.769780583679676\n"
     ]
    }
   ],
   "source": [
    "with th.autograd.set_detect_anomaly(True):\n",
    "    obs = env.reset()\n",
    "    obs, obs_dict = obs\n",
    "    for k in range(env.T):\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        print(env.pen_tracker)\n",
    "        print(\"\\n\\n   \",reconstruct_dict(action, env.mapping_act))\n",
    "        obs, reward, done, term, _ = env.step(action)\n",
    "        dobs = reconstruct_dict(obs, env.mapping_obs)\n",
    "        print(\"\\n    >>     \",dobs[\"sources\"], dobs[\"blenders\"], dobs[\"demands\"])\n",
    "        print(\"   \" ,reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0336, 0.0893, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.Tensor(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 (only once per episode)\n",
    "episode_rewards = []\n",
    "obs = env.reset()\n",
    "obs, obs_dict = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 1 Get first action\n",
    "print(env.t)\n",
    "action, _ = model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'s1': 17.46205}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_blend': {'s1': {'j1': 0.0}},\n",
       " 'blend_demand': {'j1': {'p1': 30.307917}},\n",
       " 'tau': {'s1': 8.731916},\n",
       " 'delta': {'p1': 17.08481}}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Visualize action\n",
    "print(env.t)\n",
    "reconstruct_dict(action, env.mapping_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Step once: get 2nd action\n",
    "print(env.t)\n",
    "obs, reward, done, term, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'s1': 26.193966}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# 4 Visualize new state\n",
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blendv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
