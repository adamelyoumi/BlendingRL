{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN policy ?\n",
    "- grid search for HP tuning (OK)\n",
    "- Increasingly difficult Environment\n",
    "- Positive reward for populating increasingly \"deep\" blending tanks ?\n",
    "- RL for chem sched paper (https://arxiv.org/pdf/2203.00636)\n",
    "- Masking (https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html, https://arxiv.org/pdf/2006.14171)\n",
    "    - Adding binary decision variables ?g  \n",
    "    - Requires discrete action space (only integer flows -> treated as categories ?)\n",
    "    - masking: disable incoming flows (resp. outgoing flows) for tanks at UB inv limit (resp. LB inv. limit), disable selling/buying when available = 0\n",
    "    - multiple envs with multiple agents ? (MARL, https://arxiv.org/pdf/2103.01955)\n",
    "        - Predict successive pipelines (\"source > blend\" then \"blend > blend\" (as many as required) then \"blend > demand\")\n",
    "        - Each agent has access to the whole state\n",
    "        - Action mask is derived from the previous agent's actions (0 if inventory at bounds or incoming flow already reserved, else 1)\n",
    "        - https://github.com/Rohan138/marl-baselines3/blob/main/marl_baselines3/independent_ppo.py\n",
    "- Safe RL: (https://proceedings.mlr.press/v119/wachi20a/wachi20a.pdf)\n",
    "    - \"Unsafe state\" ? > Do not enforce constraints strictly, instead opt for early episode termination to show which states are unsafe ? \n",
    "    - Implementations:\n",
    "        - https://pypi.org/project/fast-safe-rl/#description (Policy optimizers)\n",
    "        - https://github.com/PKU-Alignment/safety-gymnasium/tree/main/safety_gymnasium (environments; \"cost\" ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try other learning rates/CNN policies\n",
    "2. Implement Masking with single agent\n",
    "3. Try other ways to tell the model what are illegal/unsafe states (safe RL)\n",
    "4. Try multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masking: Discretization of action space is too slow/might not work -> Need to implement masking for continuous action space\n",
    "- Recurrent policy makes the most sense ? (window of demand forecasts)\n",
    "- https://www.reddit.com/r/reinforcementlearning/comments/17l5b47/invalid_action_masking_when_action_space_is/\n",
    "    - Suggestion of autoregressive model for having constraints respected: one predicted action is input to a second model\n",
    "    - Suggestion of editing the distribution in such a way that the constraint is respected\n",
    "- https://www.sciencedirect.com/science/article/pii/S0098135420301599\n",
    "    - Choice of ELU activation ?\n",
    "    - Choice of NN size ?\n",
    "    - \"The feature engineering in the net inventory means the network does not have to learn these relationships itself, which did help speed training.\" ?\n",
    "- Simplify the problem (remove tanks 5 to 8), find the optimal solution with Gurobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove all constraints except in/out\n",
    "- https://arxiv.org/pdf/1711.11157\n",
    "- https://arxiv.org/pdf/2111.01564\n",
    "- Softmax with large coef to produce action mask\n",
    "- Graph convolution NN instead of RNN ?\n",
    "    - https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "    - Graph rep. learning - William L Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPG\n",
    "- Softmax\n",
    "- ~~Remove non-selling rewards~~\n",
    "- MultiplexNet\n",
    "- Why softmax doesn't work ? -> gradient doesn't compute properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finalize adjustment of flows\n",
    "- Add more difficulty (bigger env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "import json\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import *\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
    "\n",
    "from envs import BlendEnv, flatten_and_track_mappings, reconstruct_dict\n",
    "from models import *\n",
    "from math import exp, log\n",
    "import yaml\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( Regexp for Tensorboard coloring )\n",
    "\n",
    "(1\\\\|2\\\\|3\\\\|4\\\\|5\\\\|6\\\\|7\\\\|8\\\\|9\\\\|10\\\\|11\\\\|12\\\\|13\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/14.yaml\", \"r\") as f:\n",
    "    s = \"\".join(f.readlines())\n",
    "    cfg = yaml.load(s, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](simplest.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"clipped_std\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_std\n",
    "elif cfg[\"custom_softmax\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_softmax\n",
    "elif cfg[\"policytype\"] == \"MLP\":\n",
    "    policytype = \"MlpPolicy\"\n",
    "elif cfg[\"policytype\"] == \"MLPtanh\":\n",
    "    policytype = CustomMLP_ACP_simplest_tanh\n",
    "    \n",
    "if cfg[\"optimizer\"] == \"PPO\":\n",
    "    optimizer_cls = PPO\n",
    "elif cfg[\"optimizer\"] == \"DDPG\":\n",
    "    optimizer_cls = DDPG\n",
    "\n",
    "if cfg[\"model\"][\"act_fn\"] == \"ReLU\":\n",
    "    act_cls = th.nn.ReLU\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"tanh\":\n",
    "    act_cls = th.nn.Tanh\n",
    "elif cfg[\"model\"][\"act_fn\"] == \"sigmoid\":\n",
    "    act_cls = th.nn.Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = {\n",
    "    \"source_blend\": {\"s1\": [\"j1\"]},\n",
    "    \"blend_blend\": {\"j1\": []},\n",
    "    \"blend_demand\": {\"j1\": [\"p1\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sample = {\n",
    "    'source_blend':{'s1': {'j1':1}},\n",
    "    'blend_blend':{},\n",
    "    'blend_demand':{'j1': {'p1':1}},\n",
    "    \"tau\": {\"s1\": 10},\n",
    "    \"delta\": {\"p1\": 0}\n",
    "}\n",
    "action_sample_flat, mapp = flatten_and_track_mappings(action_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau0   = {'s1': [10, 10, 10, 0, 0, 0]}\n",
    "delta0 = {'p1': [0, 0, 0, 10, 10, 10]}\n",
    "sigma = {\"s1\":{\"q1\": 0.06}} # Source concentrations\n",
    "sigma_ub = {\"p1\":{\"q1\": 0.16}} # Demand concentrations UBs/LBs\n",
    "sigma_lb = {\"p1\":{\"q1\": 0}}\n",
    "s_inv_lb = {'s1': 0}\n",
    "s_inv_ub = {'s1': 999}\n",
    "d_inv_lb = {'p1': 0}\n",
    "d_inv_ub = {'p1': 999}\n",
    "betaT_d = {'p1': 1} # Price of sold products\n",
    "betaT_s = {'s1': cfg[\"env\"][\"product_cost\"]} # Cost of bought products\n",
    "b_inv_ub = {\"j1\": 30} \n",
    "b_inv_lb = {j:0 for j in b_inv_ub.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = False, \n",
    "               D = cfg[\"env\"][\"D\"], \n",
    "               Q = cfg[\"env\"][\"Q\"], \n",
    "               P = cfg[\"env\"][\"P\"], \n",
    "               B = cfg[\"env\"][\"B\"], \n",
    "               Z = cfg[\"env\"][\"Z\"], \n",
    "               M = cfg[\"env\"][\"M\"],\n",
    "               MAXFLOW = cfg[\"env\"][\"maxflow\"],\n",
    "               alpha = cfg[\"env\"][\"alpha\"],\n",
    "               beta = cfg[\"env\"][\"beta\"],\n",
    "               connections = connections, \n",
    "               action_sample = action_sample,\n",
    "               tau0 = tau0,delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "\n",
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, \n",
    "                   norm_obs=cfg[\"obs_normalizer\"], \n",
    "                   norm_reward=cfg[\"reward_normalizer\"])\n",
    "env = VecCheckNan(env, raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"], \n",
    "                   vf = [cfg[\"model\"][\"arch_layersize\"]] * cfg[\"model\"][\"arch_n\"])],\n",
    "    activation_fn = act_cls,\n",
    "    log_std_init = cfg[\"model\"][\"log_std_init\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpPolicy\n"
     ]
    }
   ],
   "source": [
    "print(policytype)\n",
    "model = optimizer_cls(policytype, \n",
    "                    env,\n",
    "                    tensorboard_log = \"./logs\",\n",
    "                    clip_range = cfg[\"model\"][\"clip_range\"],\n",
    "                    learning_rate = cfg[\"model\"][\"lr\"],\n",
    "                    ent_coef = cfg[\"model\"][\"ent_coef\"],\n",
    "                    policy_kwargs = policy_kwargs)\n",
    "\n",
    "if cfg[\"starting_point\"]:\n",
    "    model.set_parameters(cfg[\"starting_point\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "if type(model.policy) == CustomRNN_ACP:\n",
    "    policytype = \"CRNN\"\n",
    "elif type(model.policy) == CustomMLP_ACP_simplest_std:\n",
    "    policytype = \"CMLP\"\n",
    "else:\n",
    "    policytype = \"MLP\"\n",
    "    \n",
    "entcoef = str(model.ent_coef) if type(model) == PPO else \"\"\n",
    "cliprange = str(model.clip_range(0)) if type(model) == PPO else \"\"\n",
    "model_name = f\"models/{cfg['id']}/{cfg['id']}_simplest_{datetime.datetime.now().strftime('%m%d-%H%M')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_stds = []\n",
    "        self.total_rewards = []\n",
    "        self.signal = True\n",
    "        self.update1 = True\n",
    "        self.print_flag = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        log_std: th.Tensor = self.model.policy.log_std\n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        \n",
    "        if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "            self.print_flag = True\n",
    "            \n",
    "        if self.print_flag:\n",
    "            print(\"\\nt:\", t)\n",
    "            if np.isnan(self.locals['rewards'][0]) or np.isinf(self.locals['rewards'][0]):\n",
    "                print(f\"is invalid reward {self.locals['rewards'][0]}\")\n",
    "            for i in ['obs_tensor', 'actions', 'values', 'clipped_actions', 'new_obs', 'rewards']:\n",
    "                if i in self.locals:\n",
    "                    print(f\"{i}: \" + str(self.locals[i]))\n",
    "            if t == 6:\n",
    "                stds = th.exp(self.model.policy.log_std).mean().item()\n",
    "        \n",
    "                if stds > 50:\n",
    "                    print(\"clipping log-stds\")\n",
    "                    print(\"before: \", self.model.policy.log_std)\n",
    "                    self.model.policy.log_std = nn.Parameter( 2*th.ones(self.model.policy.log_std.shape, requires_grad=True) )\n",
    "                    print(\"after: \",  self.model.policy.log_std)\n",
    "                    \n",
    "                self.logger.record('train/learning_rate', self.model.learning_rate)\n",
    "                self.logger.record('train/clip_range', self.model.clip_range(0))\n",
    "                self.logger.record(\"train/std\", th.exp(self.model.policy.log_std).mean().item())\n",
    "                \n",
    "                # if self.locals['rewards'][0] > 200 and self.update1:\n",
    "                #     # self.training_env.set_attr('', 1e2)\n",
    "                #     self.model.learning_rate = 1e2\n",
    "                #     self.model.clip_range = 5e-2\n",
    "                #     self.update1 = False\n",
    "                \n",
    "                self.print_flag = False\n",
    "                print(f\"\\n\\nLog-Std at step {self.num_timesteps}: {log_std.detach().numpy()}\")\n",
    "                self.log_stds.append(log_std.mean().item())\n",
    "                self.total_rewards.append(self.locals['rewards'][0])\n",
    "                print(f\"\\nAvg rewards over the last 100 episodes:{sum(self.total_rewards[-100:])/100} ; last reward: {self.total_rewards[-1]}\")\n",
    "                self.model.learning_rate\n",
    "                print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "                \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/14/14_simplest_0701-1737'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_callback = CustomLoggingCallback()\n",
    "callback = CallbackList([log_callback])\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging at 14/14_simplest_0701-1737\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-0.04118691  0.08414864 -0.00901018  0.03308134]]\n",
      "values: tensor([[-0.1208]])\n",
      "clipped_actions: [[0.         0.08414864 0.         0.03308134]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-10.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-0.05506805 -0.0168103   0.17698206 -0.06549282]]\n",
      "values: tensor([[0.8756]])\n",
      "clipped_actions: [[0.         0.         0.17698206 0.        ]]\n",
      "new_obs: [[ 0.17698206  0.          0.          0.         10.          0.\n",
      "   0.         10.          0.         10.          0.         10.\n",
      "   0.          0.          0.          0.          2.        ]]\n",
      "rewards: [-2.0201461]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.1770,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 0.15680929 -0.02132179  0.12586474  0.02681558]]\n",
      "values: tensor([[1.1400]])\n",
      "clipped_actions: [[0.15680929 0.         0.12586474 0.02681558]]\n",
      "new_obs: [[ 0.14603752  0.15680929  0.          0.06        0.         10.\n",
      "   0.         10.          0.         10.          0.          0.\n",
      "   0.          0.          0.          0.          3.        ]]\n",
      "rewards: [-1.4696987]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.1460,  0.1568,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[-0.18406983 -0.24293111  0.22260319  0.03491477]]\n",
      "values: tensor([[-1.1081]])\n",
      "clipped_actions: [[0.         0.         0.22260319 0.03491477]]\n",
      "new_obs: [[ 0.14603752  0.15680929  0.          0.06        0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [-1.351003]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.1460,  0.1568,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[-0.03027973 -0.03699247 -0.14119253  0.16099013]]\n",
      "values: tensor([[-0.8272]])\n",
      "clipped_actions: [[0.         0.         0.         0.16099013]]\n",
      "new_obs: [[ 0.14603752  0.15680929  0.          0.06        0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [-1.0589064]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.1460,  0.1568,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[-0.18465321 -0.03119456 -0.111725    0.04174531]]\n",
      "values: tensor([[-0.7737]])\n",
      "clipped_actions: [[0.         0.         0.         0.04174531]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.88721746]\n",
      "\n",
      "\n",
      "Log-Std at step 6: [-2. -2. -2. -2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.008872174620628358 ; last reward: -0.8872174620628357\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 5.241441 -8.350325 -6.78055   8.462848]]\n",
      "values: tensor([[-2.6564]])\n",
      "clipped_actions: [[5.241441 0.       0.       8.462848]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.15090464]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  7.199469 -12.297416 -10.545675  13.143745]]\n",
      "values: tensor([[-2.5872]])\n",
      "clipped_actions: [[ 7.199469  0.        0.       13.143745]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.30184507]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[  7.833745 -10.827508  -9.308249  11.882893]]\n",
      "values: tensor([[-2.6452]])\n",
      "clipped_actions: [[ 7.833745  0.        0.       11.882893]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.45286992]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[  7.7228093 -11.264669   -7.627126   10.989179 ]]\n",
      "values: tensor([[-2.2769]])\n",
      "clipped_actions: [[ 7.7228093  0.         0.        10.989179 ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.6794434]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 5.476499  -6.977632  -4.8003464  6.7018843]]\n",
      "values: tensor([[-1.8584]])\n",
      "clipped_actions: [[5.476499  0.        0.        6.7018843]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.83033365]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ 3.551559  -3.5463927 -3.1211271  4.546273 ]]\n",
      "values: tensor([[-0.8591]])\n",
      "clipped_actions: [[3.551559 0.       0.       4.546273]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.9804166]\n",
      "\n",
      "\n",
      "Log-Std at step 2058: [-0.8640469  -0.39845198 -1.5512875  -0.8657037 ]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.018676340579986572 ; last reward: -0.9804165959358215\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-23.669249 -40.98287   59.078438  52.674343]]\n",
      "values: tensor([[-3.0998]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -17.845875 -144.73865    91.19495   121.20975 ]]\n",
      "values: tensor([[-0.9119]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-376.52054 -837.96783  539.11053  872.2309 ]]\n",
      "values: tensor([[-0.8118]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -677.0458 -1357.1055   879.4349  1284.521 ]]\n",
      "values: tensor([[-0.9437]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.21176374]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -557.9785 -1222.4907   789.0767  1236.8627]]\n",
      "values: tensor([[-1.1002]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.42356068]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-337.68015 -718.0281   463.76422  667.6819 ]]\n",
      "values: tensor([[-1.0342]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.63541836]\n",
      "\n",
      "\n",
      "Log-Std at step 4104: [3.442578  3.2225778 1.2669138 3.480737 ]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.025030524134635926 ; last reward: -0.6354183554649353\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-236.60612  427.03607 -588.93884 4585.817  ]]\n",
      "values: tensor([[-1.1398]])\n",
      "clipped_actions: [[ 0. 50.  0. 50.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [-0.14736478]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -289.27603   563.11145  -582.4991  -1228.2024 ]]\n",
      "values: tensor([[-1.6316]])\n",
      "clipped_actions: [[ 0. 50.  0.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [-0.22105856]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -340.4047    326.14563  -532.5641  -1915.8007 ]]\n",
      "values: tensor([[-2.1481]])\n",
      "clipped_actions: [[ 0. 50.  0.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [-0.2947663]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 559.7983     40.436935 -497.00043   393.88672 ]]\n",
      "values: tensor([[-2.4960]])\n",
      "clipped_actions: [[50.       40.436935  0.       50.      ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [-0.5895772]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 810.4031  -970.28827 -364.55637 1413.0847 ]]\n",
      "values: tensor([[-1.8897]])\n",
      "clipped_actions: [[50.  0.  0. 50.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [-0.8106597]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[1280.7753  -151.18427 -206.94131  935.0232 ]]\n",
      "values: tensor([[-0.9608]])\n",
      "clipped_actions: [[50.  0.  0. 50.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-1.031471]\n",
      "clipping log-stds\n",
      "before:  Parameter containing:\n",
      "tensor([6.2944, 6.4150, 3.2676, 7.0010], requires_grad=True)\n",
      "after:  Parameter containing:\n",
      "tensor([2., 2., 2., 2.], requires_grad=True)\n",
      "\n",
      "\n",
      "Log-Std at step 6150: [6.294388  6.4150205 3.2676182 7.001036 ]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.0353452342748642 ; last reward: -1.0314710140228271\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 230.11191 -261.48853  194.60202  276.25607]]\n",
      "values: tensor([[-2.9115]])\n",
      "clipped_actions: [[50.  0. 50. 50.]]\n",
      "new_obs: [[ 0.   10.    0.    0.06 10.    0.   10.    0.    0.   10.    0.   10.\n",
      "   0.   10.    0.    0.    1.  ]]\n",
      "rewards: [0.07256047]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.0000, 10.0000,  0.0000,  0.0600, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 200.68199 -206.24416  149.79594  234.4288 ]]\n",
      "values: tensor([[-1.6129]])\n",
      "clipped_actions: [[50.  0. 50. 50.]]\n",
      "new_obs: [[ 0.   20.    0.    0.09 10.    0.    0.   10.    0.   10.    0.   10.\n",
      "   0.    0.    0.    0.    2.  ]]\n",
      "rewards: [0.1451155]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.0000, 20.0000,  0.0000,  0.0900, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 134.2193  -163.57341  111.12689  173.63733]]\n",
      "values: tensor([[-0.6897]])\n",
      "clipped_actions: [[50.  0. 50. 50.]]\n",
      "new_obs: [[ 0.   30.    0.    0.11  0.   10.    0.   10.    0.   10.    0.    0.\n",
      "   0.    0.    0.    0.    3.  ]]\n",
      "rewards: [0.2176572]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.0000, 30.0000,  0.0000,  0.1100,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 162.35957 -188.48943  133.2544   195.78857]]\n",
      "values: tensor([[-0.8873]])\n",
      "clipped_actions: [[50.  0. 50. 50.]]\n",
      "new_obs: [[ 0.   30.    0.    0.11  0.   10.    0.   10.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    4.  ]]\n",
      "rewards: [-0.07254805]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.0000, 30.0000,  0.0000,  0.1100,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 331.92426 -409.35287  311.10086  402.27216]]\n",
      "values: tensor([[-0.9032]])\n",
      "clipped_actions: [[50.  0. 50. 50.]]\n",
      "new_obs: [[ 0.   30.    0.    0.11  0.   10.    0.    0.    0.    0.    0.    0.\n",
      "   0.    0.    0.    0.    5.  ]]\n",
      "rewards: [-0.36273828]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.0000, 30.0000,  0.0000,  0.1100,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 372.87878 -441.7313   318.3474   444.0517 ]]\n",
      "values: tensor([[-0.5357]])\n",
      "clipped_actions: [[50.  0. 50. 50.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.6529627]\n",
      "\n",
      "\n",
      "Log-Std at step 8202: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.04187486112117767 ; last reward: -0.6529626846313477\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-36.325676 -15.168469  27.114416   3.405178]]\n",
      "values: tensor([[-0.6285]])\n",
      "clipped_actions: [[ 0.        0.       27.114416  3.405178]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-205.03816  -123.70824   163.85857   -14.983046]]\n",
      "values: tensor([[-0.7546]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.07159425]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-465.93082  -256.20795   363.8126    -46.572308]]\n",
      "values: tensor([[-0.9202]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.14318804]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-289.9394  -167.10977  249.4929   -41.39511]]\n",
      "values: tensor([[-1.0587]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.07159328]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-184.79538  -114.40211   167.30023   -34.357067]]\n",
      "values: tensor([[-0.9408]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-126.55236   -83.67896   109.37575   -22.841856]]\n",
      "values: tensor([[-0.7335]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [-0.07159237]\n",
      "\n",
      "\n",
      "Log-Std at step 10248: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: -0.07159236818552017\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-3155.6448 -2641.1826  1979.6533 -1947.2972]]\n",
      "values: tensor([[0.2412]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.07047921]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-17917.732 -15061.149  11226.371 -10909.164]]\n",
      "values: tensor([[0.2533]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.14096025]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-44778.07  -37590.54   28059.73  -27189.592]]\n",
      "values: tensor([[0.1814]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.21143979]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-56895.145 -47808.496  35698.332 -34582.426]]\n",
      "values: tensor([[0.0634]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.14095773]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-52353.332 -44008.418  32885.574 -31799.04 ]]\n",
      "values: tensor([[0.0025]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.07047736]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-46510.59  -39136.355  29269.729 -28265.457]]\n",
      "values: tensor([[-0.0166]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 12294: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -3560.1748 -14911.833   13725.77   -12989.646 ]]\n",
      "values: tensor([[0.5891]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.07071681]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -25419.916 -106691.86    98103.86   -92758.516]]\n",
      "values: tensor([[0.5356]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.14143626]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -69649.05 -292320.97  268868.16 -254137.73]]\n",
      "values: tensor([[0.4089]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.21215594]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-117068.25 -491187.5   451747.97 -426916.5 ]]\n",
      "values: tensor([[0.2059]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.14143701]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-118461.75 -496923.03  457023.78 -431860.1 ]]\n",
      "values: tensor([[0.0696]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.07071804]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-106483.63 -446618.53  410741.75 -388094.44]]\n",
      "values: tensor([[0.0014]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 14346: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -6781.0405  -5065.477   10682.345  -13842.678 ]]\n",
      "values: tensor([[0.6274]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.07193676]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -9160.311  -6831.637  14401.319 -18671.951]]\n",
      "values: tensor([[0.5612]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.14387645]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-112576.95  -83900.02  177069.6  -229343.38]]\n",
      "values: tensor([[0.4246]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.21581711]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-238851.05 -177900.52  375533.56 -486427.47]]\n",
      "values: tensor([[0.2134]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.14387865]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-249393.23 -185725.2   392071.4  -507854.66]]\n",
      "values: tensor([[0.0713]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.07193933]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-215751.12 -160660.77  339200.53 -439359.5 ]]\n",
      "values: tensor([[0.0004]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 16392: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-23540.045 -33708.613  26164.01  -56172.605]]\n",
      "values: tensor([[0.6393]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.07359581]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -67977.12   -97271.875   75491.76  -162087.19 ]]\n",
      "values: tensor([[0.5733]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.14719462]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-338141.97 -483803.44  375493.28 -806254.06]]\n",
      "values: tensor([[0.4318]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.22079481]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -719264.3  -1028973.6    798613.44 -1714852.4 ]]\n",
      "values: tensor([[0.2171]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.14719754]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -759662.1 -1086780.6   843459.8 -1811188.9]]\n",
      "values: tensor([[0.0727]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.07359904]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -647445.4   -926218.94   718852.44 -1543602.2 ]]\n",
      "values: tensor([[-0.0004]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 18438: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-155384.61 -211535.98  184152.77 -332126.88]]\n",
      "values: tensor([[0.6606]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.07546896]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-321802.28 -438067.8   381360.25 -687803.8 ]]\n",
      "values: tensor([[0.5902]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.1509409]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -851000.7 -1158462.6  1008477.8 -1818878.9]]\n",
      "values: tensor([[0.4436]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.22641441]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-2916514.5 -3969968.5  3456008.8 -6233438.5]]\n",
      "values: tensor([[0.2238]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.1509442]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-3260056.5 -4437549.5  3863062.  -6967653. ]]\n",
      "values: tensor([[0.0747]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.0754725]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-3077558.5 -4189119.   3646799.2 -6577598. ]]\n",
      "values: tensor([[-0.0002]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 20490: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-179642.31 -102731.62  147938.66 -257012.55]]\n",
      "values: tensor([[0.6720]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.07743311]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-321153.34 -183690.48  264487.75 -459491.3 ]]\n",
      "values: tensor([[0.6041]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.15486912]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-1474993.4  -843355.7  1214515.6 -2110416.5]]\n",
      "values: tensor([[0.4548]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.23230685]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-4474104.5 -2557925.2  3683800.  -6401500.5]]\n",
      "values: tensor([[0.2290]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.15487263]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-4861940.  -2779608.2  4003083.8 -6956390. ]]\n",
      "values: tensor([[0.0765]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.0774368]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-4496144.5 -2570495.5  3701908.  -6433043. ]]\n",
      "values: tensor([[0.0010]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 22536: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-268816.53 -184864.08  346196.66 -672026.75]]\n",
      "values: tensor([[0.6858]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.07943459]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-326077.5  -224260.16  419958.53 -815202.8 ]]\n",
      "values: tensor([[0.6184]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.15887201]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-2040217.1 -1402918.5  2627422.5 -5100600. ]]\n",
      "values: tensor([[0.4658]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.23831122]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -5503055.   -3783912.8   7086773.5 -13757722. ]]\n",
      "values: tensor([[0.2334]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.1588756]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -6078946.  -4179866.   7828338. -15197415.]]\n",
      "values: tensor([[0.0784]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.07943835]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -5734193.5  -3942791.2   7384361.  -14335517. ]]\n",
      "values: tensor([[-0.0011]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 24582: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-142772.1    -47465.793  201953.36  -345187.62 ]]\n",
      "values: tensor([[0.7115]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.0814465]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -681019.25  -226421.53   963286.06 -1646562.1 ]]\n",
      "values: tensor([[0.6360]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.16289575]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -6780745.   -2253908.2   9590755.  -16394332. ]]\n",
      "values: tensor([[0.4787]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.24434683]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-14896044.  -4951323.  21068956. -36015232.]]\n",
      "values: tensor([[0.2424]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.16289939]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-15762974.   -5239470.5  22295118.  -38111248. ]]\n",
      "values: tensor([[0.0806]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.08145027]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-14416951.   -4792052.5  20391298.  -34856860. ]]\n",
      "values: tensor([[0.0008]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 26634: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-203084.95  -36490.67  284813.2  -651266.  ]]\n",
      "values: tensor([[0.7254]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.08343823]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -817952.4   -146984.77  1147161.   -2623175.5 ]]\n",
      "values: tensor([[0.6460]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.16687915]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-10731097.   -1927918.4  15049774.  -34415136. ]]\n",
      "values: tensor([[0.4886]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.2503219]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[-23541688.  -4229301.  33015866. -75499272.]]\n",
      "values: tensor([[0.2444]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.16688277]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-24990186.   -4489509.5  35047264.  -80144656. ]]\n",
      "values: tensor([[0.0823]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.08344199]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[-23206496.   -4169040.8  32545748.  -74424280. ]]\n",
      "values: tensor([[6.8396e-05]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 28680: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -67879.43  -31208.28  269802.16 -449841.47]]\n",
      "values: tensor([[0.7368]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.08540477]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ -205865.7    -94648.74   818362.06 -1364411.1 ]]\n",
      "values: tensor([[0.6618]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.17081213]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -3877397.2  -1782054.9  15413000.  -25698122. ]]\n",
      "values: tensor([[0.5007]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.2562213]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -9679450.   -4448551.5  38476540.  -64152072. ]]\n",
      "values: tensor([[0.2494]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.17081574]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[-10527242.  -4838149.  41846588. -69770976.]]\n",
      "values: tensor([[0.0840]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.08540848]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -9898254.  -4549069.  39346392. -65602380.]]\n",
      "values: tensor([[-0.0026]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 30726: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ -52393.547  -45454.52   133010.22  -200863.88 ]]\n",
      "values: tensor([[0.7488]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.08734542]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-143220.75 -124266.47  363627.88 -549132.8 ]]\n",
      "values: tensor([[0.6693]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.17469338]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ -2893546.8  -2509858.5   7345953.  -11094513. ]]\n",
      "values: tensor([[0.5063]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.26204312]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -7977900.   -6919904.5  20253724.  -30589074. ]]\n",
      "values: tensor([[0.2515]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.17469695]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -8777696.   -7613637.5  22284180.  -33655688. ]]\n",
      "values: tensor([[0.0856]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.08734909]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -8410619.   -7295214.5  21352270.  -32248220. ]]\n",
      "values: tensor([[-0.0011]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 32778: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[-14500.016   -2690.7346  43240.26   -53268.8   ]]\n",
      "values: tensor([[0.7737]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.08924536]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[-17163.398   -3186.7312  51121.285  -63001.227 ]]\n",
      "values: tensor([[0.6947]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.1784932]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[-1560238.1   -288106.84  4653657.5  -5732341.  ]]\n",
      "values: tensor([[0.5257]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.2677428]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ -6816961.5  -1258560.   20332476.  -25045480. ]]\n",
      "values: tensor([[0.2630]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.17849672]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ -7878060.5  -1454421.4  23497362.  -28943978. ]]\n",
      "values: tensor([[0.0882]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.08924898]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ -7811622.5  -1442135.8  23299144.  -28699854. ]]\n",
      "values: tensor([[-6.1162e-05]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 34824: [2. 2. 2. 2.]\n",
      "\n",
      "Avg rewards over the last 100 episodes:-0.042590784803032876 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m logpath \u001b[38;5;241m=\u001b[39m model_name[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m):]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogging at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:224\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    221\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    222\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[1;32m--> 224\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32mc:\\Users\\adame\\OneDrive\\Bureau\\CODE\\BlendingRL\\blendv\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:470\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[1;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Reshape to handle multi-dim and discrete action spaces, see GH #970 #1392\u001b[39;00m\n\u001b[0;32m    468\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim))\n\u001b[1;32m--> 470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(reward)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logpath = model_name[len(\"models/\"):]\n",
    "print(f\"logging at {logpath}\")\n",
    "model.learn(total_timesteps = 100000, \n",
    "            progress_bar = False, \n",
    "            tb_log_name = logpath, \n",
    "            callback = callback,\n",
    "            reset_num_timesteps = False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt.plot([k*2048 for k in range(len(log_std_callback.log_stds))], [k for k in log_std_callback.log_stds])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average action log std\")\n",
    "plt.title(model_name.split(\"/\")[-1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M,Q,P,B,Z,D = 10, 0, 5, 5, 1, 0\n",
    "M, Q, P, B, Z, D  = cfg[\"env\"][\"M\"], cfg[\"env\"][\"Q\"], cfg[\"env\"][\"P\"], cfg[\"env\"][\"B\"], cfg[\"env\"][\"Z\"], 0\n",
    "# M,Q,P,B,Z,D = 0, 0, 0, 0, 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = True, \n",
    "               M = M, Q = Q, P = P, B = B, Z = Z, D = D, \n",
    "               action_sample = action_sample, \n",
    "               connections = connections, \n",
    "               tau0 = tau0,\n",
    "               delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub,\n",
    "               sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb,\n",
    "               s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb,\n",
    "               d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d,\n",
    "               betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t1; s1:\t\t\tbought too much (more than supply)\n",
      "s1: b: 0.2\n",
      "[PEN] t1; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0, in_flow_sources: 10.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0.0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 10.0} {'p1': 0.0}\n",
      "    -2.1\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 50.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t2; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0.0 through tank population in s1\n",
      "j1: inv: 10.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 50.0\n",
      "j1: b: 0.2\n",
      "[PEN] t2; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0.0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 10.0} {'j1': 0.0} {'p1': 10.0}\n",
      "    -4.199999999999999\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t3; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0.0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 20.0} {'j1': 0.0} {'p1': 10.0}\n",
      "    -5.199999999999999\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 50.0}}\n",
      "[PEN] t4; s1:\t\t\tbought too much (more than supply)\n",
      "[PEN] t4; p1:\t\t\tsold too much (more than demand)\n",
      "s1: b: 0.4\n",
      "[PEN] t4; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 20.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0.0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 20.0} {'p1': 0.0}\n",
      "    991.6999999999999\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 50.0}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 50.0}}\n",
      "[PEN] t5; p1:\t\t\tsold too much (more than demand)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 20.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 50.0\n",
      "j1: b: 0.4\n",
      "[PEN] t5; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0.0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 0.0} {'p1': 10.0}\n",
      "    1989.6\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 0.0}, 'delta': {'p1': 50.0}}\n",
      "[PEN] t6; p1:\t\t\tsold too much (more than demand)\n",
      "s1: b: 0.0\n",
      "[PEN] t6; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 0.0} {'p1': 0.0}\n",
      "    2987.6\n"
     ]
    }
   ],
   "source": [
    "with th.autograd.set_detect_anomaly(True):\n",
    "    obs = env.reset()\n",
    "    obs, obs_dict = obs\n",
    "    for k in range(env.T):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        print(\"\\n\\n   \",reconstruct_dict(action, env.mapping_act))\n",
    "        obs, reward, done, term, _ = env.step(action)\n",
    "        dobs = reconstruct_dict(obs, env.mapping_obs)\n",
    "        print(\"\\n    >>     \",dobs[\"sources\"], dobs[\"blenders\"], dobs[\"demands\"])\n",
    "        print(\"   \" ,reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 (only once per episode)\n",
    "episode_rewards = []\n",
    "obs = env.reset()\n",
    "obs, obs_dict = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 1 Get first action\n",
    "print(env.t)\n",
    "action, _ = model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'s1': 17.46205}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_blend': {'s1': {'j1': 0.0}},\n",
       " 'blend_demand': {'j1': {'p1': 30.307917}},\n",
       " 'tau': {'s1': 8.731916},\n",
       " 'delta': {'p1': 17.08481}}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Visualize action\n",
    "print(env.t)\n",
    "reconstruct_dict(action, env.mapping_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Step once: get 2nd action\n",
    "print(env.t)\n",
    "obs, reward, done, term, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'s1': 26.193966}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# 4 Visualize new state\n",
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blendv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
