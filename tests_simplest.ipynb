{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CNN policy ?\n",
    "- grid search for HP tuning (OK)\n",
    "- Increasingly difficult Environment\n",
    "- Positive reward for populating increasingly \"deep\" blending tanks ?\n",
    "- RL for chem sched paper (https://arxiv.org/pdf/2203.00636)\n",
    "- Masking (https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html, https://arxiv.org/pdf/2006.14171)\n",
    "    - Adding binary decision variables ?g  \n",
    "    - Requires discrete action space (only integer flows -> treated as categories ?)\n",
    "    - masking: disable incoming flows (resp. outgoing flows) for tanks at UB inv limit (resp. LB inv. limit), disable selling/buying when available = 0\n",
    "    - multiple envs with multiple agents ? (MARL, https://arxiv.org/pdf/2103.01955)\n",
    "        - Predict successive pipelines (\"source > blend\" then \"blend > blend\" (as many as required) then \"blend > demand\")\n",
    "        - Each agent has access to the whole state\n",
    "        - Action mask is derived from the previous agent's actions (0 if inventory at bounds or incoming flow already reserved, else 1)\n",
    "        - https://github.com/Rohan138/marl-baselines3/blob/main/marl_baselines3/independent_ppo.py\n",
    "- Safe RL: (https://proceedings.mlr.press/v119/wachi20a/wachi20a.pdf)\n",
    "    - \"Unsafe state\" ? > Do not enforce constraints strictly, instead opt for early episode termination to show which states are unsafe ? \n",
    "    - Implementations:\n",
    "        - https://pypi.org/project/fast-safe-rl/#description (Policy optimizers)\n",
    "        - https://github.com/PKU-Alignment/safety-gymnasium/tree/main/safety_gymnasium (environments; \"cost\" ?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Try other learning rates/CNN policies\n",
    "2. Implement Masking with single agent\n",
    "3. Try other ways to tell the model what are illegal/unsafe states (safe RL)\n",
    "4. Try multiple agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Masking: Discretization of action space is too slow/might not work -> Need to implement masking for continuous action space\n",
    "- Recurrent policy makes the most sense ? (window of demand forecasts)\n",
    "- https://www.reddit.com/r/reinforcementlearning/comments/17l5b47/invalid_action_masking_when_action_space_is/\n",
    "    - Suggestion of autoregressive model for having constraints respected: one predicted action is input to a second model\n",
    "    - Suggestion of editing the distribution in such a way that the constraint is respected\n",
    "- https://www.sciencedirect.com/science/article/pii/S0098135420301599\n",
    "    - Choice of ELU activation ?\n",
    "    - Choice of NN size ?\n",
    "    - \"The feature engineering in the net inventory means the network does not have to learn these relationships itself, which did help speed training.\" ?\n",
    "- Simplify the problem (remove tanks 5 to 8), find the optimal solution with Gurobi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- remove all constraints except in/out\n",
    "- https://arxiv.org/pdf/1711.11157\n",
    "- https://arxiv.org/pdf/2111.01564\n",
    "- Softmax with large coef to produce action mask\n",
    "- Graph convolution NN instead of RNN ?\n",
    "    - https://pytorch-geometric.readthedocs.io/en/latest/\n",
    "    - Graph rep. learning - William L Hamilton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latest Model learned in/out rule\n",
    "Watch out, bounds aren't properly respected (neg flows sometimes)\n",
    "Fix it properly without adding penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DDPG\n",
    "- Softmax\n",
    "- ~~Remove non-selling rewards~~\n",
    "- MultiplexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why softmax doesn't work ?\n",
    "\n",
    "Add difficulty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "import json\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO, DDPG\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, VecCheckNan\n",
    "\n",
    "from envs import BlendEnv, flatten_and_track_mappings, reconstruct_dict\n",
    "from models import CustomRNN_ACP, CustomMLP_ACP, CustomMLP_ACP_simplest_softmax, CustomMLP_ACP_simplest_std\n",
    "from math import exp, log\n",
    "import yaml\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"configs/1.yaml\", \"r\") as f:\n",
    "    s = \"\".join(f.readlines())\n",
    "    cfg = yaml.load(s, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](simplest.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# th.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = {\n",
    "    \"source_blend\": {\"s1\": [\"j1\"]},\n",
    "    \"blend_blend\": {\"j1\": []},\n",
    "    \"blend_demand\": {\"j1\": [\"p1\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sample = {\n",
    "    'source_blend':{'s1': {'j1':1}},\n",
    "    'blend_blend':{},\n",
    "    'blend_demand':{'j1': {'p1':1}},\n",
    "    \"tau\": {\"s1\": 10},\n",
    "    \"delta\": {\"p1\": 0}\n",
    "}\n",
    "action_sample_flat, mapp = flatten_and_track_mappings(action_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau0   = {'s1': [10, 10, 10, 0, 0, 0]}\n",
    "delta0 = {'p1': [0, 0, 0, 10, 10, 10]}\n",
    "sigma = {\"s1\":{\"q1\": 0.06}} # Source concentrations\n",
    "sigma_ub = {\"p1\":{\"q1\": 0.16}} # Demand concentrations UBs/LBs\n",
    "sigma_lb = {\"p1\":{\"q1\": 0}}\n",
    "s_inv_lb = {'s1': 0}\n",
    "s_inv_ub = {'s1': 999}\n",
    "d_inv_lb = {'p1': 0}\n",
    "d_inv_ub = {'p1': 999}\n",
    "betaT_d = {'p1': 1} # Price of sold products\n",
    "betaT_s = {'s1': cfg[\"env\"][\"product_cost\"]} # Cost of bought products\n",
    "b_inv_ub = {\"j1\": 30} \n",
    "b_inv_lb = {j:0 for j in b_inv_ub.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(p):\n",
    "    if p > 0.9:\n",
    "        return 4e-2\n",
    "    if p > 0.75:\n",
    "        return 2e-2\n",
    "    if p > 0.4:\n",
    "        return 5e-3\n",
    "    else:\n",
    "        return 1e-3\n",
    "    \n",
    "def lr_scheduler_mult(p):\n",
    "    if p > 0.9:\n",
    "        return 4e-4\n",
    "    if p > 0.75:\n",
    "        return 2e-4\n",
    "    if p > 0.4:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = False, \n",
    "               D = cfg[\"env\"][\"D\"], \n",
    "               Q = cfg[\"env\"][\"Q\"], \n",
    "               P = cfg[\"env\"][\"P\"], \n",
    "               B = cfg[\"env\"][\"B\"], \n",
    "               Z = cfg[\"env\"][\"Z\"], \n",
    "               M = cfg[\"env\"][\"M\"],\n",
    "               connections = connections, \n",
    "               action_sample = action_sample,\n",
    "               tau0 = tau0,delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub, sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb, s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb, d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d, betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "\n",
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(env, \n",
    "                   norm_obs=cfg[\"obs_normalizer\"], \n",
    "                   norm_reward=cfg[\"reward_normalizer\"])\n",
    "env = VecCheckNan(env, raise_exception=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[cfg[\"model\"][\"arch_layersize\"]]*cfg[\"model\"][\"arch_n\"], \n",
    "                   vf=[cfg[\"model\"][\"arch_layersize\"]]*cfg[\"model\"][\"arch_n\"])],\n",
    "    activation_fn = th.nn.ReLU,\n",
    "    log_std_init = cfg[\"model\"][\"log_std_init\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg[\"clipped_std\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_std\n",
    "elif cfg[\"custom_softmax\"]:\n",
    "    policytype = CustomMLP_ACP_simplest_softmax\n",
    "elif cfg[\"policytype\"] == \"MLP\":\n",
    "    policytype = \"MlpPolicy\"\n",
    "    \n",
    "if cfg[\"optimizer\"] == \"PPO\":\n",
    "    optimizer_cls = PPO\n",
    "elif cfg[\"optimizer\"] == \"DDPG\":\n",
    "    optimizer_cls = DDPG\n",
    "    \n",
    "model = optimizer_cls(policytype, \n",
    "                    env,\n",
    "                    tensorboard_log = \"./logs\",\n",
    "                    clip_range = cfg[\"model\"][\"clip_range\"],\n",
    "                    learning_rate = cfg[\"model\"][\"lr\"],\n",
    "                    ent_coef = cfg[\"model\"][\"ent_coef\"],\n",
    "                    policy_kwargs = policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.set_parameters(\"models\\\\simplest_model_0606-1629_ent_0.001_gam_0.99_clip_0.3_1000_1000_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "modeltype = \"PPO\" if type(model) == PPO else \"DDPG\"\n",
    "\n",
    "if type(model.policy) == CustomRNN_ACP:\n",
    "    policytype = \"CRNN\"\n",
    "elif type(model.policy) == CustomMLP_ACP_simplest_std:\n",
    "    policytype = \"CMLP\"\n",
    "else:\n",
    "    policytype = \"MLP\"\n",
    "    \n",
    "entcoef = str(model.ent_coef) if type(model) == PPO else \"\"\n",
    "cliprange = str(model.clip_range(0)) if type(model) == PPO else \"\"\n",
    "model_name = f\"models/{cfg['id']}_simplest_{datetime.datetime.now().strftime('%m%d-%H%M')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogStdCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(LogStdCallback, self).__init__(verbose)\n",
    "        self.log_stds = []\n",
    "        self.total_rewards = []\n",
    "        self.signal = True\n",
    "        self.update1 = True\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        log_std: th.Tensor = self.model.policy.log_std\n",
    "            \n",
    "        t = self.locals[\"infos\"][0]['dict_state']['t']\n",
    "        \n",
    "        # if self.locals['rewards'][0] > 200 and self.update1:\n",
    "        #     self.model.learning_rate = 5e-4\n",
    "        #     self.model.clip_range /= 2\n",
    "        #     self.update1 = False\n",
    "        \n",
    "        if self.num_timesteps%2048 < 6 and t == 1: # start printing\n",
    "            self.print_flag = True\n",
    "            \n",
    "        if self.print_flag:\n",
    "            print(\"\\nt:\", t)\n",
    "            for i in ['obs_tensor', 'actions', 'values', 'clipped_actions', 'new_obs', 'rewards']:\n",
    "                if i in self.locals:\n",
    "                    print(f\"{i}: \" + str(self.locals[i]))\n",
    "            \n",
    "            if t == 6:\n",
    "                self.print_flag = False\n",
    "                \n",
    "                print(f\"\\n\\nLog-Std at step {self.num_timesteps}: {log_std.detach().numpy()}\")\n",
    "                self.log_stds.append(log_std.mean().item())\n",
    "                self.total_rewards.append(self.locals['rewards'][0])\n",
    "                print(f\"\\nAvg rewards so far:{sum(self.total_rewards)/len(self.total_rewards)} ; last reward: {self.total_rewards[-1]}\")\n",
    "                self.model.learning_rate\n",
    "                print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "                 \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/1_simplest_0621-1332'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_std_callback = LogStdCallback()\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging at models/7_simplest_0621-0253\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[-1.4491]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[0.00162236 0.         0.         0.        ]]\n",
      "values: tensor([[0.4221]])\n",
      "clipped_actions: [[0.00162236 0.         0.         0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[0.08386156 0.         0.         0.        ]]\n",
      "values: tensor([[-0.4402]])\n",
      "clipped_actions: [[0.08386156 0.         0.         0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[0.         0.         0.17330283 0.13348857]]\n",
      "values: tensor([[0.3357]])\n",
      "clipped_actions: [[0.         0.         0.17330283 0.13348857]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[0.07109243 0.         0.02917896 0.        ]]\n",
      "values: tensor([[0.8789]])\n",
      "clipped_actions: [[0.07109243 0.         0.02917896 0.        ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[-0.0932]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 6: [-2. -2. -2. -2.]\n",
      "\n",
      "Avg rewards so far:0.0 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.1049]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.1231]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0856]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.1583]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.1311]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0285]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 2058: [-1.8229032 -1.8623756 -1.8498338 -1.9482424]\n",
      "\n",
      "Avg rewards so far:0.0 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0240]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0183]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0149]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0073]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0015]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[-0.0008]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 4104: [-0.42834163 -0.7341344  -0.62864685 -1.0420554 ]\n",
      "\n",
      "Avg rewards so far:0.0 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0019]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0011]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[0.0004]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[3.5502e-05]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[-3.9443e-05]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[7.4506e-09]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 6150: [1.0082229  0.53977394 0.70756716 0.05651651]\n",
      "\n",
      "Avg rewards so far:0.0 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[7.6301e-05]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[2.3499e-05]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[0. 0. 0. 0.]]\n",
      "values: tensor([[-8.1956e-08]])\n",
      "clipped_actions: [[0. 0. 0. 0.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[0.        0.        1.0532974 0.       ]]\n",
      "values: tensor([[-3.6880e-06]])\n",
      "clipped_actions: [[0.        0.        1.0532974 0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[0.7242606 0.        2.614381  0.       ]]\n",
      "values: tensor([[-2.5332e-06]])\n",
      "clipped_actions: [[0.7242606 0.        2.614381  0.       ]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[2.8236755e-04 1.5354829e+00 3.1163766e+00 2.8209519e-01]]\n",
      "values: tensor([[-3.9488e-07]])\n",
      "clipped_actions: [[2.8236755e-04 1.5354829e+00 3.1163766e+00 2.8209519e-01]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 8202: [0.73510766 0.47609937 0.52369946 0.22472315]\n",
      "\n",
      "Avg rewards so far:0.0 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 0.       0.      74.27617  0.     ]]\n",
      "values: tensor([[8.4937e-07]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [1.7688992]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       52.086636  0.      ]]\n",
      "values: tensor([[0.0324]])\n",
      "clipped_actions: [[ 0.  0. 50.  0.]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [3.5329504]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.        0.       25.487991  0.      ]]\n",
      "values: tensor([[-0.0093]])\n",
      "clipped_actions: [[ 0.        0.       25.487991  0.      ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [5.2706633]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[1.09741944e-17 3.41039824e+00 3.14786100e+00 1.73559570e+00]]\n",
      "values: tensor([[0.0998]])\n",
      "clipped_actions: [[1.09741944e-17 3.41039824e+00 3.14786100e+00 1.73559570e+00]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [5.1738505]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[0.         2.4530592  1.1564584  0.10210514]]\n",
      "values: tensor([[0.0808]])\n",
      "clipped_actions: [[0.         2.4530592  1.1564584  0.10210514]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [5.0707536]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[5.4015964e-10 1.7550081e+00 2.2343733e+00 1.6610969e+00]]\n",
      "values: tensor([[0.1251]])\n",
      "clipped_actions: [[5.4015964e-10 1.7550081e+00 2.2343733e+00 1.6610969e+00]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [4.8904333]\n",
      "\n",
      "\n",
      "Log-Std at step 10248: [0.67853856 0.45548448 0.5322788  0.17357571]\n",
      "\n",
      "Avg rewards so far:0.815072218577067 ; last reward: 4.890433311462402\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.        0.      211.35703]]\n",
      "values: tensor([[4.0602]])\n",
      "clipped_actions: [[ 0.  0.  0. 50.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[  0.        0.        0.      144.45464]]\n",
      "values: tensor([[3.4850]])\n",
      "clipped_actions: [[ 0.  0.  0. 50.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.       0.       0.      73.78371]]\n",
      "values: tensor([[2.6060]])\n",
      "clipped_actions: [[ 0.  0.  0. 50.]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[0.        0.        0.        6.1729813]]\n",
      "values: tensor([[0.6571]])\n",
      "clipped_actions: [[0.        0.        0.        6.1729813]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[0.        0.        0.        2.9668052]]\n",
      "values: tensor([[0.3310]])\n",
      "clipped_actions: [[0.        0.        0.        2.9668052]]\n",
      "new_obs: [[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[0.        2.8694928 0.        1.9161057]]\n",
      "values: tensor([[0.2504]])\n",
      "clipped_actions: [[0.        2.8694928 0.        1.9161057]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.]\n",
      "\n",
      "\n",
      "Log-Std at step 12294: [0.7082155  0.46796155 0.54586256 0.20418656]\n",
      "\n",
      "Avg rewards so far:0.6986333302089146 ; last reward: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        15.753189 118.15258 ]]\n",
      "values: tensor([[0.3549]])\n",
      "clipped_actions: [[ 0.        0.       15.753189 50.      ]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.26567104]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.        7.198172 70.665596]]\n",
      "values: tensor([[0.2538]])\n",
      "clipped_actions: [[ 0.        0.        7.198172 50.      ]]\n",
      "new_obs: [[17.198172  0.        0.        0.       10.        0.        0.\n",
      "  10.        0.       10.        0.       10.        0.        0.\n",
      "   0.        0.        2.      ]]\n",
      "rewards: [0.45691875]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[17.1982,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 0.       0.       0.      17.64114]]\n",
      "values: tensor([[0.2166]])\n",
      "clipped_actions: [[ 0.       0.       0.      17.64114]]\n",
      "new_obs: [[17.198172  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.45692247]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[17.1982,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 0.          0.          0.83956957 14.144022  ]]\n",
      "values: tensor([[0.1438]])\n",
      "clipped_actions: [[ 0.          0.          0.83956957 14.144022  ]]\n",
      "new_obs: [[17.198172  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.45691076]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[17.1982,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[0.         0.         0.55403244 8.649843  ]]\n",
      "values: tensor([[0.1179]])\n",
      "clipped_actions: [[0.         0.         0.55403244 8.649843  ]]\n",
      "new_obs: [[17.198172  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.45687744]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[17.1982,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[0.        0.        1.0500556 6.193596 ]]\n",
      "values: tensor([[0.1243]])\n",
      "clipped_actions: [[0.        0.        1.0500556 6.193596 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.45681664]\n",
      "\n",
      "\n",
      "Log-Std at step 14346: [0.18645    0.6091368  0.30270833 0.3853068 ]\n",
      "\n",
      "Avg rewards so far:0.6684062443673611 ; last reward: 0.4568166434764862\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 0.         0.         4.2892194 53.726566 ]]\n",
      "values: tensor([[52248.3164]])\n",
      "clipped_actions: [[ 0.         0.         4.2892194 50.       ]]\n",
      "new_obs: [[ 4.2892194  0.         0.         0.        10.         0.\n",
      "  10.         0.         0.        10.         0.        10.\n",
      "   0.        10.         0.         0.         1.       ]]\n",
      "rewards: [0.09384238]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 4.2892,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 0.         0.         2.0597897 27.016766 ]]\n",
      "values: tensor([[56236.1133]])\n",
      "clipped_actions: [[ 0.         0.         2.0597897 27.016766 ]]\n",
      "new_obs: [[ 6.349009  0.        0.        0.       10.        0.        0.\n",
      "  10.        0.       10.        0.       10.        0.        0.\n",
      "   0.        0.        2.      ]]\n",
      "rewards: [0.13891196]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 6.3490,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 0.        0.        0.       17.974216]]\n",
      "values: tensor([[52745.8008]])\n",
      "clipped_actions: [[ 0.        0.        0.       17.974216]]\n",
      "new_obs: [[ 6.349009  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.1389162]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 6.3490,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[0.        0.        3.8618429 5.390047 ]]\n",
      "values: tensor([[33244.3555]])\n",
      "clipped_actions: [[0.        0.        3.8618429 5.390047 ]]\n",
      "new_obs: [[ 6.349009  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.13892038]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 6.3490,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[0.        0.        2.4348478 5.1867313]]\n",
      "values: tensor([[21662.5352]])\n",
      "clipped_actions: [[0.        0.        2.4348478 5.1867313]]\n",
      "new_obs: [[ 6.349009  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.13892433]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 6.3490,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[0.        0.        2.372569  5.0857935]]\n",
      "values: tensor([[13628.4424]])\n",
      "clipped_actions: [[0.        0.        2.372569  5.0857935]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.13892794]\n",
      "\n",
      "\n",
      "Log-Std at step 16392: [0.18347226 0.6097432  0.30117747 0.38544247]\n",
      "\n",
      "Avg rewards so far:0.6095753212769827 ; last reward: 0.13892793655395508\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 0.        0.        5.606922 53.380615]]\n",
      "values: tensor([[8240.3604]])\n",
      "clipped_actions: [[ 0.        0.        5.606922 50.      ]]\n",
      "new_obs: [[ 5.606922  0.        0.        0.       10.        0.       10.\n",
      "   0.        0.       10.        0.       10.        0.       10.\n",
      "   0.        0.        1.      ]]\n",
      "rewards: [0.12602295]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 5.6069,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 0.         0.         1.4531025 27.690742 ]]\n",
      "values: tensor([[6036.6978]])\n",
      "clipped_actions: [[ 0.         0.         1.4531025 27.690742 ]]\n",
      "new_obs: [[ 7.0600247  0.         0.         0.        10.         0.\n",
      "   0.        10.         0.        10.         0.        10.\n",
      "   0.         0.         0.         0.         2.       ]]\n",
      "rewards: [0.15868756]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 7.0600,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 0.        0.        1.272017 16.076023]]\n",
      "values: tensor([[3440.9731]])\n",
      "clipped_actions: [[ 0.        0.        1.272017 16.076023]]\n",
      "new_obs: [[ 8.332042  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.18728365]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 8.3320,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[0.       0.       3.937663 5.722565]]\n",
      "values: tensor([[1931.6689]])\n",
      "clipped_actions: [[0.       0.       3.937663 5.722565]]\n",
      "new_obs: [[ 8.332042  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.18728845]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 8.3320,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[0.        0.        1.7154133 6.1939898]]\n",
      "values: tensor([[826.8887]])\n",
      "clipped_actions: [[0.        0.        1.7154133 6.1939898]]\n",
      "new_obs: [[ 8.332042  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.18729265]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 8.3320,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[0.        0.        4.3441534 4.6305237]]\n",
      "values: tensor([[0.5429]])\n",
      "clipped_actions: [[0.        0.        4.3441534 4.6305237]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.18729594]\n",
      "\n",
      "\n",
      "Log-Std at step 18438: [0.18347418 0.6097496  0.30117747 0.38544244]\n",
      "\n",
      "Avg rewards so far:0.5673473834991455 ; last reward: 0.18729594349861145\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[ 0.         0.         5.5502896 53.790764 ]]\n",
      "values: tensor([[612.7629]])\n",
      "clipped_actions: [[ 0.         0.         5.5502896 50.       ]]\n",
      "new_obs: [[ 5.5502896  0.         0.         0.        10.         0.\n",
      "  10.         0.         0.        10.         0.        10.\n",
      "   0.        10.         0.         0.         1.       ]]\n",
      "rewards: [0.1294046]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[ 5.5503,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          1.0000]])\n",
      "actions: [[ 0.         0.         1.2591808 28.693756 ]]\n",
      "values: tensor([[320.9654]])\n",
      "clipped_actions: [[ 0.         0.         1.2591808 28.693756 ]]\n",
      "new_obs: [[ 6.80947  0.       0.       0.      10.       0.       0.      10.\n",
      "   0.      10.       0.      10.       0.       0.       0.       0.\n",
      "   2.     ]]\n",
      "rewards: [0.15876608]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[ 6.8095,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          2.0000]])\n",
      "actions: [[ 0.          0.          0.98823273 14.561665  ]]\n",
      "values: tensor([[122.8101]])\n",
      "clipped_actions: [[ 0.          0.          0.98823273 14.561665  ]]\n",
      "new_obs: [[ 7.7977033  0.         0.         0.         0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [0.18181163]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[ 7.7977,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[0.        0.        2.7753696 5.7047253]]\n",
      "values: tensor([[47.9818]])\n",
      "clipped_actions: [[0.        0.        2.7753696 5.7047253]]\n",
      "new_obs: [[ 7.7977033  0.         0.         0.         0.        10.\n",
      "   0.        10.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         4.       ]]\n",
      "rewards: [0.1818159]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[ 7.7977,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[0.         0.         0.05339932 3.999287  ]]\n",
      "values: tensor([[10.6754]])\n",
      "clipped_actions: [[0.         0.         0.05339932 3.999287  ]]\n",
      "new_obs: [[ 7.7977033  0.         0.         0.         0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.18181975]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[ 7.7977,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[0.        0.        1.6458426 4.470358 ]]\n",
      "values: tensor([[-0.1886]])\n",
      "clipped_actions: [[0.        0.        1.6458426 4.470358 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.1818229]\n",
      "\n",
      "\n",
      "Log-Std at step 20490: [0.18352157 0.609983   0.30115592 0.38548517]\n",
      "\n",
      "Avg rewards so far:0.5322997028177435 ; last reward: 0.18182289600372314\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       65.89482 112.99336]]\n",
      "values: tensor([[25.4941]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.22745954]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       31.446491 46.889202]]\n",
      "values: tensor([[12.8734]])\n",
      "clipped_actions: [[ 0.        0.       31.446491 46.889202]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.45492867]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.        0.       11.324117 24.040016]]\n",
      "values: tensor([[1.2564]])\n",
      "clipped_actions: [[ 0.        0.       11.324117 24.040016]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.6823959]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 0.         2.2299728  6.604383  17.945972 ]]\n",
      "values: tensor([[1.2564]])\n",
      "clipped_actions: [[ 0.         2.2299728  6.604383  17.945972 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.6823739]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 0.         0.5191274  8.339199  19.211351 ]]\n",
      "values: tensor([[0.6810]])\n",
      "clipped_actions: [[ 0.         0.5191274  8.339199  19.211351 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.6823138]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ 0.        0.        9.572557 19.068113]]\n",
      "values: tensor([[0.2512]])\n",
      "clipped_actions: [[ 0.        0.        9.572557 19.068113]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.682203]\n",
      "\n",
      "\n",
      "Log-Std at step 22536: [-0.02437947  0.41187727  0.3433504   0.58116555]\n",
      "\n",
      "Avg rewards so far:0.5447916438182195 ; last reward: 0.6822029948234558\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        61.049656 104.81522 ]]\n",
      "values: tensor([[4437.2412]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.20861644]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.197693 45.396183]]\n",
      "values: tensor([[5392.0015]])\n",
      "clipped_actions: [[ 0.        0.       27.197693 45.396183]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.41724128]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.         1.2822806 12.196388  23.650003 ]]\n",
      "values: tensor([[5420.9233]])\n",
      "clipped_actions: [[ 0.         1.2822806 12.196388  23.650003 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.625868]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 0.        0.        8.781608 18.810835]]\n",
      "values: tensor([[4030.7356]])\n",
      "clipped_actions: [[ 0.        0.        8.781608 18.810835]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.62585795]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 0.        0.        7.868683 19.924065]]\n",
      "values: tensor([[2788.4343]])\n",
      "clipped_actions: [[ 0.        0.        7.868683 19.924065]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.6258226]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ 1.0522186  0.         9.176853  23.229118 ]]\n",
      "values: tensor([[1820.9133]])\n",
      "clipped_actions: [[ 1.0522186  0.         9.176853  23.229118 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.6675593]\n",
      "\n",
      "\n",
      "Log-Std at step 24582: [-0.03058546  0.41402456  0.3429036   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5542353116548978 ; last reward: 0.6675593256950378\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       62.14016 107.09883]]\n",
      "values: tensor([[1418056.]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.18529294]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       29.29588  45.852943]]\n",
      "values: tensor([[1574437.1250]])\n",
      "clipped_actions: [[ 0.        0.       29.29588  45.852943]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.37059283]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.07357435  1.4814236   9.335843   19.832638  ]]\n",
      "values: tensor([[1624119.6250]])\n",
      "clipped_actions: [[ 0.07357435  1.4814236   9.335843   19.832638  ]]\n",
      "new_obs: [[29.262268  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.3569288]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[29.2623,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 0.33182886  0.          8.118793   18.18374   ]]\n",
      "values: tensor([[1433447.3750]])\n",
      "clipped_actions: [[ 0.33182886  0.          8.118793   18.18374   ]]\n",
      "new_obs: [[28.930439    0.33182886  0.          0.06        0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.3673767]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[28.9304,  0.3318,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         0.         6.6416445 18.709345 ]]\n",
      "values: tensor([[1186081.]])\n",
      "clipped_actions: [[ 0.         0.         6.6416445 18.709345 ]]\n",
      "new_obs: [[28.930439    0.33182886  0.          0.06        0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.3673757]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[28.9304,  0.3318,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 1.8675838  0.         6.241936  19.209856 ]]\n",
      "values: tensor([[962331.3125]])\n",
      "clipped_actions: [[ 1.8675838  0.         6.241936  19.209856 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.43472567]\n",
      "\n",
      "\n",
      "Log-Std at step 26634: [-0.03058544  0.41402456  0.3429036   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5456989088228771 ; last reward: 0.43472567200660706\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        63.049477 108.31703 ]]\n",
      "values: tensor([[733905.6875]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.16815327]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       26.050148 46.296604]]\n",
      "values: tensor([[787771.3125]])\n",
      "clipped_actions: [[ 0.        0.       26.050148 46.296604]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.33631235]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[2.8252416e-21 2.2241399e+00 1.0864439e+01 2.1630442e+01]]\n",
      "values: tensor([[786081.9375]])\n",
      "clipped_actions: [[2.8252416e-21 2.2241399e+00 1.0864439e+01 2.1630442e+01]]\n",
      "new_obs: [[3.0000000e+01 0.0000000e+00 2.8252416e-21 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.5011125]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[3.0000e+01, 0.0000e+00, 2.8252e-21, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[5.7489088e-13 1.9411533e+00 7.1134772e+00 1.6722477e+01]]\n",
      "values: tensor([[649253.5625]])\n",
      "clipped_actions: [[5.7489088e-13 1.9411533e+00 7.1134772e+00 1.6722477e+01]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.49775073]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 0.          0.36670807  6.569479   20.069614  ]]\n",
      "values: tensor([[525136.7500]])\n",
      "clipped_actions: [[ 0.          0.36670807  6.569479   20.069614  ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.49774235]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ 0.        0.        7.847713 21.901941]]\n",
      "values: tensor([[410347.2188]])\n",
      "clipped_actions: [[ 0.        0.        7.847713 21.901941]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.49772042]\n",
      "\n",
      "\n",
      "Log-Std at step 28680: [-0.03058544  0.41402456  0.3429036   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5425003429253896 ; last reward: 0.4977204203605652\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        63.384853 106.27218 ]]\n",
      "values: tensor([[790864.0625]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.15983132]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.64213  45.691742]]\n",
      "values: tensor([[858066.6250]])\n",
      "clipped_actions: [[ 0.        0.       28.64213  45.691742]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.31966776]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.6700255e-02 2.1714127e+00 1.0397991e+01 2.0644299e+01]]\n",
      "values: tensor([[872245.8125]])\n",
      "clipped_actions: [[1.6700255e-02 2.1714127e+00 1.0397991e+01 2.0644299e+01]]\n",
      "new_obs: [[29.9833  0.      0.      0.      0.     10.      0.     10.      0.\n",
      "  10.      0.      0.      0.      0.      0.      0.      3.    ]]\n",
      "rewards: [0.31940582]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[29.9833,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 0.         2.6019087  7.566368  19.241209 ]]\n",
      "values: tensor([[728963.1875]])\n",
      "clipped_actions: [[ 0.         2.6019087  7.566368  19.241209 ]]\n",
      "new_obs: [[29.9833  0.      0.      0.      0.     10.      0.     10.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      4.    ]]\n",
      "rewards: [0.31940967]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.9833,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.        0.        7.400543 19.51161 ]]\n",
      "values: tensor([[587216.0625]])\n",
      "clipped_actions: [[ 0.        0.        7.400543 19.51161 ]]\n",
      "new_obs: [[29.9833  0.      0.      0.      0.     10.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      5.    ]]\n",
      "rewards: [0.3194114]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.9833,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.         2.5594008  6.415989  24.198683 ]]\n",
      "values: tensor([[452980.4062]])\n",
      "clipped_actions: [[ 0.         2.5594008  6.415989  24.198683 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.31941006]\n",
      "\n",
      "\n",
      "Log-Std at step 30726: [-0.03058544  0.41402456  0.3429036   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5285571999847889 ; last reward: 0.3194100558757782\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        58.745403 105.9877  ]]\n",
      "values: tensor([[843707.3125]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.15296662]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.952646 44.287083]]\n",
      "values: tensor([[919937.3125]])\n",
      "clipped_actions: [[ 0.        0.       28.952646 44.287083]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.30593777]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.1991131e-17 4.2166014e+00 1.0427824e+01 2.2070850e+01]]\n",
      "values: tensor([[942753.6875]])\n",
      "clipped_actions: [[1.1991131e-17 4.2166014e+00 1.0427824e+01 2.2070850e+01]]\n",
      "new_obs: [[3.0000000e+01 0.0000000e+00 1.1991131e-17 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.45585364]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[3.0000e+01, 0.0000e+00, 1.1991e-17, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.        0.        5.628744 19.058046]]\n",
      "values: tensor([[803877.1250]])\n",
      "clipped_actions: [[ 0.        0.        5.628744 19.058046]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.4558568]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 0.         0.         7.7041245 17.667187 ]]\n",
      "values: tensor([[656035.1875]])\n",
      "clipped_actions: [[ 0.         0.         7.7041245 17.667187 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.45585406]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ 0.       0.       9.58376 21.90305]]\n",
      "values: tensor([[507444.1875]])\n",
      "clipped_actions: [[ 0.       0.       9.58376 21.90305]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.45584285]\n",
      "\n",
      "\n",
      "Log-Std at step 32778: [-0.03058544  0.41402456  0.3429036   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5242798854322994 ; last reward: 0.45584285259246826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        62.848186 107.310814]]\n",
      "values: tensor([[181429.1875]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.15020731]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       26.326605 44.401592]]\n",
      "values: tensor([[145310.6719]])\n",
      "clipped_actions: [[ 0.        0.       26.326605 44.401592]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.30041876]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.        0.        9.095764 23.19021 ]]\n",
      "values: tensor([[115032.1953]])\n",
      "clipped_actions: [[ 0.        0.        9.095764 23.19021 ]]\n",
      "new_obs: [[29.095764  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.4370516]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[29.0958,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[1.8838045e-06 6.1135900e-01 7.9587579e+00 1.7000006e+01]]\n",
      "values: tensor([[58107.2969]])\n",
      "clipped_actions: [[1.8838045e-06 6.1135900e-01 7.9587579e+00 1.7000006e+01]]\n",
      "new_obs: [[29.095762  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.43405372]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.0958,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         1.1483378  6.337035  20.278217 ]]\n",
      "values: tensor([[30937.3184]])\n",
      "clipped_actions: [[ 0.         1.1483378  6.337035  20.278217 ]]\n",
      "new_obs: [[29.095762  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.43405262]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.0958,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.10517269  0.          8.04572    23.539143  ]]\n",
      "values: tensor([[8579.6670]])\n",
      "clipped_actions: [[ 0.10517269  0.          8.04572    23.539143  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.43570226]\n",
      "\n",
      "\n",
      "Log-Std at step 34824: [-0.03058544  0.41402456  0.3429036   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5193589064810011 ; last reward: 0.43570226430892944\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       63.95308 108.16642]]\n",
      "values: tensor([[36376.9492]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.1448721]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       29.561146 44.21807 ]]\n",
      "values: tensor([[30895.7812]])\n",
      "clipped_actions: [[ 0.        0.       29.561146 44.21807 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.28974792]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.6194725e-22 3.5899162e+00 1.0058642e+01 2.2486162e+01]]\n",
      "values: tensor([[28959.7637]])\n",
      "clipped_actions: [[1.6194725e-22 3.5899162e+00 1.0058642e+01 2.2486162e+01]]\n",
      "new_obs: [[3.0000000e+01 0.0000000e+00 1.6194725e-22 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.43173]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[3.0000e+01, 0.0000e+00, 1.6195e-22, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.         1.7029843  6.535706  15.883811 ]]\n",
      "values: tensor([[17737.3223]])\n",
      "clipped_actions: [[ 0.         1.7029843  6.535706  15.883811 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.43173358]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[6.9816926e-12 1.8249960e+00 7.1855679e+00 2.0494825e+01]]\n",
      "values: tensor([[9901.4727]])\n",
      "clipped_actions: [[6.9816926e-12 1.8249960e+00 7.1855679e+00 2.0494825e+01]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.42883554]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ 0.         1.3476453  7.398946  19.354696 ]]\n",
      "values: tensor([[4929.4849]])\n",
      "clipped_actions: [[ 0.         1.3476453  7.398946  19.354696 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.42882904]\n",
      "\n",
      "\n",
      "Log-Std at step 36870: [-0.03058534  0.41402417  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5145941768821917 ; last reward: 0.42882904410362244\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        61.418034 110.41933 ]]\n",
      "values: tensor([[861778.0625]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.14205788]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.       0.      27.67824 44.80744]]\n",
      "values: tensor([[974354.8750]])\n",
      "clipped_actions: [[ 0.       0.      27.67824 44.80744]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.28411916]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.        0.       11.314324 20.849213]]\n",
      "values: tensor([[976777.]])\n",
      "clipped_actions: [[ 0.        0.       11.314324 20.849213]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.42618406]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 0.         2.1302235 11.144587  18.069437 ]]\n",
      "values: tensor([[805418.7500]])\n",
      "clipped_actions: [[ 0.         2.1302235 11.144587  18.069437 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.42618763]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 0.5003542  0.         7.1520524 15.79532  ]]\n",
      "values: tensor([[612131.3125]])\n",
      "clipped_actions: [[ 0.5003542  0.         7.1520524 15.79532  ]]\n",
      "new_obs: [[29.499645   0.5003542  0.         0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.43898317]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.4996,  0.5004,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.28703988  0.          9.108142   20.891663  ]]\n",
      "values: tensor([[430285.6562]])\n",
      "clipped_actions: [[ 0.28703988  0.          9.108142   20.891663  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.44571185]\n",
      "\n",
      "\n",
      "Log-Std at step 38922: [-0.03058534  0.41402417  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5111500605940819 ; last reward: 0.4457118511199951\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       62.21177 108.1711 ]]\n",
      "values: tensor([[580396.3750]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.1383771]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.987577 43.035923]]\n",
      "values: tensor([[630680.3125]])\n",
      "clipped_actions: [[ 0.        0.       27.987577 43.035923]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.2767573]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.2387141e-11 4.0664797e+00 7.9586215e+00 2.2987055e+01]]\n",
      "values: tensor([[610711.6875]])\n",
      "clipped_actions: [[1.2387141e-11 4.0664797e+00 7.9586215e+00 2.2987055e+01]]\n",
      "new_obs: [[2.7958622e+01 0.0000000e+00 1.2387141e-11 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.3841247]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.7959e+01, 0.0000e+00, 1.2387e-11, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.        0.        5.353429 16.009811]]\n",
      "values: tensor([[439654.0312]])\n",
      "clipped_actions: [[ 0.        0.        5.353429 16.009811]]\n",
      "new_obs: [[27.958622  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.3841283]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[27.9586,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         2.744896   7.8836246 17.586212 ]]\n",
      "values: tensor([[306708.5312]])\n",
      "clipped_actions: [[ 0.         2.744896   7.8836246 17.586212 ]]\n",
      "new_obs: [[27.958622  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.38412952]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[27.9586,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.65053135  0.          8.511332   18.743065  ]]\n",
      "values: tensor([[182172.2500]])\n",
      "clipped_actions: [[ 0.65053135  0.          8.511332   18.743065  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.40074742]\n",
      "\n",
      "\n",
      "Log-Std at step 40968: [-0.03058534  0.41402417  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5058927919183459 ; last reward: 0.4007474184036255\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       62.68505 108.59058]]\n",
      "values: tensor([[360223.5938]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.1364597]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.535973 44.833538]]\n",
      "values: tensor([[377478.7812]])\n",
      "clipped_actions: [[ 0.        0.       27.535973 44.833538]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.27292228]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.2839107e-12 2.6688426e+00 7.5883236e+00 2.2982580e+01]]\n",
      "values: tensor([[353562.3750]])\n",
      "clipped_actions: [[1.2839107e-12 2.6688426e+00 7.5883236e+00 2.2982580e+01]]\n",
      "new_obs: [[2.7588324e+01 0.0000000e+00 1.2839107e-12 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.37374848]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.7588e+01, 0.0000e+00, 1.2839e-12, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[7.9074989e-06 1.4476756e+00 7.3016782e+00 1.8985189e+01]]\n",
      "values: tensor([[216335.1406]])\n",
      "clipped_actions: [[7.9074989e-06 1.4476756e+00 7.3016782e+00 1.8985189e+01]]\n",
      "new_obs: [[27.588316  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.3710335]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[27.5883,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[9.3273129e-16 2.5404966e+00 8.1270771e+00 1.7486425e+01]]\n",
      "values: tensor([[132257.1250]])\n",
      "clipped_actions: [[9.3273129e-16 2.5404966e+00 8.1270771e+00 1.7486425e+01]]\n",
      "new_obs: [[27.588316  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.3683058]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[27.5883,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.64185697  0.          7.967299   15.268305  ]]\n",
      "values: tensor([[60675.2852]])\n",
      "clipped_actions: [[ 0.64185697  0.          7.967299   15.268305  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.38445768]\n",
      "\n",
      "\n",
      "Log-Std at step 43014: [-0.03058534  0.41402417  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.5003730139949105 ; last reward: 0.38445767760276794\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       62.35205 109.74884]]\n",
      "values: tensor([[61167.8672]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.13461055]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.699368 43.839268]]\n",
      "values: tensor([[42344.5898]])\n",
      "clipped_actions: [[ 0.        0.       27.699368 43.839268]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.2692238]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.2093563e-25 3.2076426e+00 9.5273409e+00 2.0416607e+01]]\n",
      "values: tensor([[28854.9531]])\n",
      "clipped_actions: [[1.2093563e-25 3.2076426e+00 9.5273409e+00 2.0416607e+01]]\n",
      "new_obs: [[2.9527340e+01 0.0000000e+00 1.2093563e-25 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.39478526]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.9527e+01, 0.0000e+00, 1.2094e-25, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.         2.5797617  9.011182  18.681627 ]]\n",
      "values: tensor([[14928.8652]])\n",
      "clipped_actions: [[ 0.         2.5797617  9.011182  18.681627 ]]\n",
      "new_obs: [[29.52734  0.       0.       0.       0.      10.       0.      10.\n",
      "   0.       0.       0.       0.       0.       0.       0.       0.\n",
      "   4.     ]]\n",
      "rewards: [0.3947887]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.5273,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.59052014  0.          8.05397    18.944532  ]]\n",
      "values: tensor([[9621.6650]])\n",
      "clipped_actions: [[ 0.59052014  0.          8.05397    18.944532  ]]\n",
      "new_obs: [[28.936821    0.59052014  0.          0.06        0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.40934217]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[28.9368,  0.5905,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.63869286  0.         10.314506   21.62432   ]]\n",
      "values: tensor([[5169.3433]])\n",
      "clipped_actions: [[ 0.63869286  0.         10.314506   21.62432   ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.42518857]\n",
      "\n",
      "\n",
      "Log-Std at step 45066: [-0.03058534  0.41402403  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.49710412517837854 ; last reward: 0.4251885712146759\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        63.431126 105.080086]]\n",
      "values: tensor([[257891.1875]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.13264146]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.670752 44.622944]]\n",
      "values: tensor([[270648.6250]])\n",
      "clipped_actions: [[ 0.        0.       27.670752 44.622944]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.26528546]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[5.2725307e-12 1.4696941e+00 1.1478399e+01 2.0339188e+01]]\n",
      "values: tensor([[271079.8438]])\n",
      "clipped_actions: [[5.2725307e-12 1.4696941e+00 1.1478399e+01 2.0339188e+01]]\n",
      "new_obs: [[3.0000000e+01 0.0000000e+00 5.2725307e-12 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.39527947]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[3.0000e+01, 0.0000e+00, 5.2725e-12, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.7849287   0.05801029  7.4838285  17.282928  ]]\n",
      "values: tensor([[241371.0781]])\n",
      "clipped_actions: [[ 0.7849287   0.05801029  7.4838285  17.282928  ]]\n",
      "new_obs: [[29.21507  0.       0.       0.       0.      10.       0.      10.\n",
      "   0.       0.       0.       0.       0.       0.       0.       0.\n",
      "   4.     ]]\n",
      "rewards: [0.26263785]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.2151,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.6748772  0.         9.664091  16.834356 ]]\n",
      "values: tensor([[175031.7500]])\n",
      "clipped_actions: [[ 0.6748772  0.         9.664091  16.834356 ]]\n",
      "new_obs: [[28.540194   0.6748772  0.         0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.27921724]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[28.5402,  0.6749,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.        0.        9.18333  16.189383]]\n",
      "values: tensor([[106727.4375]])\n",
      "clipped_actions: [[ 0.        0.        9.18333  16.189383]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.27921802]\n",
      "\n",
      "\n",
      "Log-Std at step 47112: [-0.03058534  0.41402403  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.48802553738156956 ; last reward: 0.27921801805496216\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        62.384586 107.514725]]\n",
      "values: tensor([[88862.1797]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.1319574]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       29.190361 46.495266]]\n",
      "values: tensor([[89287.8672]])\n",
      "clipped_actions: [[ 0.        0.       29.190361 46.495266]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.26391718]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.         0.         9.6623535 22.457636 ]]\n",
      "values: tensor([[85660.6484]])\n",
      "clipped_actions: [[ 0.         0.         9.6623535 22.457636 ]]\n",
      "new_obs: [[29.662354  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.39142418]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[29.6624,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 0.          0.31167105  9.007081   19.558279  ]]\n",
      "values: tensor([[62159.5781]])\n",
      "clipped_actions: [[ 0.          0.31167105  9.007081   19.558279  ]]\n",
      "new_obs: [[29.662354  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.39142746]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.6624,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.          0.66899055  7.8801265  18.82468   ]]\n",
      "values: tensor([[39688.0625]])\n",
      "clipped_actions: [[ 0.          0.66899055  7.8801265  18.82468   ]]\n",
      "new_obs: [[29.662354  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.3914288]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.6624,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[3.9166608e-09 1.0399395e+00 9.3137445e+00 1.9536392e+01]]\n",
      "values: tensor([[20392.1816]])\n",
      "clipped_actions: [[3.9166608e-09 1.0399395e+00 9.3137445e+00 1.9536392e+01]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.38878796]\n",
      "\n",
      "\n",
      "Log-Std at step 49158: [-0.03058534  0.41402403  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.48405603408813475 ; last reward: 0.3887879550457001\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        63.578312 108.49193 ]]\n",
      "values: tensor([[94478.8672]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.13144988]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       29.198534 43.80646 ]]\n",
      "values: tensor([[99442.7812]])\n",
      "clipped_actions: [[ 0.        0.       29.198534 43.80646 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.26290202]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.         2.7793634  9.772078  21.42191  ]]\n",
      "values: tensor([[98106.6328]])\n",
      "clipped_actions: [[ 0.         2.7793634  9.772078  21.42191  ]]\n",
      "new_obs: [[29.772078  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.3913608]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[29.7721,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 0.         1.1600785  6.5638943 18.871746 ]]\n",
      "values: tensor([[72020.5938]])\n",
      "clipped_actions: [[ 0.         1.1600785  6.5638943 18.871746 ]]\n",
      "new_obs: [[29.772078  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.39136392]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.7721,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.39449087  0.          9.059511   21.63229   ]]\n",
      "values: tensor([[44507.9023]])\n",
      "clipped_actions: [[ 0.39449087  0.          9.059511   21.63229   ]]\n",
      "new_obs: [[29.377586    0.39449087  0.          0.06        0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.4004222]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.3776,  0.3945,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.        0.        9.424522 22.207218]]\n",
      "values: tensor([[21754.8340]])\n",
      "clipped_actions: [[ 0.        0.        9.424522 22.207218]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.40042055]\n",
      "\n",
      "\n",
      "Log-Std at step 51210: [-0.03058534  0.41402403  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.4808392845667325 ; last reward: 0.40042054653167725\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        61.227264 107.19692 ]]\n",
      "values: tensor([[54929.4883]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.1303635]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       29.842892 44.403522]]\n",
      "values: tensor([[52778.1211]])\n",
      "clipped_actions: [[ 0.        0.       29.842892 44.403522]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.26072913]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[7.0667765e-22 2.8457406e+00 1.1485085e+01 2.2502935e+01]]\n",
      "values: tensor([[43031.7148]])\n",
      "clipped_actions: [[7.0667765e-22 2.8457406e+00 1.1485085e+01 2.2502935e+01]]\n",
      "new_obs: [[3.0000000e+01 0.0000000e+00 7.0667765e-22 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.38849002]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[3.0000e+01, 0.0000e+00, 7.0668e-22, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.         2.1306229  8.921483  17.428951 ]]\n",
      "values: tensor([[18351.1660]])\n",
      "clipped_actions: [[ 0.         2.1306229  8.921483  17.428951 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.38849312]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 0.        2.02062   8.750322 17.509785]]\n",
      "values: tensor([[8788.6650]])\n",
      "clipped_actions: [[ 0.        2.02062   8.750322 17.509785]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.38849455]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ 1.9234022  0.        11.350878  22.72155  ]]\n",
      "values: tensor([[1653.0564]])\n",
      "clipped_actions: [[ 1.9234022  0.        11.350878  22.72155  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.43733877]\n",
      "\n",
      "\n",
      "Log-Std at step 53256: [-0.03058534  0.41402403  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.4792281543767011 ; last reward: 0.43733876943588257\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       62.51763 108.09781]]\n",
      "values: tensor([[301962.4688]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.1286714]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.322432 45.00772 ]]\n",
      "values: tensor([[331256.1562]])\n",
      "clipped_actions: [[ 0.        0.       27.322432 45.00772 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.25734478]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[3.1322395e-24 3.4159670e+00 9.8961496e+00 2.3509668e+01]]\n",
      "values: tensor([[328906.1562]])\n",
      "clipped_actions: [[3.1322395e-24 3.4159670e+00 9.8961496e+00 2.3509668e+01]]\n",
      "new_obs: [[2.9896149e+01 0.0000000e+00 3.1322395e-24 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.3821109]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.9896e+01, 0.0000e+00, 3.1322e-24, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.14329682  0.          6.5228596  17.180132  ]]\n",
      "values: tensor([[293913.1562]])\n",
      "clipped_actions: [[ 0.14329682  0.          6.5228596  17.180132  ]]\n",
      "new_obs: [[29.752853    0.14329682  0.          0.06        0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.38451487]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.7529,  0.1433,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[8.1395470e-03 9.3457210e-01 8.6811543e+00 2.0444792e+01]]\n",
      "values: tensor([[219801.3906]])\n",
      "clipped_actions: [[8.1395470e-03 9.3457210e-01 8.6811543e+00 2.0444792e+01]]\n",
      "new_obs: [[29.744713  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.576802]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.7447,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.5332031  0.         8.64795   20.32258  ]]\n",
      "values: tensor([[134589.3750]])\n",
      "clipped_actions: [[ 0.5332031  0.         8.64795   20.32258  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.5892302]\n",
      "\n",
      "\n",
      "Log-Std at step 55302: [-0.03058534  0.41402403  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.48315679814134327 ; last reward: 0.5892301797866821\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       62.51143 108.48752]]\n",
      "values: tensor([[132473.2812]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12821391]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.285887 43.672123]]\n",
      "values: tensor([[127935.0078]])\n",
      "clipped_actions: [[ 0.        0.       27.285887 43.672123]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.25642973]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[4.9269323e-13 3.1017370e+00 1.1920052e+01 2.2439068e+01]]\n",
      "values: tensor([[111739.2734]])\n",
      "clipped_actions: [[4.9269323e-13 3.1017370e+00 1.1920052e+01 2.2439068e+01]]\n",
      "new_obs: [[3.0000000e+01 0.0000000e+00 4.9269323e-13 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.38208362]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[3.0000e+01, 0.0000e+00, 4.9269e-13, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.07369654  1.7683177   7.6970596  20.268814  ]]\n",
      "values: tensor([[77203.2422]])\n",
      "clipped_actions: [[ 0.07369654  1.7683177   7.6970596  20.268814  ]]\n",
      "new_obs: [[29.926304  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.25386974]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.9263,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         0.         6.9126067 19.970354 ]]\n",
      "values: tensor([[45775.6914]])\n",
      "clipped_actions: [[ 0.         0.         6.9126067 19.970354 ]]\n",
      "new_obs: [[29.926304  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.25387144]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.9263,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.57633054  0.          7.506253   21.586327  ]]\n",
      "values: tensor([[20261.1680]])\n",
      "clipped_actions: [[ 0.57633054  0.          7.506253   21.586327  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.2673695]\n",
      "\n",
      "\n",
      "Log-Std at step 57354: [-0.03058533  0.41402403  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.47571585712761716 ; last reward: 0.26736950874328613\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        64.342896 106.29962 ]]\n",
      "values: tensor([[131497.4375]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12709361]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.027294 40.789112]]\n",
      "values: tensor([[145517.6406]])\n",
      "clipped_actions: [[ 0.        0.       28.027294 40.789112]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.25418904]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.        0.        8.907769 21.212769]]\n",
      "values: tensor([[149369.5469]])\n",
      "clipped_actions: [[ 0.        0.        8.907769 21.212769]]\n",
      "new_obs: [[28.907768  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.36740497]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[28.9078,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 0.        0.        8.191047 16.500723]]\n",
      "values: tensor([[133685.]])\n",
      "clipped_actions: [[ 0.        0.        8.191047 16.500723]]\n",
      "new_obs: [[28.907768  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.36740774]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[28.9078,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         1.429187   8.3646755 16.368504 ]]\n",
      "values: tensor([[99556.2344]])\n",
      "clipped_actions: [[ 0.         1.429187   8.3646755 16.368504 ]]\n",
      "new_obs: [[28.907768  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.36740938]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[28.9078,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.         0.6620525  8.646444  19.976816 ]]\n",
      "values: tensor([[50771.0273]])\n",
      "clipped_actions: [[ 0.         0.6620525  8.646444  19.976816 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.36740914]\n",
      "\n",
      "\n",
      "Log-Std at step 59400: [-0.03058533  0.41402403  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.47210563321908317 ; last reward: 0.3674091398715973\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        60.739143 109.40726 ]]\n",
      "values: tensor([[206228.5156]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12675768]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.342558 46.23242 ]]\n",
      "values: tensor([[200174.0938]])\n",
      "clipped_actions: [[ 0.        0.       28.342558 46.23242 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.2535171]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[4.0469295e-34 3.7805066e+00 8.4974518e+00 2.0995413e+01]]\n",
      "values: tensor([[187345.5000]])\n",
      "clipped_actions: [[4.0469295e-34 3.7805066e+00 8.4974518e+00 2.0995413e+01]]\n",
      "new_obs: [[2.8497452e+01 0.0000000e+00 4.0469295e-34 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.3586973]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.8497e+01, 0.0000e+00, 4.0469e-34, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[7.6326396e-13 2.1921203e+00 6.4558945e+00 1.7130926e+01]]\n",
      "values: tensor([[156892.7500]])\n",
      "clipped_actions: [[7.6326396e-13 2.1921203e+00 6.4558945e+00 1.7130926e+01]]\n",
      "new_obs: [[28.497452  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.35616472]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[28.4975,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 1.1105995  0.         6.197854  17.923418 ]]\n",
      "values: tensor([[111365.8828]])\n",
      "clipped_actions: [[ 1.1105995  0.         6.197854  17.923418 ]]\n",
      "new_obs: [[27.386852   1.1105995  0.         0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.38305485]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[27.3869,  1.1106,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.          0.16545594  9.143143   17.152739  ]]\n",
      "values: tensor([[54743.6758]])\n",
      "clipped_actions: [[ 0.          0.16545594  9.143143   17.152739  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.5915183]\n",
      "\n",
      "\n",
      "Log-Std at step 61446: [-0.03058533  0.41402403  0.3429037   0.57664764]\n",
      "\n",
      "Avg rewards so far:0.4759576541762198 ; last reward: 0.5915182828903198\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        61.651035 108.87876 ]]\n",
      "values: tensor([[34258.4688]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12677139]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       26.250841 43.351963]]\n",
      "values: tensor([[24574.2949]])\n",
      "clipped_actions: [[ 0.        0.       26.250841 43.351963]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.25354442]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[3.3299305e-06 2.0516186e+00 8.0628281e+00 2.1853313e+01]]\n",
      "values: tensor([[15919.1846]])\n",
      "clipped_actions: [[3.3299305e-06 2.0516186e+00 8.0628281e+00 2.1853313e+01]]\n",
      "new_obs: [[2.8062824e+01 0.0000000e+00 3.3299305e-06 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.35322616]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.8063e+01, 0.0000e+00, 3.3299e-06, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[8.8305300e-01 1.4213726e-04 7.1614037e+00 1.9947189e+01]]\n",
      "values: tensor([[8683.1875]])\n",
      "clipped_actions: [[8.8305300e-01 1.4213726e-04 7.1614037e+00 1.9947189e+01]]\n",
      "new_obs: [[27.179771    0.88291085  0.          0.06000966  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.37326366]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[27.1798,  0.8829,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         1.6495459  5.5053945 16.171068 ]]\n",
      "values: tensor([[3085.4565]])\n",
      "clipped_actions: [[ 0.         1.6495459  5.5053945 16.171068 ]]\n",
      "new_obs: [[27.179771  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [1.4912728]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[27.1798,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[2.0762004e-03 0.0000000e+00 9.1966724e+00 1.7175207e+01]]\n",
      "values: tensor([[1.7128]])\n",
      "clipped_actions: [[2.0762004e-03 0.0000000e+00 9.1966724e+00 1.7175207e+01]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.4899457]\n",
      "\n",
      "\n",
      "Log-Std at step 63498: [-0.03055939  0.4140362   0.3429066   0.5766362 ]\n",
      "\n",
      "Avg rewards so far:0.5076447790488601 ; last reward: 1.489945650100708\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        60.864964 109.55674 ]]\n",
      "values: tensor([[346605.0312]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.1263617]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       31.945654 48.749344]]\n",
      "values: tensor([[367656.5312]])\n",
      "clipped_actions: [[ 0.        0.       31.945654 48.749344]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.25272498]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.        0.        9.078161 20.206654]]\n",
      "values: tensor([[357486.4688]])\n",
      "clipped_actions: [[ 0.        0.        9.078161 20.206654]]\n",
      "new_obs: [[29.078161  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.36744165]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[29.0782,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 0.04182962  0.          8.589824   15.406126  ]]\n",
      "values: tensor([[299156.6562]])\n",
      "clipped_actions: [[ 0.04182962  0.          8.589824   15.406126  ]]\n",
      "new_obs: [[29.036331    0.04182962  0.          0.06        0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.36723772]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.0363,  0.0418,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         2.3561382  7.169685  18.161112 ]]\n",
      "values: tensor([[194689.5312]])\n",
      "clipped_actions: [[ 0.         2.3561382  7.169685  18.161112 ]]\n",
      "new_obs: [[29.036331  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.41883337]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.0363,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.10822238  0.          7.504761   21.62015   ]]\n",
      "values: tensor([[95573.6250]])\n",
      "clipped_actions: [[ 0.10822238  0.          7.504761   21.62015   ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.420304]\n",
      "\n",
      "\n",
      "Log-Std at step 65544: [-0.03055832  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.5049980887860963 ; last reward: 0.42030400037765503\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        61.134537 106.127495]]\n",
      "values: tensor([[153033.9375]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12528782]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.296305 44.435738]]\n",
      "values: tensor([[146981.9531]])\n",
      "clipped_actions: [[ 0.        0.       27.296305 44.435738]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.25057712]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.1009105e-27 4.3379049e+00 8.9091740e+00 2.0160622e+01]]\n",
      "values: tensor([[133377.6562]])\n",
      "clipped_actions: [[1.1009105e-27 4.3379049e+00 8.9091740e+00 2.0160622e+01]]\n",
      "new_obs: [[2.8909174e+01 0.0000000e+00 1.1009105e-27 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.35969576]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.8909e+01, 0.0000e+00, 1.1009e-27, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.3987201   0.43309972  7.327016   16.349117  ]]\n",
      "values: tensor([[82448.1562]])\n",
      "clipped_actions: [[ 0.3987201   0.43309972  7.327016   16.349117  ]]\n",
      "new_obs: [[28.510454  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.23440798]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[28.5105,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[1.4091338e+00 3.4495368e-11 5.6326857e+00 1.9267511e+01]]\n",
      "values: tensor([[40449.8359]])\n",
      "clipped_actions: [[1.4091338e+00 3.4495368e-11 5.6326857e+00 1.9267511e+01]]\n",
      "new_obs: [[27.10132    1.4091338  0.         0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.267214]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[27.1013,  1.4091,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.03435821  0.          9.6177025  19.166775  ]]\n",
      "values: tensor([[15867.6016]])\n",
      "clipped_actions: [[ 0.03435821  0.          9.6177025  19.166775  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.26682314]\n",
      "\n",
      "\n",
      "Log-Std at step 67590: [-0.03055832  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.497992943314945 ; last reward: 0.2668231427669525\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        61.999565 109.278915]]\n",
      "values: tensor([[108166.8281]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12472691]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.516983 44.212246]]\n",
      "values: tensor([[120529.3906]])\n",
      "clipped_actions: [[ 0.        0.       28.516983 44.212246]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.2494553]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.22902115e-23 3.32578850e+00 1.02050476e+01 2.15309086e+01]]\n",
      "values: tensor([[136053.1406]])\n",
      "clipped_actions: [[1.22902115e-23 3.32578850e+00 1.02050476e+01 2.15309086e+01]]\n",
      "new_obs: [[3.00000000e+01 0.00000000e+00 1.22902115e-23 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+01 0.00000000e+00 1.00000000e+01\n",
      "  0.00000000e+00 1.00000000e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  3.00000000e+00]]\n",
      "rewards: [0.37169105]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[3.0000e+01, 0.0000e+00, 1.2290e-23, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.         0.         7.6143074 18.349178 ]]\n",
      "values: tensor([[127241.8984]])\n",
      "clipped_actions: [[ 0.         0.         7.6143074 18.349178 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.37169346]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 0.6192953  0.         8.883173  14.476553 ]]\n",
      "values: tensor([[90099.4375]])\n",
      "clipped_actions: [[ 0.6192953  0.         8.883173  14.476553 ]]\n",
      "new_obs: [[29.380705   0.6192953  0.         0.06       0.        10.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         5.       ]]\n",
      "rewards: [0.38589656]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.3807,  0.6193,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[1.1763949e-03 3.0885783e-01 9.9891520e+00 1.6610382e+01]]\n",
      "values: tensor([[41869.1992]])\n",
      "clipped_actions: [[1.1763949e-03 3.0885783e-01 9.9891520e+00 1.6610382e+01]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.76863414]\n",
      "\n",
      "\n",
      "Log-Std at step 69642: [-0.03055832  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.5057255489485605 ; last reward: 0.7686341404914856\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       64.32814 107.6306 ]]\n",
      "values: tensor([[193205.3438]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12409613]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.190254 45.85394 ]]\n",
      "values: tensor([[196863.2812]])\n",
      "clipped_actions: [[ 0.        0.       27.190254 45.85394 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24819365]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[2.2219489e-21 2.6090691e+00 7.8365889e+00 2.1349442e+01]]\n",
      "values: tensor([[206097.7031]])\n",
      "clipped_actions: [[2.2219489e-21 2.6090691e+00 7.8365889e+00 2.1349442e+01]]\n",
      "new_obs: [[2.7836590e+01 0.0000000e+00 2.2219489e-21 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.34296367]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.7837e+01, 0.0000e+00, 2.2219e-21, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[9.2260122e-01 7.1051172e-06 6.7796278e+00 1.9882143e+01]]\n",
      "values: tensor([[177579.5938]])\n",
      "clipped_actions: [[9.2260122e-01 7.1051172e-06 6.7796278e+00 1.9882143e+01]]\n",
      "new_obs: [[26.913988    0.92259413  0.          0.06000046  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.36339128]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[26.9140,  0.9226,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         1.4238724  7.239498  14.978669 ]]\n",
      "values: tensor([[123257.3438]])\n",
      "clipped_actions: [[ 0.         1.4238724  7.239498  14.978669 ]]\n",
      "new_obs: [[26.913988  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [1.5070541]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[26.9140,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.         0.8702661  8.270106  17.432375 ]]\n",
      "values: tensor([[55280.2734]])\n",
      "clipped_actions: [[ 0.         0.8702661  8.270106  17.432375 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.5069546]\n",
      "\n",
      "\n",
      "Log-Std at step 71688: [-0.03055832  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.5335374656650755 ; last reward: 1.506954550743103\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        62.486237 110.096436]]\n",
      "values: tensor([[71841.1172]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12412154]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       26.890303 42.247723]]\n",
      "values: tensor([[60869.4766]])\n",
      "clipped_actions: [[ 0.        0.       26.890303 42.247723]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24824442]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[5.7098376e-08 2.7990143e+00 9.9280081e+00 2.0802284e+01]]\n",
      "values: tensor([[58049.8203]])\n",
      "clipped_actions: [[5.7098376e-08 2.7990143e+00 9.9280081e+00 2.0802284e+01]]\n",
      "new_obs: [[2.9928007e+01 6.6174449e-24 5.7098376e-08 5.1770774e+14 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.3689931]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.9928e+01, 6.6174e-24, 5.7098e-08, 5.1771e+14, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[0.0000000e+00 0.0000000e+00 1.5484228e+14 2.3971776e+14]]\n",
      "values: tensor([[1.4224e+17]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[2.9928007e+01 6.6174449e-24 0.0000000e+00 5.1770774e+14 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 4.0000000e+00]]\n",
      "rewards: [0.36899546]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[2.9928e+01, 6.6174e-24, 0.0000e+00, 5.1771e+14, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0000e+00]])\n",
      "actions: [[0.0000000e+00 0.0000000e+00 1.5484228e+14 2.3971776e+14]]\n",
      "values: tensor([[1.4224e+17]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[2.9928007e+01 6.6174449e-24 0.0000000e+00 5.1770774e+14 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 5.0000000e+00]]\n",
      "rewards: [0.36899695]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[2.9928e+01, 6.6174e-24, 0.0000e+00, 5.1771e+14, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.0000e+00]])\n",
      "actions: [[0.0000000e+00 0.0000000e+00 1.5484228e+14 2.3971776e+14]]\n",
      "values: tensor([[1.4224e+17]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.36899698]\n",
      "\n",
      "\n",
      "Log-Std at step 73734: [-0.03055828  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.5290904254526705 ; last reward: 0.3689969778060913\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        62.181282 106.99273 ]]\n",
      "values: tensor([[123756.6875]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12347732]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.36656  43.257248]]\n",
      "values: tensor([[115345.6406]])\n",
      "clipped_actions: [[ 0.        0.       28.36656  43.257248]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24695593]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.        0.       11.077893 21.668932]]\n",
      "values: tensor([[107309.0156]])\n",
      "clipped_actions: [[ 0.        0.       11.077893 21.668932]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.3704363]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 0.        3.644722 10.487654 19.438913]]\n",
      "values: tensor([[78165.1484]])\n",
      "clipped_actions: [[ 0.        3.644722 10.487654 19.438913]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.37043858]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 0.         0.8724904  7.0932493 21.863674 ]]\n",
      "values: tensor([[36984.9023]])\n",
      "clipped_actions: [[ 0.         0.8724904  7.0932493 21.863674 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.37044]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[1.5334235e+00 7.1346136e-03 9.9614658e+00 1.7136648e+01]]\n",
      "values: tensor([[8903.5625]])\n",
      "clipped_actions: [[1.5334235e+00 7.1346136e-03 9.9614658e+00 1.7136648e+01]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.4144733]\n",
      "\n",
      "\n",
      "Log-Std at step 75786: [-0.03055828  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.5260741851831737 ; last reward: 0.414473295211792\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        60.029087 110.581116]]\n",
      "values: tensor([[179078.6250]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12346002]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       26.242704 46.40663 ]]\n",
      "values: tensor([[177145.2969]])\n",
      "clipped_actions: [[ 0.        0.       26.242704 46.40663 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.2469213]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.         0.         9.4767885 23.846525 ]]\n",
      "values: tensor([[173642.2500]])\n",
      "clipped_actions: [[ 0.         0.         9.4767885 23.846525 ]]\n",
      "new_obs: [[29.476788  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.36392462]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[29.4768,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[2.0145681e+00 9.8185224e-11 8.7760601e+00 1.9733740e+01]]\n",
      "values: tensor([[142867.9688]])\n",
      "clipped_actions: [[2.0145681e+00 9.8185224e-11 8.7760601e+00 1.9733740e+01]]\n",
      "new_obs: [[27.462221  2.014568  0.        0.06      0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.4112021]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[27.4622,  2.0146,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.          0.05557324  7.6853867  20.634521  ]]\n",
      "values: tensor([[80976.3203]])\n",
      "clipped_actions: [[ 0.          0.05557324  7.6853867  20.634521  ]]\n",
      "new_obs: [[27.462221    1.9589949   0.          0.05829791  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [0.47858062]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[27.4622,  1.9590,  0.0000,  0.0583,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 1.1299852  0.         8.689256  19.135185 ]]\n",
      "values: tensor([[23564.8164]])\n",
      "clipped_actions: [[ 1.1299852  0.         8.689256  19.135185 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.50524616]\n",
      "\n",
      "\n",
      "Log-Std at step 77832: [-0.03055828  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.5255401333173116 ; last reward: 0.5052461624145508\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       65.24222 108.72674]]\n",
      "values: tensor([[98277.2031]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.1228056]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.094711 44.201313]]\n",
      "values: tensor([[90170.3125]])\n",
      "clipped_actions: [[ 0.        0.       28.094711 44.201313]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24561241]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 1.1639745  0.        10.861812  18.575527 ]]\n",
      "values: tensor([[82717.5625]])\n",
      "clipped_actions: [[ 1.1639745  0.        10.861812  18.575527 ]]\n",
      "new_obs: [[28.836025   1.1639745  0.         0.06       0.        10.\n",
      "   0.        10.         0.        10.         0.         0.\n",
      "   0.         0.         0.         0.         3.       ]]\n",
      "rewards: [0.38148725]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[28.8360,  1.1640,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[ 1.4917485  0.         5.3059545 16.003717 ]]\n",
      "values: tensor([[47631.1211]])\n",
      "clipped_actions: [[ 1.4917485  0.         5.3059545 16.003717 ]]\n",
      "new_obs: [[27.344276    2.655723    0.          0.09370265  0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.41690096]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[27.3443,  2.6557,  0.0000,  0.0937,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[1.5760650e-19 4.0692315e+00 4.4135737e+00 1.6961843e+01]]\n",
      "values: tensor([[14234.2949]])\n",
      "clipped_actions: [[1.5760650e-19 4.0692315e+00 4.4135737e+00 1.6961843e+01]]\n",
      "new_obs: [[27.344276  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [3.6755326]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[27.3443,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.        0.        9.343075 17.677572]]\n",
      "values: tensor([[2127.5422]])\n",
      "clipped_actions: [[ 0.        0.        9.343075 17.677572]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [3.6742215]\n",
      "\n",
      "\n",
      "Log-Std at step 79878: [-0.03055826  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.6042571678757668 ; last reward: 3.6742215156555176\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        63.85249  107.623055]]\n",
      "values: tensor([[361036.7812]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12244592]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.096983 45.189228]]\n",
      "values: tensor([[338392.2188]])\n",
      "clipped_actions: [[ 0.        0.       28.096983 45.189228]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.244893]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.        0.        9.549567 23.170862]]\n",
      "values: tensor([[319438.5000]])\n",
      "clipped_actions: [[ 0.        0.        9.549567 23.170862]]\n",
      "new_obs: [[29.549568  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.       10.        0.        0.        0.        0.\n",
      "   0.        0.        3.      ]]\n",
      "rewards: [0.36182633]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[29.5496,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000, 10.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          3.0000]])\n",
      "actions: [[5.8924174e-23 2.4979417e+00 9.6866198e+00 1.6358919e+01]]\n",
      "values: tensor([[234134.2188]])\n",
      "clipped_actions: [[5.8924174e-23 2.4979417e+00 9.6866198e+00 1.6358919e+01]]\n",
      "new_obs: [[29.549568  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.3593794]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.5496,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.        0.        8.670568 17.894583]]\n",
      "values: tensor([[141222.3125]])\n",
      "clipped_actions: [[ 0.        0.        8.670568 17.894583]]\n",
      "new_obs: [[29.549568  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.35938087]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.5496,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 1.8719153  0.         9.244068  20.297602 ]]\n",
      "values: tensor([[45485.4180]])\n",
      "clipped_actions: [[ 1.8719153  0.         9.244068  20.297602 ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.40399906]\n",
      "\n",
      "\n",
      "Log-Std at step 81930: [-0.03055826  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.5993728237908061 ; last reward: 0.40399906039237976\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        63.149254 110.323044]]\n",
      "values: tensor([[134553.2812]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12249488]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       26.278812 41.408592]]\n",
      "values: tensor([[118585.9688]])\n",
      "clipped_actions: [[ 0.        0.       26.278812 41.408592]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24499092]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[2.3490268e-22 3.4150364e+00 9.9848909e+00 2.0447432e+01]]\n",
      "values: tensor([[108324.8281]])\n",
      "clipped_actions: [[2.3490268e-22 3.4150364e+00 9.9848909e+00 2.0447432e+01]]\n",
      "new_obs: [[2.9984890e+01 0.0000000e+00 2.3490268e-22 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.36485353]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.9985e+01, 0.0000e+00, 2.3490e-22, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 1.2785504  0.         8.055741  20.043232 ]]\n",
      "values: tensor([[63676.4141]])\n",
      "clipped_actions: [[ 1.2785504  0.         8.055741  20.043232 ]]\n",
      "new_obs: [[28.70634    1.2785504  0.         0.06       0.        10.\n",
      "   0.        10.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         4.       ]]\n",
      "rewards: [0.39495426]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[28.7063,  1.2786,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         3.0569315  9.596157  15.228999 ]]\n",
      "values: tensor([[20602.4082]])\n",
      "clipped_actions: [[ 0.         3.0569315  9.596157  15.228999 ]]\n",
      "new_obs: [[28.70634  0.       0.       0.       0.      10.       0.       0.\n",
      "   0.       0.       0.       0.       0.       0.       0.       0.\n",
      "   5.     ]]\n",
      "rewards: [1.959867]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[28.7063,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[9.4001654e-05 1.4324454e+00 1.1593685e+01 1.8006927e+01]]\n",
      "values: tensor([[2433.7781]])\n",
      "clipped_actions: [[9.4001654e-05 1.4324454e+00 1.1593685e+01 1.8006927e+01]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.9573429]\n",
      "\n",
      "\n",
      "Log-Std at step 83976: [-0.03055826  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.6317054437739509 ; last reward: 1.9573428630828857\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        62.558628 107.56824 ]]\n",
      "values: tensor([[368995.7188]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12171541]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.000746 45.674107]]\n",
      "values: tensor([[388129.6562]])\n",
      "clipped_actions: [[ 0.        0.       28.000746 45.674107]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.2434319]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[2.9590833e-19 4.1085148e+00 6.8926125e+00 2.2961519e+01]]\n",
      "values: tensor([[426908.8438]])\n",
      "clipped_actions: [[2.9590833e-19 4.1085148e+00 6.8926125e+00 2.2961519e+01]]\n",
      "new_obs: [[2.6892612e+01 0.0000000e+00 2.9590833e-19 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.3248935]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.6893e+01, 0.0000e+00, 2.9591e-19, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[3.1074298e-16 2.7436943e+00 8.4178419e+00 1.6702753e+01]]\n",
      "values: tensor([[360339.5938]])\n",
      "clipped_actions: [[3.1074298e-16 2.7436943e+00 8.4178419e+00 1.6702753e+01]]\n",
      "new_obs: [[26.892612  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.322461]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[26.8926,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.        0.        8.53833  16.713863]]\n",
      "values: tensor([[273946.1250]])\n",
      "clipped_actions: [[ 0.        0.        8.53833  16.713863]]\n",
      "new_obs: [[26.892612  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.32246247]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[26.8926,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 2.861796  0.        8.915135 14.523283]]\n",
      "values: tensor([[140916.9219]])\n",
      "clipped_actions: [[ 2.861796  0.        8.915135 14.523283]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.39091223]\n",
      "\n",
      "\n",
      "Log-Std at step 86022: [-0.03055826  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.6261056017043979 ; last reward: 0.3909122347831726\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        62.369713 105.15088 ]]\n",
      "values: tensor([[139041.5312]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12168128]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.560932 45.66816 ]]\n",
      "values: tensor([[137039.0469]])\n",
      "clipped_actions: [[ 0.        0.       28.560932 45.66816 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24336363]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.1405094e-19 3.1390245e+00 1.1235461e+01 1.9998350e+01]]\n",
      "values: tensor([[147972.4688]])\n",
      "clipped_actions: [[1.1405094e-19 3.1390245e+00 1.1235461e+01 1.9998350e+01]]\n",
      "new_obs: [[3.0000000e+01 0.0000000e+00 1.1405094e-19 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.36261386]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[3.0000e+01, 0.0000e+00, 1.1405e-19, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 1.1476481  0.        10.795253  17.422901 ]]\n",
      "values: tensor([[122101.7812]])\n",
      "clipped_actions: [[ 1.1476481  0.        10.795253  17.422901 ]]\n",
      "new_obs: [[28.852352   1.1476481  0.         0.06       0.        10.\n",
      "   0.        10.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         4.       ]]\n",
      "rewards: [0.3893288]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[28.8524,  1.1476,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         1.0538144  7.255561  19.233263 ]]\n",
      "values: tensor([[68507.8594]])\n",
      "clipped_actions: [[ 0.         1.0538144  7.255561  19.233263 ]]\n",
      "new_obs: [[28.852352    0.09383368  0.         -0.61383975  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [1.6704009]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[28.8524,  0.0938,  0.0000, -0.6138,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.5403245  0.         9.014368  20.31357  ]]\n",
      "values: tensor([[15996.4248]])\n",
      "clipped_actions: [[ 0.5403245  0.         9.014368  20.31357  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [1.682221]\n",
      "\n",
      "\n",
      "Log-Std at step 88074: [-0.03055826  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.6501082256436348 ; last reward: 1.6822210550308228\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       64.06724 104.67612]]\n",
      "values: tensor([[225142.2812]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12140116]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       28.979263 45.07646 ]]\n",
      "values: tensor([[214715.2500]])\n",
      "clipped_actions: [[ 0.        0.       28.979263 45.07646 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24280335]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.         3.7534103 10.576577  22.695538 ]]\n",
      "values: tensor([[208889.7969]])\n",
      "clipped_actions: [[ 0.         3.7534103 10.576577  22.695538 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.36420703]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 0.         2.0852199  6.117509  17.102106 ]]\n",
      "values: tensor([[157740.5625]])\n",
      "clipped_actions: [[ 0.         2.0852199  6.117509  17.102106 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  4.]]\n",
      "rewards: [0.3642089]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  4.]])\n",
      "actions: [[ 0.         1.9152126  8.063269  17.831795 ]]\n",
      "values: tensor([[86282.2578]])\n",
      "clipped_actions: [[ 0.         1.9152126  8.063269  17.831795 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  5.]]\n",
      "rewards: [0.36421022]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  5.]])\n",
      "actions: [[ 0.34887102  0.05685017  8.555574   21.20658   ]]\n",
      "values: tensor([[16969.4297]])\n",
      "clipped_actions: [[ 0.34887102  0.05685017  8.555574   21.20658   ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.24280727]\n",
      "\n",
      "\n",
      "Log-Std at step 90120: [-0.03055825  0.4140341   0.34290573  0.57663435]\n",
      "\n",
      "Avg rewards so far:0.6410570932759179 ; last reward: 0.2428072690963745\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       66.14316 105.87118]]\n",
      "values: tensor([[56286.6484]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12140536]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       32.411385 42.94477 ]]\n",
      "values: tensor([[40542.3555]])\n",
      "clipped_actions: [[ 0.        0.       32.411385 42.94477 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24281172]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[ 0.         1.2271181 11.729468  23.801805 ]]\n",
      "values: tensor([[29708.2559]])\n",
      "clipped_actions: [[ 0.         1.2271181 11.729468  23.801805 ]]\n",
      "new_obs: [[30.  0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  3.]]\n",
      "rewards: [0.36421952]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[30.,  0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  3.]])\n",
      "actions: [[ 2.1721158  0.         8.648227  18.46727  ]]\n",
      "values: tensor([[17995.9277]])\n",
      "clipped_actions: [[ 2.1721158  0.         8.648227  18.46727  ]]\n",
      "new_obs: [[27.827885   2.1721158  0.         0.06       0.        10.\n",
      "   0.        10.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         4.       ]]\n",
      "rewards: [0.4157493]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[27.8279,  2.1721,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[ 0.         1.327108   7.9395595 19.55263  ]]\n",
      "values: tensor([[4486.4502]])\n",
      "clipped_actions: [[ 0.         1.327108   7.9395595 19.55263  ]]\n",
      "new_obs: [[27.827885    0.8450078   0.         -0.03423166  0.         10.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          5.        ]]\n",
      "rewards: [2.025692]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[27.8279,  0.8450,  0.0000, -0.0342,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.         2.1507034 10.24737   20.64518  ]]\n",
      "values: tensor([[263.2780]])\n",
      "clipped_actions: [[ 0.         2.1507034 10.24737   20.64518  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [3.0499027]\n",
      "\n",
      "\n",
      "Log-Std at step 92166: [-0.03055802  0.4140341   0.34290573  0.5766342 ]\n",
      "\n",
      "Avg rewards so far:0.6934233016293981 ; last reward: 3.0499026775360107\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       61.84267 107.58623]]\n",
      "values: tensor([[544379.1250]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12121426]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       30.078215 46.160004]]\n",
      "values: tensor([[550194.3750]])\n",
      "clipped_actions: [[ 0.        0.       30.078215 46.160004]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24242951]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[4.8786732e-17 2.8109736e+00 9.4777393e+00 2.0235559e+01]]\n",
      "values: tensor([[546610.5000]])\n",
      "clipped_actions: [[4.8786732e-17 2.8109736e+00 9.4777393e+00 2.0235559e+01]]\n",
      "new_obs: [[2.9477739e+01 0.0000000e+00 4.8786732e-17 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.35489124]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.9478e+01, 0.0000e+00, 4.8787e-17, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.        0.        8.34783  18.128544]]\n",
      "values: tensor([[464627.8438]])\n",
      "clipped_actions: [[ 0.        0.        8.34783  18.128544]]\n",
      "new_obs: [[29.47774  0.       0.       0.       0.      10.       0.      10.\n",
      "   0.       0.       0.       0.       0.       0.       0.       0.\n",
      "   4.     ]]\n",
      "rewards: [0.35489303]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.4777,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[4.8301302e-04 1.1799170e+00 9.0990238e+00 2.1863346e+01]]\n",
      "values: tensor([[323528.5000]])\n",
      "clipped_actions: [[4.8301302e-04 1.1799170e+00 9.0990238e+00 2.1863346e+01]]\n",
      "new_obs: [[29.477257  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.35305554]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.4773,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[7.6007139e-10 2.2345073e+00 1.0151795e+01 1.8068920e+01]]\n",
      "values: tensor([[150403.2188]])\n",
      "clipped_actions: [[7.6007139e-10 2.2345073e+00 1.0151795e+01 1.8068920e+01]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.3506316]\n",
      "\n",
      "\n",
      "Log-Std at step 94218: [-0.03055802  0.4140341   0.34290573  0.5766342 ]\n",
      "\n",
      "Avg rewards so far:0.6861298610555365 ; last reward: 0.35063159465789795\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.         0.        61.41575  107.615585]]\n",
      "values: tensor([[82644.8125]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12093713]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       27.867378 45.39595 ]]\n",
      "values: tensor([[56450.0625]])\n",
      "clipped_actions: [[ 0.        0.       27.867378 45.39595 ]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24187522]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[3.7311945e-15 2.3706613e+00 1.0065834e+01 2.4512943e+01]]\n",
      "values: tensor([[37063.4805]])\n",
      "clipped_actions: [[3.7311945e-15 2.3706613e+00 1.0065834e+01 2.4512943e+01]]\n",
      "new_obs: [[3.0000000e+01 0.0000000e+00 3.7311945e-15 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.3603959]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[3.0000e+01, 0.0000e+00, 3.7312e-15, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[ 0.15560377  0.          8.832581   16.399155  ]]\n",
      "values: tensor([[14973.4951]])\n",
      "clipped_actions: [[ 0.15560377  0.          8.832581   16.399155  ]]\n",
      "new_obs: [[29.844397    0.15560377  0.          0.06        0.         10.\n",
      "   0.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.        ]]\n",
      "rewards: [0.36295202]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.8444,  0.1556,  0.0000,  0.0600,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[1.0613913e-09 1.7613304e+00 7.2784400e+00 2.0445856e+01]]\n",
      "values: tensor([[5285.8052]])\n",
      "clipped_actions: [[1.0613913e-09 1.7613304e+00 7.2784400e+00 2.0445856e+01]]\n",
      "new_obs: [[29.844397  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.5487198]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.8444,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.09410854  0.         11.87933    20.558006  ]]\n",
      "values: tensor([[305.9850]])\n",
      "clipped_actions: [[ 0.09410854  0.         11.87933    20.558006  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.5497848]\n",
      "\n",
      "\n",
      "Log-Std at step 96264: [-0.0305564   0.41403565  0.34290272  0.5766355 ]\n",
      "\n",
      "Avg rewards so far:0.6832893385241429 ; last reward: 0.549784779548645\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t: 1\n",
      "obs_tensor: tensor([[ 0.,  0.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,\n",
      "          0., 10.,  0.]])\n",
      "actions: [[  0.        0.       62.13448 108.89464]]\n",
      "values: tensor([[342345.8125]])\n",
      "clipped_actions: [[ 0.  0. 50. 50.]]\n",
      "new_obs: [[10.  0.  0.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  1.]]\n",
      "rewards: [0.12087487]\n",
      "\n",
      "t: 2\n",
      "obs_tensor: tensor([[10.,  0.,  0.,  0., 10.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,\n",
      "          0.,  0.,  1.]])\n",
      "actions: [[ 0.        0.       30.0512   44.830353]]\n",
      "values: tensor([[363867.1562]])\n",
      "clipped_actions: [[ 0.        0.       30.0512   44.830353]]\n",
      "new_obs: [[20.  0.  0.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  2.]]\n",
      "rewards: [0.24175064]\n",
      "\n",
      "t: 3\n",
      "obs_tensor: tensor([[20.,  0.,  0.,  0., 10.,  0.,  0., 10.,  0., 10.,  0., 10.,  0.,  0.,\n",
      "          0.,  0.,  2.]])\n",
      "actions: [[1.9405024e-04 2.5665708e+00 9.2426033e+00 2.2450384e+01]]\n",
      "values: tensor([[342679.9062]])\n",
      "clipped_actions: [[1.9405024e-04 2.5665708e+00 9.2426033e+00 2.2450384e+01]]\n",
      "new_obs: [[2.9242409e+01 0.0000000e+00 1.9405024e-04 0.0000000e+00 0.0000000e+00\n",
      "  1.0000000e+01 0.0000000e+00 1.0000000e+01 0.0000000e+00 1.0000000e+01\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 3.0000000e+00]]\n",
      "rewards: [0.35106218]\n",
      "\n",
      "t: 4\n",
      "obs_tensor: tensor([[2.9242e+01, 0.0000e+00, 1.9405e-04, 0.0000e+00, 0.0000e+00, 1.0000e+01,\n",
      "         0.0000e+00, 1.0000e+01, 0.0000e+00, 1.0000e+01, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00]])\n",
      "actions: [[8.737282e-13 1.945962e+00 9.024500e+00 1.913180e+01]]\n",
      "values: tensor([[274660.1562]])\n",
      "clipped_actions: [[8.737282e-13 1.945962e+00 9.024500e+00 1.913180e+01]]\n",
      "new_obs: [[29.242409  0.        0.        0.        0.       10.        0.\n",
      "  10.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        4.      ]]\n",
      "rewards: [0.34888095]\n",
      "\n",
      "t: 5\n",
      "obs_tensor: tensor([[29.2424,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000, 10.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          4.0000]])\n",
      "actions: [[3.6244178e-11 1.9865756e+00 6.5341029e+00 1.8976942e+01]]\n",
      "values: tensor([[162380.5469]])\n",
      "clipped_actions: [[3.6244178e-11 1.9865756e+00 6.5341029e+00 1.8976942e+01]]\n",
      "new_obs: [[29.242409  0.        0.        0.        0.       10.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        5.      ]]\n",
      "rewards: [0.34646466]\n",
      "\n",
      "t: 6\n",
      "obs_tensor: tensor([[29.2424,  0.0000,  0.0000,  0.0000,  0.0000, 10.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          5.0000]])\n",
      "actions: [[ 0.71762365  0.          9.829247   18.400797  ]]\n",
      "values: tensor([[58685.9531]])\n",
      "clipped_actions: [[ 0.71762365  0.          9.829247   18.400797  ]]\n",
      "new_obs: [[ 0.  0.  0.  0. 10.  0. 10.  0. 10.  0.  0. 10.  0. 10.  0. 10.  0.]]\n",
      "rewards: [0.36260515]\n",
      "\n",
      "\n",
      "Log-Std at step 98310: [-0.03055619  0.41403577  0.34290236  0.5766355 ]\n",
      "\n",
      "Avg rewards so far:0.6767447633402688 ; last reward: 0.36260515451431274\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2191a825eb0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"logging at {model_name}\")\n",
    "model.learn(total_timesteps=100000, \n",
    "            progress_bar=False, \n",
    "            tb_log_name=model_name, \n",
    "            callback=log_std_callback,\n",
    "            reset_num_timesteps=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "plt.plot([k*2048 for k in range(len(log_std_callback.log_stds))], [k for k in log_std_callback.log_stds])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average action log std\")\n",
    "plt.title(model_name.split(\"/\")[-1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_parameters('models\\\\1_simplest_0619-1445_PPO_MLP_ent_0.0001_gam_0.99_clip_0.1_10_100_0_0_ReLU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "M,Q,P,B,Z,D = 0, 0, 0, 0, 1, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlendEnv(v = True, \n",
    "               M = M, Q = Q, P = P, B = B, Z = Z, D = D, \n",
    "               action_sample = action_sample, \n",
    "               connections = connections, \n",
    "               tau0 = tau0,\n",
    "               delta0 = delta0,\n",
    "               sigma = sigma,\n",
    "               sigma_ub = sigma_ub,\n",
    "               sigma_lb = sigma_lb,\n",
    "               s_inv_lb = s_inv_lb,\n",
    "               s_inv_ub = s_inv_ub,\n",
    "               d_inv_lb = d_inv_lb,\n",
    "               d_inv_ub = d_inv_ub,\n",
    "               betaT_d = betaT_d,\n",
    "               betaT_s = betaT_s,\n",
    "               b_inv_ub = b_inv_ub,\n",
    "               b_inv_lb = b_inv_lb)\n",
    "env = Monitor(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t1; s1:\t\t\tbought too much (more than supply)\n",
      "Increased reward by 0.0 through tank population in s1\n",
      "j1: inv: 0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 10.0} {'j1': 0.0} {'p1': 0.0}\n",
      "    0.0\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t2; s1:\t\t\tbought too much (more than supply)\n",
      "s1: b: 0.4\n",
      "[PEN] t2; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 20.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0.0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 20.0} {'p1': 0.0}\n",
      "    -0.1\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 0.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 0.0}}\n",
      "[PEN] t3; s1:\t\t\tbought too much (more than supply)\n",
      "s1: b: 0.2\n",
      "[PEN] t3; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 20.0, in_flow_sources: 10.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 0.0\n",
      "Increased reward by 0.0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 30.0} {'p1': 0.0}\n",
      "    -0.2\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 0.0}}, 'blend_demand': {'j1': {'p1': 50.0}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 50.0}}\n",
      "[PEN] t4; s1:\t\t\tbought too much (more than supply)\n",
      "[PEN] t4; p1:\t\t\tsold too much (more than demand)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 30.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 50.0\n",
      "j1: b: 0.6\n",
      "[PEN] t4; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0.0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 0.0} {'p1': 20.0}\n",
      "    9.700000000000001\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 26.330013}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 50.0}}\n",
      "[PEN] t5; s1:\t\t\tbought too much (more than supply)\n",
      "[PEN] t5; p1:\t\t\tsold too much (more than demand)\n",
      "s1: b: 0.0\n",
      "[PEN] t5; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 26.330013275146484\n",
      "j1: b: 0.0\n",
      "[PEN] t5; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 0.0} {'p1': 10.0}\n",
      "    19.700000000000003\n",
      "\n",
      "\n",
      "    {'source_blend': {'s1': {'j1': 50.0}}, 'blend_demand': {'j1': {'p1': 28.335169}}, 'tau': {'s1': 50.0}, 'delta': {'p1': 50.0}}\n",
      "[PEN] t6; s1:\t\t\tbought too much (more than supply)\n",
      "[PEN] t6; p1:\t\t\tsold too much (more than demand)\n",
      "s1: b: 0.0\n",
      "[PEN] t6; s1:\t\t\tbought too little (resulting amount less than source tank LB)\n",
      "Increased reward by 0 through tank population in s1\n",
      "j1: inv: 0.0, in_flow_sources: 0.0, in_flow_blend: 0, out_flow_blend: 0, out_flow_demands: 28.335168838500977\n",
      "j1: b: 0.0\n",
      "[PEN] t6; j1:\t\t\tinventory OOB (resulting amount less than blending tank LB)\n",
      "Increased reward by 0 through tank population in j1\n",
      "Increased reward by 0 through tank population in p1\n",
      "\n",
      "    >>      {'s1': 0.0} {'j1': 0.0} {'p1': 0.0}\n",
      "    29.700000000000003\n"
     ]
    }
   ],
   "source": [
    "with th.autograd.set_detect_anomaly(True):\n",
    "    obs = env.reset()\n",
    "    obs, obs_dict = obs\n",
    "    for k in range(env.T):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        print(\"\\n\\n   \",reconstruct_dict(action, env.mapping_act))\n",
    "        obs, reward, done, term, _ = env.step(action)\n",
    "        dobs = reconstruct_dict(obs, env.mapping_obs)\n",
    "        print(\"\\n    >>     \",dobs[\"sources\"], dobs[\"blenders\"], dobs[\"demands\"])\n",
    "        print(\"   \" ,reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 (only once per episode)\n",
    "episode_rewards = []\n",
    "obs = env.reset()\n",
    "obs, obs_dict = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 1 Get first action\n",
    "print(env.t)\n",
    "action, _ = model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'s1': 17.46205}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'source_blend': {'s1': {'j1': 0.0}},\n",
       " 'blend_demand': {'j1': {'p1': 30.307917}},\n",
       " 'tau': {'s1': 8.731916},\n",
       " 'delta': {'p1': 17.08481}}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Visualize action\n",
    "print(env.t)\n",
    "reconstruct_dict(action, env.mapping_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Step once: get 2nd action\n",
    "print(env.t)\n",
    "obs, reward, done, term, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'s1': 26.193966}\n",
      "{'j1': 0.0}\n",
      "{'p1': 0.0}\n",
      "{'j1': {'q1': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "# 4 Visualize new state\n",
    "print(env.t)\n",
    "d = reconstruct_dict(obs, env.mapping_obs)\n",
    "print(d[\"sources\"])\n",
    "print(d[\"blenders\"])\n",
    "print(d[\"demands\"])\n",
    "print(d[\"properties\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blendv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
